{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image segmentation for self-driving car"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to Image segmentation for self-driving car project.\n",
    "In this project I will explore about semantic image segmentation for self-driving car.\n",
    "In this jupyter notebook I am going to go through the complete process of the project along with code snippets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Content\n",
    "- [1 - Problem statement](#1)\n",
    "    - [1.1 - The goals of the project](#1-1)\n",
    "    - [1.2 - The challenges of the project](#1-2)\n",
    "- [2 - Introduction to the Problem](#2)\n",
    "    - [2.1 - What is image segmentation?](#2-1)\n",
    "    - [2.2 - Types of image segmentation](#2-2)\n",
    "        - [2.2.1 - Image semantic segmentation](#2-2-1)\n",
    "        - [2.2.2 - Image Instance segmentation](#2-2-2)\n",
    "        - [2.2.3 - Image Panoptic segmentation](#2-2-3)\n",
    "    - [2.3 - Image segmentation Methods](#2-3)\n",
    "        - [2.3.1 - Traditional Methods](#2-3-1)\n",
    "        - [2.3.2 - Deep learning Methods](#2-3-2)\n",
    "- [3 - CityScapes dataset](#3)\n",
    "    - [3.1 - Features](#3-1)\n",
    "    - [3.2 - Classes](#3-2)\n",
    "- [4 - Packages](#4)\n",
    "- [5 - Preprocessing](#5)\n",
    "    - [5.1 - Load and split our data to train/dev/test datasets](#5-1)  \n",
    "    - [5.2 - Process Labels](#5-2)\n",
    "    - [5.3 - Process path](#5-3)\n",
    "    - [5.4 - Explore train dataset](#5-4)\n",
    "    - [5.5 - Preprocessing the dataset](#5-5) \n",
    "        - [5.5.1 - Why we need to resize our dataset?](#5-5-1)\n",
    "        - [5.5.2 - Why we need to normalize our dataset?](#5-5-2)\n",
    "        - [5.5.3 - Code Preprocessing function](#5-5-3)\n",
    "    - [5.6 - Data augmentation](#5-6)\n",
    "        - [5.6.1 - What is Data augmentation and why we use this?](#5-6-1)\n",
    "        - [5.6.2 - Why some data augmentation techniques is not good for self-driving car?](#5-6-2)\n",
    "        - [5.6.3 - Which data augmentation techniques is still good for self-driving car?](#5-6-3)\n",
    "        - [5.6.4 - Code Data augmentation](#5-6-4)\n",
    "    - [5.7 - Divide our train dataset to mini batches and shuffle our train dataset](#5-7)\n",
    "    - [5.8 - Use prefetch](#5-8)\n",
    "- [6 - Unet explanation](#6)\n",
    "    - [6.1 - What is Unet?](#6-1)\n",
    "    - [6.2 - Unet model detials](#6-2)\n",
    "        - [6.2.1 - Unet model genral detials and Architecture](#6-2-1)\n",
    "        - [6.2.2 - Unet Encoder](#6-2-2)\n",
    "        - [6.2.3 - Unet Decoder](#6-2-3)\n",
    "        - [6.2.4 - Unet Connecting paths](#6-2-4)\n",
    "        - [6.2.5 - Unet bottlenack](#6-2-5)\n",
    "        - [6.2.6 - Initialization of the weights](#6-2-6)\n",
    "        - [6.2.7 - Regulzation](#6-2-7)    \n",
    "    - [6.3 - Pros and Cons of Unet](#6-3)\n",
    "        - [6.3.1 - Pros of Unet](#6-3-1)\n",
    "        - [6.3.2 - Cons of Unet](#6-3-2)\n",
    "- [7 - Cost functions and evaluation metrics](#7)\n",
    "    - [7.1 - Pixel accuracy](#7-1)\n",
    "    - [7.2 - What is the problem of pixel accuracy?](#7-2)\n",
    "    - [7.3 - Sparse Categorical Cross entropy](#7-3)\n",
    "    - [7.4 - Why still Sparse Categorical Cross entropy is not good enough?](#7-4)\n",
    "    - [7.5 - The solutions for highly unbalanced segmentations and giving importance to certain pixels](#7-5)\n",
    "        - [7.5.1 - Weighted Sparse Categorical Cross entropy for each class](#7-5-1)\n",
    "        - [7.5.2 - Weighted Sparse Categorical Cross entropy for each pixel](#7-5-2)\n",
    "        - [7.5.3 - Dice Coefficient and soft Dice Coefficient](#7-5-3)\n",
    "            - [7.5.3.1 - What is Precision and Recall?](#7-5-3-1)\n",
    "            - [7.5.3.2 - What is F1 score?](#7-5-3-2)\n",
    "            - [7.5.3.3 - Dice Coefficient explanation and formula explanation](#7-5-3-3)\n",
    "            - [7.5.3.4 - Soft Dice Coefficient](#7-5-3-4)\n",
    "            - [7.5.3.5 - Why Soft Dice Coefficient is good?](#7-5-3-5)\n",
    "- [8 - The models that we will build](#8)\n",
    "- [9 - Let's build Unet encoders and decoders](#9)\n",
    "    - [9.1 - Let's build Unet encoder](#9-1)\n",
    "    - [9.2 - Let's build Unet decoder](#9-2)\n",
    "- [10 - Let's build the genral unet model](#10)\n",
    "- [11 - Let's implement the cost functions and evaluation metrics that we need for the models](#11)\n",
    "- [12 - Let's implement plot function for plot history of the model](#12)\n",
    "- [13 - Let's implement functions for shows model predictions](#13)\n",
    "- [14 - Let's implement visualization callbacks functions](#14)\n",
    "- [15 - Let's implement schedulers](#15)\n",
    "    - [15.1 - Let's implement Dropout scheduler](#15-1)\n",
    "    - [15.2 - Let's implement Learning Rate scheduler](#15-2) \n",
    "- [16 - Regular Unet model](#14)\n",
    "    - [16.1 - First model version](#16-1)\n",
    "        - [16.1.1 - Create the model](#16-1-1)\n",
    "        - [16.1.2 - Train the model and evaluate him on train and dev datasets](#16-1-2)\n",
    "        - [16.1.3 - Model's predictions on the train and dev datasets](#16-1-3)\n",
    "    - [16.2 - Second model version](#16-2)\n",
    "        - [16.2.1 - Create the model](#16-2-1)\n",
    "        - [16.2.2 - Train the model and evaluate him on train and dev datasets](#16-2-2)\n",
    "    - [16.3 - Third model version](#16-3)\n",
    "        - [16.3.1 - Create the model](#16-3-1)\n",
    "        - [16.3.2 - Train the model and evaluate him on train and dev datasets](#16-3-2)\n",
    "        - [16.3.3 - Model's predictions on the train and dev datasets](#16-3-3)\n",
    "    - [16.4 - Fourth model version](#16-4)\n",
    "        - [16.4.1 - Create the model](#16-4-1)\n",
    "        - [16.4.2 - Train the model and evaluate him on train and dev datasets](#16-4-2)\n",
    "    - [16.5 - Fiveth model version](#16-5)\n",
    "        - [16.5.1 - Create the model](#16-5-1)\n",
    "        - [16.5.2 - Train the model and evaluate him on train and dev datasets](#16-5-2)\n",
    "    - [16.6 - Sixth model version](#16-6)\n",
    "        - [16.6.1 - Create the model](#16-6-1)\n",
    "        - [16.6.2 - Train the model and evaluate him on train and dev datasets](#16-6-2)\n",
    "    - [16.7 - Seventh model version](#16-7)\n",
    "        - [16.7.1 - Create the model](#16-7-1)\n",
    "        - [16.7.2 - Train the model and evaluate him on train and dev datasets](#16-7-2)\n",
    "    - [16.8 - Eighth model version](#16-8)\n",
    "        - [16.8.1 - Create the model](#16-8-1)\n",
    "        - [16.8.2 - Train the model and evaluate him on train and dev datasets](#16-8-2)\n",
    "        - [16.8.3 - Model's predictions on the train and dev datasets](#16-8-3)\n",
    "    - [16.9 - Ninth model version](#16-9)\n",
    "        - [16.9.1 - Create the model](#16-9-1)\n",
    "        - [16.9.2 - Train the model and evaluate him on train and dev datasets](#16-9-2)\n",
    "    - [16.10 - Tenth model version](#16-10)\n",
    "        - [16.10.1 - Create the model](#16-10-1)\n",
    "        - [16.10.2 - Train the model and evaluate him on train and dev datasets](#16-10-2)\n",
    "        - [16.10.3 - Model's predictions on the train and dev datasets](#16-10-3)\n",
    "    - [16.11 - Eleventh model version](#16-11)\n",
    "        - [16.11.1 - Create the model](#16-11-1)\n",
    "        - [16.11.2 - Train the model and evaluate him on train and dev datasets](#16-11-2)\n",
    "        - [16.11.3 - Model's predictions on the train and dev datasets](#16-11-3)\n",
    "    - [16.12 - Twelfth model version](#16-12)\n",
    "        - [16.12.1 - Create the model](#16-12-1)\n",
    "        - [16.12.2 - Train the model and evaluate him on train and dev datasets](#16-12-2)\n",
    "        - [16.12.3 - Model's predictions on the train and dev datasets](#16-12-3)\n",
    "    - [16.13 - Thirteen model version](#16-13)\n",
    "        - [16.13.1 - Create the model](#16-13-1)\n",
    "        - [16.13.2 - Train the model and evaluate him on train and dev datasets](#16-13-2)\n",
    "        - [16.13.3 - Model's predictions on the train and dev datasets](#16-13-3)\n",
    "- [17 - Final model](#17)\n",
    "    - [17.1 - Choose the model that we will use](#17-1)\n",
    "    - [17.2 - Final model's predictions on the test dataset](#17-2)\n",
    "    - [17.3 - Final model evaluation on the test dataset](#17-3)\n",
    "- [18 - Summary](#18)\n",
    "- [19 - Goals for the future](#19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Problem statement\n",
    "\n",
    "Our problem is to explore Unet architectures for do semantic segmentation for self-driving car on Cityscapes dataset\n",
    "and try to get the lowest Computational Cost, lowest storage and the best accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1-1'></a>\n",
    "### 1.1 - The goals of the project\n",
    "The goals of the project are:\n",
    "\n",
    "* To explore semantic segmentation for self-driving car.\n",
    "\n",
    "* To explore how Unet architecture work for self-driving car.\n",
    "\n",
    "* To get the the lowest Computational Cost, lowest storage and the best accuracy that I can on Cityscapes Dataset when I use Unet architectures.\n",
    "\n",
    "* To explore different models that will help to achieve the thrid goal\n",
    "and compare between them.\n",
    "\n",
    "* To experience build projects of this magnitude(my first project of this magnitude).\n",
    "\n",
    "* To experience kaggle and their computing power for run the models that we will explore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1-2'></a>\n",
    "### 1.2 - The challenges of the project\n",
    "The challenges of the project are:\n",
    "\n",
    "* Our data set is contains large images. This is Makes it difficult for us to do semantic segmentation problem for self-driving car and increases the Computational Cost and storage. This leads us to use complex architectures for get good accuracy and therefore we encounter situation that we need to striking balance the trade off between accuracy of the model and Computational Cost and storage.\n",
    "\n",
    "* Self-driving car tasks are real-time tasks and therefore we must to solve the problem with the lowest Computational Cost, lowest storage and use efficient segmentation algorithms\n",
    "\n",
    "* Self-driving car tasks are difficlut, because there are many situations we can encounter while driving, for example: different weather conditions, daytime vs nighttime, different road conditions, blurry noisable and unintelligible images and more.\n",
    "\n",
    "* Our data set contain 34 classes, so we have a lot of classes.\n",
    "\n",
    "* Semantic segmentation for self-driving car is difficult. We need to achieve high accuracy in pixel-level segmentation for ensure safe driving. Even small error can lead significant consequences.\n",
    "\n",
    "* Because we have limited dataset size we have more chance for overfitting.\n",
    "\n",
    "* Because we have limited dataset size we have more chance for bad generalization and bad adaptation to scenarios and environments that the model don't saw.\n",
    "\n",
    "And many more challenges that we will encounter during the project.\n",
    "\n",
    "This is my first project of this magnitude so this is challenging for me and I will have to solve and overcome many challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Introduction to the Problem\n",
    "In this section I will give some introduction to the Problem.\n",
    "I will explain briefly about image segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2-1'></a>\n",
    "### 2.1 - What is image segmentation?\n",
    "Image segmentation is a famous task in computer vision.\n",
    "\n",
    "Image segmentation is the process of dividing an image into multiple meaningful regions or objects based on their inherent characteristics, such as color, texture, shape, or brightness. \n",
    "\n",
    "In this way we can get more information about the image and we can easier to analyze the image.\n",
    "For example in self-driving car tasks we can get better scene understanding and get desicions accrding this, like stop if we close to car or pedestrian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2-2'></a>\n",
    "### 2.2 - Types of image segmentation\n",
    "There are 3 types of image segmentation: semantic segmentation, instance segmentation and panoptic segmentation.\n",
    "\n",
    "In this project I will work on semantic segmentation but now I will cover briefly all the 3 types of image segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2-2-1'></a>\n",
    "#### 2.2.1 - Image semantic segmentation\n",
    "Image semantic segmentation is a famous task in computer vision of labelling each pixel of the image into a predefined set of classes.\n",
    "That mean that we ask the following question:\n",
    "\n",
    "\"What objects are in this image and where exactly in the image are those objects located? \n",
    "Give me precise mask for each object in the image by labeling each pixel in the image with its corresponding class.\"\n",
    "\n",
    "For example:\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "<img src=\"Images/semantic segmentation.webp\" style=\"width:500px;height:250;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 1</u></b>: Example of a semantic segmented image <a href=\"https://medium.com/analytics-vidhya/introduction-to-semantic-image-segmentation-856cda5e5de8\">(Source)</a> <br> </center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2-2-2'></a>\n",
    "#### 2.2.2 - Image instance segmentation\n",
    "Image instance segmentation involves detecting and segmenting each object in an image.\n",
    "\n",
    "Differently from semantic segmentation that labelling each pixel of the image into a predefined set of classes, this type of segmentation segmenting the object’s boundaries.\n",
    "That mean that we ask the following question:\n",
    "\n",
    "\"What objects are in this image and what the boundaries of those objects?\"\n",
    "\n",
    "For example:\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "<img src=\"Images/Instance segmentation input.png\" style=\"width:500px;height:300px;\">\n",
    "<img src=\"Images/Instance segmentation output.png\" style=\"width:500px;height:300px;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 2</u></b>: Example of a instance segmented image<br> </center></caption>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2-2-3'></a>\n",
    "#### 2.2.3 - Image panoptic segmentation\n",
    "Panoptic segmentation generalizes both semantic and instance segmentation.\n",
    "\n",
    "That mean that we ask the following question:\n",
    "\n",
    "\"What different objects are in this image and where exactly in the image are those objects located?\n",
    "Give me precise mask for each object in the image by labeling each pixel in the image with its corresponding class and distinguish between objects that are different from the same class.\"\n",
    "\n",
    "This type of image segmentation get the most information from the image, because he knows also tell where there are different object. For example in self-driving car taks this is important because we want to know what and where the different objects but also want to distinguish between different cars or pedestrians.\n",
    "\n",
    "For example:\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "<img src=\"Images/Panoptic segmentation input.png\" style=\"width:500px;height:300;\">\n",
    "<img src=\"Images/Panoptic segmentation output.png\" style=\"width:500px;height:300;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 3</u></b>: Example of a panoptic segmented image<br><a href=\"https://medium.com/hasty-ai/panoptic-segmentation-explained-ca10597fb357\">(Source)</a> </center></caption>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2-3'></a>\n",
    "### 2.3 - Image segmentation Methods\n",
    "In this project I will explore Unet architectures for image semantic segmentation.\n",
    "\n",
    "Now I will cover briefly Image segmentation methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2-3-1'></a>\n",
    "#### 2.3.1 - Traditional Methods\n",
    "Traditional methods for image segmentation are usually computationally efficient and relatively simple to implement(for example relatively to deep learning methods). \n",
    "\n",
    "Some examples of traditional methods for image segmentation are:\n",
    "* Thresholding like: Global thresholding and Adaptive thresholding\n",
    "* Edge-based Segmentation like: Canny edge detection and Sobel edge detection\n",
    "* Clustering like: K-means clustering\n",
    "\n",
    "Traditional image segmentation methods are more suitable for simpler image segmentation and have limited accuracy on complex scenes. \n",
    "\n",
    "Therefore when deep learning field start raise, pepole started use deep learning methods for image segmentation tasks, because they are have high accuracy(relatively to traditional methods) on complex scenes and suitable not only for simpler image segmentation, but also for complex image segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2-3-2'></a>\n",
    "#### 2.3.2 - Deep learning Methods\n",
    "Deap learning methods for image segmentation are less computationally efficient than Traditional methods for image segmentation and relatively complex to implement. \n",
    "\n",
    "As I said in section 2.3.1 deap learning methods for image segmentation are have high accuracy(relatively to traditional methods) on complex scenes and suitable not only for simpler image segmentation, but also for complex image segmentation.\n",
    "\n",
    "Some examples of deap learning methods for image segmentation are:\n",
    "* U-Net, in this project I will explore Unet architectures for image semantic segmentation.\n",
    "* SegNet\n",
    "* DeepLab\n",
    "* Panoptic FPN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - CityScapes dataset\n",
    "In this project I will use the CityScapes dataset. \n",
    "\n",
    "In this section I will give overiew of CityScapes dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3-1'></a>\n",
    "### 3.1 - Features\n",
    "The features of CityScapes dataset that I use according the <a href=\"https://www.cityscapes-dataset.com/dataset-overview/#features\">CityScapes site</a>, are(Some of the things on the site are not updated, so I changed and wrote here what is correct):\n",
    "\n",
    "* Complexity\n",
    "    * 34 classes\n",
    "    * See Class Definitions in section 3.2\n",
    "\n",
    "* Diversity\n",
    "    * 50 cities\n",
    "    * Several months (spring, summer, fall)\n",
    "    * Daytime\n",
    "    * Good/medium weather conditions\n",
    "    * Manually selected frames\n",
    "        * Large number of dynamic objects\n",
    "        * Varying scene layout\n",
    "        * Varying background\n",
    "\n",
    "* Volume\n",
    "    * 3475 annotated images with fine annotations\n",
    "\n",
    "* Images dimensions\n",
    "    * 2048x1024 pixels \n",
    "\n",
    "* Division into train/dev/test sets\n",
    "    * Train set contains 2975 images, that mean the size of the train set is 59.5% of the dataset.\n",
    "    * Dev(or validation set) set contains 500 images, that mean the size of the dev set is 10% of the dataset. \n",
    "    * Test set contains 1,525 images, that mean the size of the dev set is 30.5% of the dataset.\n",
    "\n",
    "    In summary, the dataset division into train/dev/test sets is 59.5%/10%/30.5%.\n",
    "    The problem that the test dataset not has labels, and therefore we need to divide the dataset into train/dev/test sets manually.\n",
    "    \n",
    "    We will divide the dataset into train/dev/test sets in this way:\n",
    "\n",
    "    * Train set contains 2085 images, that mean the size of the train set is 60% of the dataset. \n",
    "    * Dev set contains 521 images, that mean the size of the train set is approx 15% of the dataset. \n",
    "    * Test set contains 869 images, that mean the size of the train set is approx 25% of the dataset. \n",
    "\n",
    "    In total this is a logical division into train/dev/test, according our dataset size.\n",
    "    \n",
    "<div style=\"text-align:center\">\n",
    "<img src=\"Images/cityscapesCities.jpg\" style=\"width:500px;height:400px;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 4</u></b>: Contained cities in CityScapes dataset<br><a href=\"https://www.cityscapes-dataset.com/dataset-overview/#features\">(Source)</a> </center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3-2'></a>\n",
    "### 3.2 - Classes\n",
    "The Classes of CityScapes dataset according the CityScapes site</a>, are:\n",
    "<div style=\"text-align:center\">\n",
    "<img src=\"Images/cityscapesClasses.png\" style=\"width:700px;height:400px;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 5</u></b>: Contained classes in CityScapes dataset<br><a href=\"https://www.cityscapes-dataset.com/dataset-overview/#features\">(Source)</a> </center></caption>\n",
    "\n",
    "The classes of CityScapes dataset for semantic segmentation are described in the picture without sign and with * sign.\n",
    "\n",
    "In summary for semantic segmentation task we have 8 groups and 19 classes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4 - Packages\n",
    "After all the introduction to the project, let's start work on project and import all the libraries that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import copy\n",
    "\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Dropout \n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "## 5 - Preprocessing\n",
    "In this section I will preprocess the data set. This is an important and essential part before I start building a model to solve my problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5-1'></a>\n",
    "### 5.1 - Load and split our data to train/dev/test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly I will get the paths of all the images and all the masks in the train/dev/test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_top_directory = './data/leftImg8bit_trainvaltest/leftImg8bit/train/'\n",
    "train_ground_truth_images_top_directory = './data/gtFine_trainvaltest/gtFine/train/'\n",
    "dev_images_top_directory = './data/leftImg8bit_trainvaltest/leftImg8bit/val/'\n",
    "dev_ground_truth_images_top_directory = './data/gtFine_trainvaltest/gtFine/val/'\n",
    "\n",
    "TOTAL_FILES = 3475\n",
    "TOTAL_TRAIN_FILES = 2085\n",
    "TOTAL_DEV_FILES = 521\n",
    "TOTAL_TEST_FILES = 869\n",
    "\n",
    "def read_files_from_directory(directory, extensions):\n",
    "    files = []\n",
    "    for root, dirs, dir_files in os.walk(directory):\n",
    "        for file in dir_files:\n",
    "            if any(file.endswith(ext) for ext in extensions):\n",
    "                files.append(os.path.join(root, file))\n",
    "                pbar.update(1)\n",
    "    return sorted(files)  # Sort the collected files\n",
    "\n",
    "images_extensions = ('.jpg', '.png')\n",
    "ground_truth_extensions = ('labelIds.jpg', 'labelIds.png')\n",
    "\n",
    "with tqdm(total=TOTAL_FILES, desc=\"Processing images files\") as pbar:\n",
    "    images_path_first_part = read_files_from_directory(train_images_top_directory, images_extensions)\n",
    "    images_path_second_part = read_files_from_directory(dev_images_top_directory, images_extensions)\n",
    "    images_path = images_path_first_part + images_path_second_part\n",
    "\n",
    "with tqdm(total=TOTAL_FILES, desc=\"Processing ground truth images files\") as pbar:\n",
    "    ground_truth_images_path_first_part = read_files_from_directory(train_ground_truth_images_top_directory, ground_truth_extensions)\n",
    "    ground_truth__images_path_second_part = read_files_from_directory(dev_ground_truth_images_top_directory, ground_truth_extensions)\n",
    "    ground_truth_images_path = ground_truth_images_path_first_part + ground_truth__images_path_second_part\n",
    "\n",
    "train_images_path = images_path[:TOTAL_TRAIN_FILES]\n",
    "train_ground_truth_images_path = ground_truth_images_path[:TOTAL_TRAIN_FILES]\n",
    "dev_images_path = images_path[TOTAL_TRAIN_FILES:TOTAL_TRAIN_FILES+TOTAL_DEV_FILES]\n",
    "dev_ground_truth_path = ground_truth_images_path[TOTAL_TRAIN_FILES:TOTAL_TRAIN_FILES+TOTAL_DEV_FILES]\n",
    "test_images_path = images_path[TOTAL_TRAIN_FILES+TOTAL_DEV_FILES:TOTAL_TRAIN_FILES+TOTAL_DEV_FILES+TOTAL_TEST_FILES]\n",
    "test_ground_truth_path = ground_truth_images_path[TOTAL_TRAIN_FILES+TOTAL_DEV_FILES:TOTAL_TRAIN_FILES+TOTAL_DEV_FILES+TOTAL_TEST_FILES]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now We will use tensorflow in order to do load our datasets efficiently.\n",
    "\n",
    "Firstly we will use tf.data.Dataset.from_tensor_slices method for create datasets from our lists of the images paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_path = tf.constant(train_images_path)\n",
    "train_ground_truth_images_path = tf.constant(train_ground_truth_images_path)\n",
    "\n",
    "dev_images_path = tf.constant(dev_images_path)\n",
    "dev_ground_truth_path = tf.constant(dev_ground_truth_path)\n",
    "\n",
    "test_images_path = tf.constant(test_images_path)\n",
    "test_ground_truth_path = tf.constant(test_ground_truth_path)\n",
    "\n",
    "train_image_dataset_before_path_processing = tf.data.Dataset.from_tensor_slices((train_images_path, train_ground_truth_images_path))\n",
    "dev_image_dataset_before_path_processing = tf.data.Dataset.from_tensor_slices((dev_images_path, dev_ground_truth_path))\n",
    "test_image_dataset_before_path_processing = tf.data.Dataset.from_tensor_slices((test_images_path, test_ground_truth_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5-2'></a>\n",
    "### 5.2 - Process Labels\n",
    "In this section I will process the labels in the dataset.\n",
    "\n",
    "For each image in the datasets we have corresponding mask image that represents segmantation map of our image. For each object we have unique color, that represents object's label.\n",
    "How we will know what the pixel's label, what the label represents, to which catagory of labels he belongs, which labels we will need to take in considaration when we evalute our models?\n",
    "\n",
    "The solution for these questions is to create list of Labels, when each label will contain her features and with this solution we will can for example to convert color to the corresponding label id in order to encode our mask to segmatation map that contatins the labels ids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start create label class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Label:\n",
    "    def __init__(self, name: str, id: int, catagory: str, catagory_id: int, color: Tuple[int, int, int]):\n",
    "        self.__name = name\n",
    "        self.__id = id\n",
    "        self.__catagory = catagory\n",
    "        self.__catagory_id = catagory_id\n",
    "        self.__color = color\n",
    "\n",
    "    def get_name(self) -> str:\n",
    "        return self.__name\n",
    "    \n",
    "    def get_id(self) -> int:\n",
    "        return self.__id\n",
    "    \n",
    "    def get_catagory(self) -> str:\n",
    "        return self.__catagory\n",
    "    \n",
    "    def get_catagory_id(self) -> int:\n",
    "        return self.__catagory_id\n",
    "    \n",
    "    def get_color(self) -> Tuple[int, int, int]:\n",
    "        return self.__color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will initialize the list of the Labels in our dataset, inspired by this <a href=\"https://github.com/mcordts/cityscapesScripts/blob/master/cityscapesscripts/helpers/labels.py\">page</a> and according section\n",
    "3.2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\n",
    "    #       name                     id      category       catagory Id       color\n",
    "    Label(  'unlabeled'            ,  0 ,    'void'            , 0       , (  0,  0,  0) ),\n",
    "    Label(  'ego vehicle'          ,  1 ,    'void'            , 0       , (  0,  0,  0) ),\n",
    "    Label(  'rectification border' ,  2 ,    'void'            , 0       , (  0,  0,  0) ),\n",
    "    Label(  'out of roi'           ,  3 ,    'void'            , 0       , (  0,  0,  0) ),\n",
    "    Label(  'static'               ,  4 ,    'void'            , 0       , (  0,  0,  0) ),\n",
    "    Label(  'dynamic'              ,  5 ,    'void'            , 0       , (111, 74,  0) ),\n",
    "    Label(  'ground'               ,  6 ,    'void'            , 0       , ( 81,  0, 81) ),\n",
    "    Label(  'road'                 ,  7 ,    'flat'            , 1       , (128, 64,128) ),\n",
    "    Label(  'sidewalk'             ,  8 ,    'flat'            , 1       , (244, 35,232) ),\n",
    "    Label(  'parking'              ,  9 ,    'flat'            , 1       , (250,170,160) ),\n",
    "    Label(  'rail track'           , 10 ,    'flat'            , 1       , (230,150,140) ),\n",
    "    Label(  'building'             , 11 ,    'construction'    , 2       , ( 70, 70, 70) ),\n",
    "    Label(  'wall'                 , 12 ,    'construction'    , 2       , (102,102,156) ),\n",
    "    Label(  'fence'                , 13 ,    'construction'    , 2       , (190,153,153) ),\n",
    "    Label(  'guard rail'           , 14 ,    'construction'    , 2       , (180,165,180) ),\n",
    "    Label(  'bridge'               , 15 ,    'construction'    , 2       , (150,100,100) ),\n",
    "    Label(  'tunnel'               , 16 ,    'construction'    , 2       , (150,120, 90) ),\n",
    "    Label(  'pole'                 , 17 ,    'object'          , 3       , (153,153,153) ),\n",
    "    Label(  'polegroup'            , 18 ,    'object'          , 3       , (153,153,153) ),\n",
    "    Label(  'traffic light'        , 19 ,    'object'          , 3       , (250,170, 30) ),\n",
    "    Label(  'traffic sign'         , 20 ,    'object'          , 3       , (220,220,  0) ),\n",
    "    Label(  'vegetation'           , 21 ,    'nature'          , 4       , (107,142, 35) ),\n",
    "    Label(  'terrain'              , 22 ,    'nature'          , 4       , (152,251,152) ),\n",
    "    Label(  'sky'                  , 23 ,    'sky'             , 5       , ( 70,130,180) ),\n",
    "    Label(  'person'               , 24 ,    'human'           , 6       , (220, 20, 60) ),\n",
    "    Label(  'rider'                , 25 ,    'human'           , 6       , (255,  0,  0) ),\n",
    "    Label(  'car'                  , 26 ,    'vehicle'         , 7       , (  0,  0,142) ),\n",
    "    Label(  'truck'                , 27 ,    'vehicle'         , 7       , (  0,  0, 70) ),\n",
    "    Label(  'bus'                  , 28 ,    'vehicle'         , 7       , (  0, 60,100) ),\n",
    "    Label(  'caravan'              , 29 ,    'vehicle'         , 7       , (  0,  0, 90) ),\n",
    "    Label(  'trailer'              , 30 ,    'vehicle'         , 7       , (  0,  0,110) ),\n",
    "    Label(  'train'                , 31 ,    'vehicle'         , 7       , (  0, 80,100) ),\n",
    "    Label(  'motorcycle'           , 32 ,    'vehicle'         , 7       , (  0,  0,230) ),\n",
    "    Label(  'bicycle'              , 33 ,    'vehicle'         , 7       , (119, 11, 32) ),\n",
    "    Label(  'license plate'        , -1 ,    'vehicle'         , 7       , (  0,  0,142) ),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important note is that I can include only some classes in the evaluation matric and some classes not include, but in this project I want to include all of them, and challange myself to build a model that give good performance to all the classes.\n",
    "In industery applications this is will be more smart to include sometimes not all the classes, but only the classes that are the most important to us. In addition I include the columns catagory and catagory id in case if we will want to evalutate our model on spesific catagory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5-3'></a>\n",
    "### 5.3 - Process path\n",
    "In this section I will process the format and the type of the datasets.\n",
    "I will decode each img and convert her to floast32 type.\n",
    "\n",
    "This is essential process because in order our model will work good and will not happen bugs, we need that all the images in our datasets will be in the same format and with the same type.\n",
    "\n",
    "In addition, I will resize each mask image, because now our mask's shape is (2048, 1024, 3), but for each pixel all his three chanels have the same value, because this is labal ids mask and not rgb mask for visualizetion.\n",
    "Thus we will resize each mask image to shape (2048, 1024), that mean now each value of pixel will be the label id of the pixel, according section 5.2 .\n",
    " \n",
    "We will convert all the images in our datasets to png format and to float32 type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_path(image_path, ground_truth_path):\n",
    "    img = tf.io.read_file(image_path)  \n",
    "    img = tf.image.decode_png(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32) \n",
    "\n",
    "    ground_truth_img = tf.io.read_file(ground_truth_path)  \n",
    "    ground_truth_img = tf.image.decode_png(ground_truth_img, channels=1)\n",
    "\n",
    "    return img, ground_truth_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_dataset_before_pre_processing = train_image_dataset_before_path_processing.map(process_path)\n",
    "dev_image_dataset_before_pre_processing = dev_image_dataset_before_path_processing.map(process_path)\n",
    "test_image_dataset_before_pre_processing = test_image_dataset_before_path_processing.map(process_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5-4'></a>\n",
    "### 5.4 - Explore dataset\n",
    "In this section I will explore our dataset, in order I will get better familiar with the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, let's see the sizes of our datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_dataset_m = len(train_image_dataset_before_path_processing)\n",
    "dev_image_dataset_m = len(dev_image_dataset_before_path_processing)\n",
    "test_image_dataset_m = len(test_image_dataset_before_path_processing)\n",
    "NUM_CLASSES = 34\n",
    "print(\"Train dataset size is: \" + str(train_image_dataset_m))\n",
    "print(\"Dev dataset size is: \" + str(dev_image_dataset_m))\n",
    "print(\"Test dataset size is: \" + str(test_image_dataset_m))\n",
    "print(\"Number of classes is: \" + str(NUM_CLASSES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start plot some examples of images and their corresponding masks from the train dataset,\n",
    "we need to create a function that can convert a mask to a visualization mask.\n",
    "\n",
    "What this is mean?\n",
    "So, our masks are label ids mask, that mean each pixel in the mask have only one value, that represents his label id, according section 5.2.\n",
    "Now in order visulaize our mask we need to convert each label id to his corresponding color, according section 5.2.\n",
    "\n",
    "This function is importent, because visualization mask give us better understanding of the segmantation,\n",
    "and for pepole that does not knows what is label ids(like pepole that not understanding DL) they are can better understanding the segmantation with visualization mask.\n",
    "\n",
    "In summary, visualization mask is more beautiful :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_mask_to_visualization_mask(ground_truth_img, labels):\n",
    "    if ground_truth_img.ndim == 3:\n",
    "        height, width, _ = ground_truth_img.shape\n",
    "        new_ground_truth_img = tf.math.reduce_max(ground_truth_img, axis=-1, keepdims=False)\n",
    "    else:\n",
    "        height, width = ground_truth_img.shape\n",
    "        new_ground_truth_img = ground_truth_img\n",
    "    visualize_ground_truth_img = np.zeros((height, width, 3), dtype=int)\n",
    "    for label in labels:\n",
    "        visualize_ground_truth_img[new_ground_truth_img[:,:] == label.get_id(), :] = label.get_color()\n",
    "    return visualize_ground_truth_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot some exampels of images and their corresponding visualization masks from the train dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_example(img, ground_truth_img):\n",
    "    fig, arr = plt.subplots(1, 2, figsize=(14, 10))\n",
    "    arr[0].imshow(img)\n",
    "    arr[0].set_title('Image')\n",
    "    arr[1].imshow(ground_truth_img)\n",
    "    arr[1].set_title('Segmentation')\n",
    "\n",
    "\n",
    "AMOUNT_OF_EXAMPELS_TO_PLOT = 3 \n",
    "for img, ground_truth_img in train_image_dataset_before_pre_processing:\n",
    "    if AMOUNT_OF_EXAMPELS_TO_PLOT:\n",
    "        visualize_ground_truth_img = convert_mask_to_visualization_mask(ground_truth_img, labels)\n",
    "        plot_example(img, visualize_ground_truth_img)\n",
    "        AMOUNT_OF_EXAMPELS_TO_PLOT -= 1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5-5'></a>\n",
    "### 5.5 - Preprocessing the dataset\n",
    "In this section I will cover the part of Preprocessing the dataset that include:\n",
    "* Resize our dataset\n",
    "* Normalize our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5-5-1'></a>\n",
    "#### 5.5.1 - Why we need to resize our dataset?\n",
    "Our dataset contains images with dimensions of 2048x1024 pixels, that mean we are handle with large images.\n",
    "Most of the models architectures in DL are not suitable for images of this size.\n",
    "\n",
    "However we can change these models architectures in order they will be suitable for images of this size,\n",
    "but the result of this operation is to create very big and deep architectures what lead to higher computational cost and higher memory storage.\n",
    "Also, usually big and deep architectures tends to be with higher chance for overfitting, because they are more complex.\n",
    "There are other problems with large and deep architectures such as vanishing and exploding gradients (because they are deep).\n",
    "\n",
    "Now let's recall our problem. Our problem is to do semantic segmentation for self-driving car, so we need to build model for Real-Time problem.\n",
    "Thus, we need to create model with the lowest Computational Cost an the lowest storage as we can.\n",
    "And of course, we not want to create model with higher chance for overfitting, because we need that our model has a succseful adaptive for new images and has high accuarcy on the dev dataset and test dataset.\n",
    "\n",
    "Thus, we can not use large images because this leed to problem, that can leed our model to slower performence in Real-Time and bad adaptive for new images, what can lead to for accidents(let's recal our problem is for self-driving car, so we need to create fastest and safe(make as few mistakes as possible) model).\n",
    "\n",
    "I have not large memory storage and large Computational Resoreces, and therefore I will resize the images to $ 128 \\times 128 $ pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5-5-2'></a>\n",
    "#### 5.5.2 - Why we need to normalize our dataset?\n",
    "Our dataset is not normalized in the beginning and we need to normalize him,\n",
    "that mean to do opeartion that will lead that all our pixels in the images on the dataset will be in the same scale and between 0 and 1 inclusive.\n",
    "\n",
    "But why we need to do this? \n",
    "1. **Sigmoid and tanh activations:** \n",
    "\n",
    "    Sigmoid and tanh activations are sometimes used in DL(usually sigmoid used in the output layer) and they are look like this:\n",
    "    <div style=\"text-align:center\">\n",
    "    <img src=\"Images/Sigmoid.webp\" style=\"width:500px;height:250px;\">\n",
    "    <img src=\"Images/tanh.webp\" style=\"width:500px;height:250px;\">\n",
    "    </div>\n",
    "    <caption><center> <u><b>Figure 6</u></b>: Sigmoid and tanh functions <a href=\"https://medium.com/@toprak.mhmt/activation-functions-for-deep-learning-13d8b9b20e\">(Source)</a> <br> </center></caption>\n",
    "\n",
    "    We can see that when z is become larger and larger number or become smaller and smaller number the slope of the Sigmoid and tanh functions gets closer and closer to zero, that mean:\n",
    "    \n",
    "    When f(z) = sigmoid(z) and g(z) = tanh(z) then:\n",
    "     $$\\lim_{{z \\to +-\\infty}} \\frac{df(z)}{dz} = 0$$ \n",
    "     $$\\lim_{{z \\to +-\\infty}} \\frac{dg(z)}{dz} = 0$$\n",
    "\n",
    "    So if our pixels in the images on the dataset will be between 0 and 255 inclusive so there is a bigger chance that z will be larger(in different layers) and thus the slope of the Sigmoid and tanh functions gets around zero, which will lead that the gradients will be around zero and thus the network will be more slowly learn the optimal parameters for the network.\n",
    "\n",
    "    We want that the network will learn the optimal parameters for the network, as fast as possible, and thus we need that all our pixels in the images on the dataset will be between 0 and 1 inclusive(in this way the slope of the sigmoid and tanh functions probably will not gets around zero, at least in the beginning of the learning). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Lower cost function and gradients**\n",
    "\n",
    "    If our pixels in the images on the dataset will not be between 0 and 1 inclusive, but will be between 0 and 255 inclusive, so if we using Relu(what very commonly used in DL) also the activations values will be huge and thus after the forward pass we will end up with hugh loss value and hugh gradients values.\n",
    "\n",
    "    Hugh loss value will lead the network be more slowly learn the optimal parameters for the network.\n",
    "    \n",
    "    Hugh gradients values can lead that the cost function not decrease after each iteration, because in the update parameters part we will change each parameter with relativly big value. \n",
    "    We can try prevent this with set very small learning rate, but this will cause to the network be more slowly learn the optimal parameters for the network, and this is not good.\n",
    "\n",
    "    The solution of this problem is to do that all our pixels in the images on the dataset will be between 0 and 1 inclusive.\n",
    "\n",
    "    <div style=\"text-align:center\">\n",
    "    <img src=\"Images/Relu.webp\" style=\"width:500px;height:250px;\">\n",
    "    </div>\n",
    "    <caption><center> <u><b>Figure 7</u></b>: Relu function <a href=\"https://medium.com/@toprak.mhmt/activation-functions-for-deep-learning-13d8b9b20e\">(Source)</a> <br> </center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Avoid weird mathematical artifacts with floating-point number precision**\n",
    "\n",
    "    If our pixels in the images on the dataset will not be between 0 and 1 inclusive, but will be between 0 and 255 inclusive, so if we using Relu(what very commonly used in DL) also the activations values will be huge, and this can lead us to handle mathematical operations in our network on really large or really small numbers.\n",
    "\n",
    "    When we handle mathematical operations on really large or really small numbers we can loss information and lose accuracy, because computers lose accuracy when performing math operations on really large or really small numbers.\n",
    "\n",
    "    The solution of this problem is to do that all our pixels in the images on the dataset will be between 0 and 1 inclusive(now we will work on numbers that are not really large and not really small numbers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Fastest learning**\n",
    "\n",
    "    If our pixels in the images on the dataset will not be in the same scale so the parameters that associated with each pixel will be in different scale.\n",
    "\n",
    "    Assume that we have 2 pixels and they are have 2 parameters that associated with them. They(the parameters) are will be in different scale and the cost function will be like in this image:\n",
    "\n",
    "    <div style=\"text-align:center\">\n",
    "    <img src=\"Images/cost_func_with_diff_scale.png\" style=\"width:500px;height:250;\">\n",
    "    </div>\n",
    "    <caption><center> <u><b>Figure 8</u></b>: Cost function with parameters that have different scale <a href=\"https://www.coursera.org/specializations/deep-learning\">From andraw ng course</a> <br> </center></caption>\n",
    "\n",
    "\n",
    "    Let's recall that we want to minimize the cost function, but with this cost function the minimize process will be more slower, because each parmeter has different scale, and thus if we want our cost function to converge we need set our learning rate to small number.\n",
    "\n",
    "    Setting our learning rate to small number will cause to the network be more slowly learn the optimal parameters for the network, and this is not good. \n",
    "\n",
    "    Thus, we need that our pixels in the images on the dataset will be in the same scale, and in this situation our cost function will probably looks like this(with two parameters):\n",
    "\n",
    "    <div style=\"text-align:center\">\n",
    "    <img src=\"Images/cost_func_with_same_scale.png\" style=\"width:500px;height:250;\">\n",
    "    </div>\n",
    "    <caption><center> <u><b>Figure 9</u></b>: Cost function with parameters that have same scale <a href=\"https://www.coursera.org/specializations/deep-learning\">From andraw ng course</a> <br> </center></caption>\n",
    "\n",
    "    With this cost function the minimize process will be more faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5-5-3'></a>\n",
    "#### 5.5.3 - Code Preprocessing function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two possible methods for normalizing the input.  \n",
    "\n",
    "**1.** We will divide each pixel by 255, and the result of this is that all our pixels in the images on the dataset will be in the between 0 and 1 inclusive.\n",
    "\n",
    "**2.** \n",
    "Compute the mean across the entire dataset:\n",
    "\n",
    "$Mean = \\frac{1}{m} \\sum_{i=1}^{m} X_i$ , when $X_i$ is the i img in the dataset. \n",
    "\n",
    "Compute the variance across the entire dataset:\n",
    "\n",
    "$Var = \\frac{1}{m} \\sum_{i=1}^{m} (X_i - Mean)^2$\n",
    "\n",
    "And compute:\n",
    "\n",
    "$X_i = \\frac{X_i - Mean}{\\sqrt{Var}}$, when $X_i$ is the i img in the dataset. \n",
    "\n",
    "In this project we will experiment the two methods. One important thing that it is worth noting is that the second method for normalizing the input can cause to negative values of pixels, but we still can use her, because we enter this to our model, that can handle with this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly we will resize our images in the datasets.\n",
    "\n",
    "We will resize our images to $ 128 \\times 128 $ pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (128, 128, 3)\n",
    "TARGET_SIZE_TO_RESIZE = (128, 128)\n",
    "\n",
    "def resize_dataset(img, ground_truth_img):\n",
    "    resized_img = tf.image.resize(img, TARGET_SIZE_TO_RESIZE, method=tf.image.ResizeMethod.LANCZOS3)\n",
    "    resized_ground_truth_img = tf.image.resize(ground_truth_img, TARGET_SIZE_TO_RESIZE, method='nearest')\n",
    "    return resized_img, resized_ground_truth_img\n",
    "\n",
    "\n",
    "train_image_dataset_before_normalize = train_image_dataset_before_pre_processing.map(resize_dataset)\n",
    "dev_image_dataset_before_normalize = dev_image_dataset_before_pre_processing.map(resize_dataset)\n",
    "test_image_dataset_before_normalize = test_image_dataset_before_pre_processing.map(resize_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the mean and the variance of the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_example_variance(example, dataset_size):\n",
    "    img = example[0]\n",
    "    variance = tf.math.reduce_variance(img)\n",
    "    return variance / dataset_size\n",
    "\n",
    "\n",
    "def compute_example_mean(example, dataset_size):\n",
    "    img = example[0]\n",
    "    return tf.cast(img, tf.float32) / dataset_size\n",
    "\n",
    "train_dataset_mean = train_image_dataset_before_normalize.reduce(\n",
    "    tf.zeros(INPUT_SHAPE, dtype=tf.float32), lambda x, example: x + compute_example_mean(example, train_image_dataset_m))\n",
    "dev_dataset_mean = dev_image_dataset_before_normalize.reduce(\n",
    "    tf.zeros(INPUT_SHAPE, dtype=tf.float32), lambda x, example: x + compute_example_mean(example, dev_image_dataset_m))\n",
    "test_dataset_mean = test_image_dataset_before_normalize.reduce(\n",
    "    tf.zeros(INPUT_SHAPE, dtype=tf.float32), lambda x, example: x + compute_example_mean(example, test_image_dataset_m))\n",
    "\n",
    "train_dataset_variance = train_image_dataset_before_normalize.reduce(\n",
    "    0.0, lambda x, example: x + compute_example_variance(example, train_image_dataset_m))\n",
    "dev_dataset_variance = dev_image_dataset_before_normalize.reduce(\n",
    "    0.0, lambda x, example: x + compute_example_variance(example, dev_image_dataset_m))\n",
    "test_dataset_variance = test_image_dataset_before_normalize.reduce(\n",
    "    0.0, lambda x, example: x + compute_example_variance(example, test_image_dataset_m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataset_first_version(img, ground_truth_img):\n",
    "    normalized_img = img / 255\n",
    "    return normalized_img, ground_truth_img\n",
    "\n",
    "\n",
    "def normalize_dataset_second_version(img, ground_truth_img, dataset_mean, dataset_variance):\n",
    "    normalized_img = img - dataset_mean\n",
    "    normalized_img /= tf.sqrt(dataset_variance)\n",
    "    return normalized_img, ground_truth_img\n",
    "\n",
    "\n",
    "train_dataset_before_normalize_first_version = copy.deepcopy(train_image_dataset_before_normalize)\n",
    "dev_dataset_before_normalize_first_version = copy.deepcopy(dev_image_dataset_before_normalize)\n",
    "test_dataset_before_normalize_first_version = copy.deepcopy(test_image_dataset_before_normalize)\n",
    "\n",
    "train_dataset = train_dataset_before_normalize_first_version.map(normalize_dataset_first_version)\n",
    "dev_dataset = dev_dataset_before_normalize_first_version.map(normalize_dataset_first_version)\n",
    "test_dataset = test_dataset_before_normalize_first_version.map(normalize_dataset_first_version)\n",
    "\n",
    "train_dataset_second_version = train_image_dataset_before_normalize.map(\n",
    "    lambda img, ground_truth_img: normalize_dataset_second_version(img, ground_truth_img, train_dataset_mean, train_dataset_variance))\n",
    "dev_dataset_second_version = dev_image_dataset_before_normalize.map(\n",
    "    lambda img, ground_truth_img: normalize_dataset_second_version(img, ground_truth_img, dev_dataset_mean, dev_dataset_variance))\n",
    "test_dataset_second_version = test_image_dataset_before_normalize.map(\n",
    "    lambda img, ground_truth_img: normalize_dataset_second_version(img, ground_truth_img, test_dataset_mean, test_dataset_variance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important thing that it is worth noting is that copy.deepcopy not alwayes work in all the computers, so we can use other alternative method that will copy our dataset that if we will do changes on this copy this not affect on the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5-6'></a>\n",
    "### 5.6 - Data augmentation\n",
    "In this section I will cover the part of data augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5-6-1'></a>\n",
    "#### 5.6.1 - What is Data augmentation and why we use this?\n",
    "Overfitting is a common problem for deep neural networks.\n",
    "Neural networks are often very big and deep relative to our dataset,\n",
    "what means that neural networks often have more parameters than we really need for our size of dataset.\n",
    "\n",
    "As result of this, neural networks often over fitting our dataset, that is they can memorize some unimportent features of our dataset, instead of learn some genral and useful information about our dataset and genralize this information to other examples that she does not saw.\n",
    "Thus, when we give to our neural network new, real-world data that he never saw, he fail to yield useful results.\n",
    "\n",
    "There are some techniques to address overfitting like dropout, regularization, early stopping, get bigger dataset and data augmentation.\n",
    "\n",
    "In this project we will discuss about data augmentation.\n",
    "\n",
    "Data augmentation is a technic to address overfitting, and what she offer is to \"augment\" our training dataset.\n",
    "\n",
    "What this mean? \"Augment\" our training dataset mean to take our training dataset and on each iteration of our model do for each example an operation like: randomly flipping horizontally, shifting their hues, cropping random sections and more.\n",
    "In this way we increasing the amount of information we have, and on each iteration our model will randomly change the current example with one or more types of data augmentation.\n",
    "\n",
    "The result of this is our model need to be more genral because now he have more data information.\n",
    "For example if I train a network to recognize cat and I have example with cat facing right,\n",
    "if I will do on her flipping horizontally, so the model will learn that cat is cat, regardless of orientation.\n",
    "In this way our model must to learn genral and useful information about our dataset, what will genralize this information to other examples that he does not saw.\n",
    "\n",
    "Of course, that to add more new data is better way to cause our model to generalize itself, but in the most of cases add more new data is expensive, so data augmentation is good solution.\n",
    "\n",
    "In addition, model genralzation is very important in real-world problems, because many datasets contain images from many sources, taken from different cameras in various conditions. Thus networks need to generalize over many factors to perform well. \n",
    "Some factors for example are lighting, scale, camera conditions and more.\n",
    "With data augmentation we can cause our model to genralize over all of these factors.\n",
    "For example if I train a network to recognize cat and I have example with cat in image with a lot of light with data augmentation we can cause our model to understand that lighting conditions not detrmine whether it is a cat in the picture or not.Thus cat in image that more darker, is also cat.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images/data_augmentation_exampels.png\" style=\"width:500px;height:250;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 10</u></b>: Data augmentation exampels <a href=\"https://medium.com/@tagxdata/data-augmentation-for-computer-vision-9c9ed474291e\">(Source)</a> <br> </center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5-6-2'></a>\n",
    "#### 5.6.2 - Why some data augmentation techniques is not good for self-driving car?\n",
    "In self-driving car we have consistent data, because cars generally have consistent pose with respect to other vehicles and road objects. For example car alwayes will be in the right side of the road and the camera that took the pictures in the data set that we have, always in the same position, orientation and zoom.\n",
    "Thus all our data always will be from the same system that have consistent camera and features.\n",
    "\n",
    "This is because in self-driving car we collect our data with the same sensor system as will be used in production, and therefore we alwayes will have the some properties that will be the same in every image.\n",
    "\n",
    "Thus we not need that our neural network genralize these properties.\n",
    "For example we don't need that our neural network genralize to fliped, croped and rotationed images,\n",
    "beacuse always our camera will be in the same position, orientation and zoom.\n",
    "Therefore our neural network will never get images that fliped, croped or rotationed.\n",
    "\n",
    "**And now we come to out conclusion:**\n",
    "\n",
    "If we still do data augmentation like flip, crop or rotation this is will hurt the performance of our neural network because we take some of the resources of our neural network and assign them to genralize our neural network for fliped, croped and rotationed images.\n",
    "But the problem is that our neural network will never get images that fliped, croped or rotationed,\n",
    "and therefore we wasted a lot of resources for something that will never happend.\n",
    "The result of this is that our neural network will have more worse performance(because she not use all her resources), and when we will do not use data augmentation like flip, crop or rotation, the neural network will have more good performance.\n",
    "\n",
    "This is beutiful problem because, always in the internet we are told us ,that data augmentation only can improve and help to our neural network get better preformance on real-world data and production, but this is not right in all situations.\n",
    "\n",
    "In genral, in DL field there are many tips and rules of thumb about things, like that overfitting is bad, how to chosse some hyperparaters(some rules of thumb for default recommended values), but one important thing\n",
    "that we always should to remember is to these tips and rules of thumb does not right to all situations, and do not rely on them.\n",
    "\n",
    "In summary I think this section is very interesting, and I was very surprised to discover that sometimes overfitting is very good thing, and try to address overfitting only leads to worse performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5-6-3'></a>\n",
    "#### 5.6.3 - Which data augmentation techniques is still good for self-driving car?\n",
    "There are some data augmentation techniques that are still good for self-driving car.\n",
    "For example hue jitter augmentation technique is good for self-driving car because she has not affect camera properties(that are consistent). This is help to our nerual network to genralize the color of objects in the image, like car. This result of this, is that our nerual network understand that a red car and a blue car should both be detected the same, and the color is not important.\n",
    "\n",
    "There is more data augmentation techniques that are still good for self-driving car, like random contrast, random brightness, random saturation and more, for reasons similar to those we described earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5-6-4'></a>\n",
    "#### 5.6.4 - Code data augmentation\n",
    "\n",
    "Let's code our data augmenters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataAugmentationLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, augmentation_func, data_augmentation_prob, **kwargs):\n",
    "        super(DataAugmentationLayer, self).__init__(**kwargs)\n",
    "        self.__augmentation_func = augmentation_func\n",
    "        self.__data_augmentation_prob = data_augmentation_prob\n",
    "\n",
    "    @tf.autograph.experimental.do_not_convert\n",
    "    def call(self, input_data, training):\n",
    "        apply_augmentation = np.random.uniform() < self.__data_augmentation_prob\n",
    "        if apply_augmentation and training:\n",
    "            augmented_data = self.__augmentation_func(input_data)\n",
    "        else:\n",
    "            augmented_data = input_data \n",
    "        return augmented_data\n",
    "\n",
    "def basic_data_augmentation(input_data):\n",
    "    augmented_data = tf.image.random_hue(input_data, max_delta=0.1)\n",
    "    return augmented_data\n",
    "\n",
    "def advance_data_augmentation(input_data):\n",
    "    augmented_data = tf.image.random_hue(input_data, max_delta=0.1)\n",
    "    augmented_data = tf.image.random_brightness(augmented_data)\n",
    "    augmented_data = tf.image.random_contrast(augmented_data)\n",
    "    return augmented_data\n",
    "\n",
    "def bad_data_augmentation(input_data):\n",
    "    augmented_data = tf.image.random_flip_left_right(input_data)\n",
    "    augmented_data = tf.image.random_flip_up_down(augmented_data)\n",
    "    return augmented_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5-7'></a>\n",
    "### 5.7 - Divide our train dataset to mini batches and shuffle our train dataset\n",
    "\n",
    "<span style=\"border-bottom: 2px solid blue\">Divide our train dataset to mini batches:</span>\n",
    "\n",
    "Beacuse our training dataset is big(2085 images), we need to divide him to mini batches.\n",
    "If we will not do this, so for each iteration in gradient deceant we will pass over 2085 images, that mean update our parmeters after every time we go through 2085 images, that is do forward propogation and backward propogation of 2085 images. This is storage costs a lot, and this slow down our learning process because we update our parmeters after every time we go through 2085 images, that is large amount of images.\n",
    "Thus for example after 100 iterations we will pass over $ 2085 \\times 100 = 208,500$ images, but do only 100 updates to our parameters. Probably 100 updates to our parameters is not enogth to get a good result, but we still wasted expensive resources in terms of calculation, which is not good for us.\n",
    "\n",
    "In conclusion, we are wasted expensive resources in terms of calculation for 100 iterations, but not get good result.\n",
    "\n",
    "The solution of this is divide our training dataset to mini batches, that mean we divide our training dataset to mini groups of data, and pass over each group individually and update the parameters, and like this until we finish going through all the mini groups.\n",
    "\n",
    "In this way we update the parameters more times after each pass over all the training dataset,\n",
    "and this is more effinantly and speed our neural networks. In addition pass over mini group is more effienct in terms of storage than pass over the entire dataset. \n",
    "\n",
    "I have not large memory storage and large Computational Resources, and therefore I will choose mini batch size of 32, and during the project I will change this if I have to.\n",
    "\n",
    "The pros of mini batch size that is 32 are that our learning procees more faster, because we do a lot of updates to our parameters after we pass all over our training dataset, and that we use less memory(this is good when you are have limited hardware or larger models).\n",
    "\n",
    "The cons of mini batch size that is 32 are that in the learning process may be noisier gradients, and this can lead us to slower convergence and require do more training iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"border-bottom: 2px solid blue\">Shuffle our train dataset:</span>\n",
    "\n",
    "After we divide our train dataset to mini batches, it's important to shuffle our train dataset in order each mini batch contain random data, that mean we want that each mini batch contain genral and diverse data.\n",
    "\n",
    "This is important thing because if spesific mini batch will contain only one type of data (for example only data from spesific city), our learning process will be not effecitve, and will not reflect all information.\n",
    "This will lead to that when we update the model parameters for the spesific mini batch, this update is worng, and does not contribute to our algorithm to minimize the cost function, because it only represents a very specific type of data.\n",
    "\n",
    "Thus, we shuffle our train dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start writing the code I want to comment on one more small thing.\n",
    "\n",
    "<span style=\"border-bottom: 2px solid blue\">Buffer size:</span>\n",
    "\n",
    "When we want to shuffle our train dataset we need to detrmine the buffer size.\n",
    "Buffer size refers to the number of images that are loaded into memory and shuffled at a time, that mean if our buffer size is 300 we load randomly 300 images from our train dataset and shuffled them. After this we take randomly amount of images from the 300 images, according our mini batch size.\n",
    "\n",
    "We need to use buffer size because if we have large dataset and low available memory so we can not loaded into memory our entire dataset and shuffled him at the same time.\n",
    "Thus, we use buffer size.\n",
    "\n",
    "As we increase the buffer size and will be closer and closer to our training dataset size we will shuffle more images and thus the chosse of the images from shuffled images, will be more randomly and genral(because for example if I chosse 32 images from 300 shuffled images it's less randomly and genral, than if I chosse 32 images from 900 shuffled images).\n",
    "\n",
    "In conclusion we need to have buffer size that will balance between available memory that we have and the need for randomness.\n",
    "\n",
    "I will start with buffer size of 2085, and during the project I will change this if I have to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's code this section:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will set initial sizes for BUFFER_SIZE and BATCH_SIZE and during the project I will experiment more values if I have to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 2085\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset.batch(BATCH_SIZE)\n",
    "train_dataset = train_dataset.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dev_dataset.batch(BATCH_SIZE)\n",
    "dev_dataset = dev_dataset.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "test_dataset.batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "train_dataset_second_version.batch(BATCH_SIZE)\n",
    "train_dataset_second_version = train_dataset_second_version.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dev_dataset_second_version.batch(BATCH_SIZE)\n",
    "dev_dataset_second_version = dev_dataset_second_version.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "test_dataset_second_version.batch(BATCH_SIZE)\n",
    "test_dataset_second_version = test_dataset_second_version.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5-8'></a>\n",
    "### 5.8 - Use prefetch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will use prefetch method that prevents a memory bottleneck that can occur when reading from disk. It save aside some amount of images(from our train dataset), and keeps this data ready for when it's needed.\n",
    "\n",
    "We can set the number of images that we save aside, or we can use `tf.data.experimental.AUTOTUNE` to choose the number of images that we save aside automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "dev_dataset = dev_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "train_dataset_second_version = train_dataset_second_version.prefetch(buffer_size=AUTOTUNE)\n",
    "dev_dataset_second_version = dev_dataset_second_version.prefetch(buffer_size=AUTOTUNE)\n",
    "test_dataset_second_version = test_dataset_second_version.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6'></a>\n",
    "## 6 - Unet explnation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section I will discuss about the Unet architecture.\n",
    "\n",
    "In my explanation I am going to use the paper <a href=\"https://arxiv.org/pdf/1505.04597.pdf\">U-Net: Convolutional Networks for Biomedical Image Segmentation</a> of the Unet authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6-1'></a>\n",
    "### 6.1 - What is Unet?\n",
    "Unet is architecture that published in 2015 by Olaf Ronneberger, Philipp Fischer, and Thomas Brox From University of Freiburg, Germany.\n",
    "\n",
    "Unet is very common arcitcthure and in the beginnig the purpose of the Unet authors was to find solution for mediacl segmentation application, and so Unet invented.\n",
    "\n",
    "The surprising thing that since 2015, in other fields of computer vision also started use Unet too, and discovered that Unet is powerful architecture.\n",
    "\n",
    "**But how the Unet authors thought about to do model such?**\n",
    "\n",
    "So, before 2015 we had good models for calssifcation images tasks.\n",
    "These models says as what there is an the image, but does not answer about important answer that is where are the objects(that we intersting them) in the image, i.e., we want that a class label to be assigned to each pixel in the image.\n",
    "\n",
    "Thus the localization problem raise. Before 2015, were already models for localization images tasks, but they are still was not very effective and did not given good results of semantic segmentation on smaller datasets.\n",
    "\n",
    "So the purpose of the Unet authors was to found model arcitcthure, that will be very effective \n",
    "and can give very good results of on segmentation on smaller datasets spesfily in biomedical tasks(remember that the Unet authors worked on mediacl problems), that have small datasets.\n",
    "\n",
    "Often, thousands of training images are usually beyond reach in biomedical tasks, because very difficult and expensive to achive this dataset.\n",
    "This is also true to a certain extent about other fields in computer vision, because in genral it is more difficult and more expensive to achive dataset for computer vision tasks compare to other fields in DL.\n",
    "\n",
    "Hence, this is very important to us to find model arcitcthures, that will can give very good results of different tasks on smaller datasets.\n",
    "\n",
    "**One more intersting question is what the unet aplications?**\n",
    "\n",
    "Some aplications of Unet are:\n",
    " * Image segmentation\n",
    " * Super resulotion, that is get lower resultion image and output higher resultion image\n",
    " * Diffusion models were transforming gausian noise to newly genrated images.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images/semantic segmentation.webp\" style=\"width:370px;height:300px;\">\n",
    "    <img src=\"Images/super_resulotion.png\" style=\"width:370px;height:300px;\">\n",
    "    <img src=\"Images/diffusion_models.jpg\" style=\"width:370px;height:300px;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 11</u></b>: <a href=\"https://medium.com/analytics-vidhya/introduction-to-semantic-image-segmentation-856cda5e5de8\">Image segmentation</a>, <a href=\"https://www.v7labs.com/blog/image-super-resolution-guide\">Super resulotion</a> and <a href=\"https://www.youtube.com/watch?app=desktop&v=fbLgFrlTnGU\">Diffusion models</a> examples from left to right <br> </center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6-2'></a>\n",
    "### 6.2 - Unet model details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6-2-1'></a>\n",
    "#### 6.2.1 - Unet Architecture\n",
    "\n",
    "The Unet architecture that described in the Unet papaer is:\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images/Unet_architecture.png\" style=\"width:700px;height:400;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 12</u></b>: Unet Architecture that was described in the Unet paper.\n",
    "\n",
    "Each blue box corresponds to a multi-channel feature map.\n",
    "The number of channels is denoted on top of the box.\n",
    "The x-y-size is provided at the lower left edge of the box.\n",
    "White boxes represent copied feature maps. The arrows denote the different operations.\n",
    "<a href=\"https://arxiv.org/pdf/1505.04597.pdf\">(Source)</a><br> </center></caption>\n",
    "\n",
    "In the originl paper, the Unet only uses the valid part of each convolution, i.e., the segmentation map only contains the pixels, for which the full context is available in the input image.\n",
    "In this project we will use padding, in order we will output the same image size as the input image size and we will refer to the information at the edges of the input image.\n",
    "\n",
    "The result of padding operation described in this image:\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images/padding_example_Unet.png\" style=\"width:500px;height:400;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 13</u></b>: Result of padding operation on image\n",
    "<a href=\"https://arxiv.org/pdf/1505.04597.pdf\">(Source)</a><br> </center></caption>\n",
    "\n",
    "\n",
    "We can see in the example that prediction of the segmentation in the yellow area, requires image data within the blue area as input. One possible solution for completing missing input data is to complete the missing input data by mirroring.\n",
    "\n",
    "In addition in this project in each blue arrow we will do also batch-normalization(after conv $3\\times3$ and ReLU).\n",
    "Another change that we will do in this project is to use Unet model on images with resolution of $286\\times572$.  \n",
    "\n",
    "In addition, Unet have encoder and decoder. The encoder is the left part of the Unet and he responsible for extract importent information and usful features from the image. \n",
    "The decoder is the right part of the Unet and he responsible for take this features back by starting returning to the size of the input image, and try to do perfect segmentation for each pixel. \n",
    "\n",
    "At the final layer a 1x1 convolution is used to map each 64-component feature vector to the desired number of classes.\n",
    "\n",
    "Another thing that worth noting is that Unet architecture is FCN, i.e. it only contains conv layers and does not contain any FC layer, and this is a strong advantage, that we will talk about him soon.\n",
    "\n",
    "In total the network has 23 conv layers, 4 connecting paths and 4 max-pooling layers.\n",
    "\n",
    "Another thing that worth noting is that we use ReLU after each conv layer but we can also use ELU.\n",
    "In this project I will use ReLU.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images/relu_vs_elu.png\" style=\"width:500px;height:400;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 14</u></b>: Fun image of ReLU versus ELU\n",
    "<a href=\"https://pallawi-ds.medium.com/understand-semantic-segmentation-with-the-fully-convolutional-network-u-net-step-by-step-9d287b12c852\">(Source)</a><br> </center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6-2-2'></a>\n",
    "#### 6.2.2 - Unet Encoder\n",
    "\n",
    "The encoder is a network that takes the input and outputs a feature map of the input.\n",
    "We can consider the encoder as just FCN that tries to understand what information the image has.\n",
    "\n",
    "There is 4 levels in the ecnoder, where that each level repated by two $ 3\\times3 $ conv(with padding) + Relu + Batch normalzation layers, and one $ 2\\times2 $ max pooling layer to downsample the resolution of the image. After each level we double the amount of the chanels.\n",
    "\n",
    "This mean that after each level in the encoder the resolution of the image decrease but the amount of the chanels doubles.\n",
    "\n",
    "This is give us levels with multi chanels, and this is very good because multi chanels gives us more fetures and more diverse information about the image. As bigger our resultion we have more detail informatoin about the fetures.\n",
    "\n",
    "The encoder purpose is to get a lot of fetures, and in the end to output feture map with many fetures, that each feture has relative small resolution. In this way the encoder can learn very much information about the image and learn complex realtionships in the image data. \n",
    "\n",
    "So until now the encoder is regular FCN and behaves like many FCN for understeend very well what is the image represents. In the last stage, instead to add dense layer and fully connected layers like reulgar CNN for classifcation tasks, the Unet use the bootlenack and after this the decoder.\n",
    "\n",
    "This is because for our purpose know what the image represents is not enough. We want to know where each object located and what the pixels of each object in the image.\n",
    "\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images/Unet_encoder.png\" style=\"width:500;height:500px;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 15</u></b>: Unet encoder from Unet paper\n",
    "<a href=\"https://arxiv.org/pdf/1505.04597.pdf\">(Source)</a><br> </center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6-2-3'></a>\n",
    "#### 6.2.3 - Unet Decoder\n",
    "\n",
    "The decoder is a network that takes as input the feature map and give us information where are the objects in the image, in that she upsample our bottleneck resolution to the original resolution.\n",
    "\n",
    "There is 4 levels in the decoder, where the first 3 levels repated by two $ 3\\times3 $ conv(with padding) + Relu + Batch normalzation layers, and one $ 2\\times2 $ Transpose conv/Deconvultion layer to upsample the resolution of the image. After each level we reduce by 2 times the amount of the chanels.\n",
    "In the final level we do two $ 3\\times3 $ conv(with padding) + Relu + Batch normalzation layers, and one $ 1\\times1 $ conv(with padding), in order to map each 64-component feature vector to the desired number of classes. \n",
    "\n",
    "In each level we get feature map from the connection path and we combine this with the output of the last Deconvultion layer. We will discuss about this in the next section.\n",
    "\n",
    "Deconvultion layer is the reverse of max pooling layer. She upsample the resolution of the image.\n",
    "We use Deconvultion layer because we want to get closer to the resolution of the input image.\n",
    "\n",
    "In the decoder in each level our resolution increase but the amount of chanels decrese, because we want to use the information that we found and combine this to come close the orginal resulation, and to know where each object located and what the pixels of each object in the image.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images/Unet_decoder.png\" style=\"width:500;height:500px;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 16</u></b>: Unet decoder from Unet paper\n",
    "<a href=\"https://arxiv.org/pdf/1505.04597.pdf\">(Source)</a><br> </center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6-2-4'></a>\n",
    "#### 6.2.4 - Unet Connecting paths\n",
    "\n",
    "In order to localize, features maps from the encoder are combined with the upsampled layers outputs from the decoder.\n",
    "\n",
    "In each level in the decoder we take the fetures of the symetrical part of the encoder, and concante them onto their opoosing level in the decoder.\n",
    "\n",
    "**A good question is why are we even thinking of doing this?**\n",
    "The answer is simple: in the encoder part we learn so much information about the image so why not to use this information in the decoder part and help to our decoder with extra information. In this way the conv layers can operate on both the decoder features and the encoder features.\n",
    "Connecting paths should provide the necessary detail in order to know the acuurate pixels that belonging to each object. We can recover more fine-grain detail with the addition of these skip connections,\n",
    "becuase in the encoder we learn useful information about our image.\n",
    "\n",
    "This is good becuase the encoder fetures can tell us for example more detail information of the pixels and the decoder can tell us what the area of the object. When we combining this we get more powerful information.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images/Unet_connecting_paths.png\" style=\"width:500;height:500px;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 17</u></b>: Unet connecting paths\n",
    "<a href=\"https://pallawi-ds.medium.com/understand-semantic-segmentation-with-the-fully-convolutional-network-u-net-step-by-step-9d287b12c852\">(Source)</a><br> </center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6-2-5'></a>\n",
    "#### 6.2.5 - Unet Bottlenack \n",
    "The lowest level in the Unet called bottlenack.\n",
    "\n",
    "This is the level that connect between the encoder and the decoder. \n",
    "\n",
    "The bootlenack repated by two $ 3\\times3 $ conv(with padding) + Relu + Batch normalzation layers, and one $ 2\\times2 $ Transpose conv/Deconvultion layer to upsample the resolution of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6-2-6'></a>\n",
    "#### 6.2.6 Intialization of the weights\n",
    "\n",
    "In deep networks with many convs layers, a good intialization of the weights is very important.\n",
    "Otherwise, for example if we set some of the weights to very big values and the set the other weights to very small values, we will get that some parts of the network will give excessive activations, while other parts will never contribute or will give very small activations.\n",
    "\n",
    "Another bad intialization is zero-intialziation that set all the weights to zero.\n",
    "This cause that all the parts in our network will compute exactly the same function, i.e, our network is not powrful, and there is really no difference between it compared to a simple learning algorithm like logistic regression for example.\n",
    "\n",
    "This is can cause bad accuracy and that our network will give bad semantic segmentation of the input image.\n",
    "\n",
    "Hence, we will use he-normal initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6-2-7'></a>\n",
    "#### 6.2.7 - Regulzation\n",
    "\n",
    "In Unet we need to care about regulzation in order our model will can adapt to new exampels that he did not saw and genralize.\n",
    "\n",
    "One regulzation technique that we covered is data augmentation, and we saw how to do data augmentation for self-driving car tasks.\n",
    "\n",
    "In addition, we will use Dropout that is famous technique for prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6-3'></a>\n",
    "### 6.3 - Pros and Cons of Unet\n",
    "\n",
    "In this section we will discuss about the Pros and the Cons of Unet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6-3-1'></a>\n",
    "#### 6.3.1 - Pros of Unet\n",
    "\n",
    "Here are several pros of U Net:\n",
    "\n",
    "* According the Unet paper, Unet can use the available annotated samples more efficiently and even if we have small dataset we can get very good results.\n",
    "\n",
    "* According the Unet paper, Unet is fast(on small resultion images). Segmentation of a 512x512 image takes less than a second on a recent GPU(in 2015 year).\n",
    "\n",
    "* Unet is an end-to-end fully convolutional network (FCN), because of which it can accept the training and testing images of any size.\n",
    "\n",
    "* Unet has unique structure that lead that unet very effective for tasks with high resulotion inputs and outputs.\n",
    "\n",
    "* Unet is a genral architecture, i.e, we can use many different encoders and decoders. For example we can use VGG-16 in the encoder and decoder or use other architecture. This is useful because we combine Unet architecture with other good architectures and can use Transfer Learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6-3-2'></a>\n",
    "#### 6.3.2 - Cons of Unet \n",
    "\n",
    "Here are several cons of U Net:\n",
    "\n",
    "* Unet can has optimization challenges especially when we have small train dataset. This is because Unet contains a lot of parameters and tuning them well is not simple task.\n",
    "\n",
    "* Unet can has not good performance on Imbalanced Data. We will discuss about this and will discuss how to try to address this later in the project.\n",
    "\n",
    "* Unet can overfit the train dataset especially when we have small train dataset. This is because Unet contains a lot of parameters and when we have small dataset this can cause to overfitting the train dataset. We can solve this with regulzation tecniques.\n",
    "\n",
    "* Unet can be computationally and memory expensive, when we need to deal with high resolution images. This can be challenging and impossible to use when we use limited GPU memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='7'></a>\n",
    "## 7 - Loss functions and evaluation metrics\n",
    "\n",
    "In this section I will disucss about loss functions and evaluation metrics for semantic segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='7-1'></a>\n",
    "### 7.1 - Pixel accuracy\n",
    "\n",
    "Pixel accuracy is a very simple evaluation metric for semantic segmentation.\n",
    "\n",
    "Pixel accuracy measures the overall accuracy of the segmentation.\n",
    "\n",
    "For each image we take the output of our network for the image, and in each pixel take the id of the class that have the highest probability that pixel will be this class.\n",
    "For example if I in spesific pixel, I have 4 classes and in the output of our network these are the probabilties for the pixel: 0.1, 0.2, 0.4, 0.3 , so the pixel will have the value 2 because the class with the id 2, has the highest probability that pixel will be this class.\n",
    "\n",
    "After this we have 2d array that each pixel in the array contain the id of the class that we predicted. Now we will use this formula for the evaluation of our prediction for the image:\n",
    "\n",
    "$Pixel\\hspace{0.1cm}Accuracy = \\frac{Number\\hspace{0.1cm}of\\hspace{0.1cm}correctly\\hspace{0.1cm}classified\\hspace{0.1cm}pixels}{Total\\hspace{0.1cm}number\\hspace{0.1cm}of\\hspace{0.1cm}Pixels}$ \n",
    "\n",
    "We compute number of correctly classified pixels in that we compare our 2d array that we get, to the ground-truth of the image.\n",
    "\n",
    "The pixel accuracy is between 0 and 1 and as closer the pixel accuracy to 1, so we probably better predicted our pixels classes. I say probably, because high pixel accuracy does not necessarily mean that our model is good. There some problems with pixel accuracy that we will discuss about them in the next section.\n",
    "\n",
    "If we want to compute the loss for spesific example(that is image), we will do:\n",
    "\n",
    "$Loss = 1 - Pixel\\hspace{0.1cm}Accuracy$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='7-2'></a>\n",
    "### 7.2 - What is the problem of pixel accuracy?\n",
    "\n",
    "Pixel accuracy has two main problems.\n",
    "The first problem is that we calcultate a rough estimate of our accuracy, and we do not consider the probabilities of each pixel to be the class we have chosen. For example if we chose that for sepsific pixel the prediceted id class is 3, that is important if the pixel had very high probabilty to be this class or low probabilty to be this class. \n",
    "\n",
    "When we consider the probabilities of each pixel to be the class we have chosen, the learning process is better, and we say to our model to  to be more sure about his decisions, and not only choose the right id of the class for each pixel, but also be as sure as possible if it is a correct decision(that is get right decisions and get the highest probabilities for them).\n",
    "\n",
    "Is like the differnce between softmax and hardmax.\n",
    "\n",
    "The second problem is class imbalance, that says the pixel accuracy can be very good, but can be some classes, that the model accuracy on them is very bad.\n",
    "\n",
    "This is often cause where there is class imbalance, i.e, there is classes that appear more in our dataset and there is calsses that appear less in our dataset. This problem is often cause in computer vision apllications.\n",
    "\n",
    "Because pixel accuracy is give overall accuracy on our modal on image, she does not good for situations like class imbalance.\n",
    "\n",
    "For example if we have 2 classes and our image resolution is $10\\times10$, and in the ground-truth the first class appear in 95 pixels and the second class appear in 5 pixels.\n",
    "If our model predicted all the pixels in the image that belongs to the first class as first class, and predicted all the pixels that belongs to the second class as first class, the pixel accuracy that we will have for our image is:\n",
    "\n",
    "$Pixel\\hspace{0.1cm}Accuracy = \\frac{Number\\hspace{0.1cm}of\\hspace{0.1cm}correctly\\hspace{0.1cm}classified\\hspace{0.1cm}pixels}{Total\\hspace{0.1cm}number\\hspace{0.1cm}of\\hspace{0.1cm}Pixels} = \\frac{95}{100} = 0.95 $ \n",
    "\n",
    "So we have very high accuracy but in actual our model is not good,  because our model predicted all the pixels in the image that belongs to the second class as first class.\n",
    "\n",
    "In addition, sometimes we need to give more importance to certain pixels in the images, because if our model will mistake in them this will be more worse, than he will mistake in other pixels. Pixel accuracy can not give more importance to certain pixels than others pixels, and this is a problem.\n",
    "\n",
    "We will cover the problem of giving more importance to certain pixels in the image in 7.5.2 section.\n",
    "\n",
    "Thus, we need to find other evaluation matrix that solves these problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='7-3'></a>\n",
    "### 7.3 - Sparse Categorical Cross entropy\n",
    "\n",
    "The Sparse Categorical Cross entropy is very common loss function for semantic segmentation.\n",
    "\n",
    "This is actually pixel-wise soft-max over the final output(i.e final feature map) of our network combined with the sparse cross entropy loss function.\n",
    "\n",
    "Let's defined the soft-max function:\n",
    "\n",
    "$ p_c(x) = \\frac {exp(\\hat{y}_c(x))}{\\sum_{c'=1}^{C} exp(\\hat{y}_{c'}(x))} $\n",
    "\n",
    "Where C is the amount of the classes, $ \\hat{y}_c(x) $ is the value that the pixel have in the final feature map of our network in the c feature chanel, and x is the coordinates of the pixel in our image.\n",
    "Of course, $ p_c(x) $ for spesific class c and the coordinates of the pixel x, represents the probability of x to be the c class. \n",
    "\n",
    "In addition, $ p_c(x) \\approx 1 $ when $ \\hat{y}_c(x) $ is the maximum value from all the values in the feature chanels for x(in the the final feature map), and $ p_c(x) \\approx 0 $ for all the other classes.\n",
    "\n",
    "This is good propertie, because thanks to this propertie the cross entropy loss function forces our model to give the higest value that he can to $ p_c(x) $ when c is the correct class, and the lowest value that he can to $ p_c(x) $ to all the other classes.\n",
    "In this way our model will learn the right class for each pixel, and will more sure with his decisions(because he will give more probability to the correct class and give approximate zero probabilty to the other classes).\n",
    "\n",
    "The sparse cross entropy loss function for specific image, when P contains all the pixels coordinates in the image is:\n",
    "\n",
    "$ L = - \\sum\\limits_{\\substack{x \\in P}} \\log(p_{g(x)}(x)) $\n",
    "\n",
    "Where g(x) is a function that gives the true class of x. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='7-4'></a>\n",
    "### 7.4 - Why still Sparse Categorical Cross entropy is not good enough?\n",
    "\n",
    "Firstly, the first problem that was in pixel accuracy solved, because the Sparse Categorical Cross entropy take in consider the probabilities of each pixel to be the class we have chosen.\n",
    "\n",
    "But the second problem that was in pixel accuracy not solved, i.e, the Sparse Categorical Cross entropy also not good for highly unbalanced segmentations and not good when we want to give more importance to certain pixels in the image, from reasons similar to what we described in section 7.2.\n",
    "\n",
    "In the next section we will cover other evaluation metrics and losses, that solves the problems of giving more importance to certain pixels in the image, and highly unbalanced segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='7-5'></a>\n",
    "### 7.5 - The solutions for highly unbalanced segmentations and giving importance to certain pixels\n",
    "\n",
    "In this section I will cover the solutions for highly unbalanced segmentations and giving importance to certain pixels.\n",
    "\n",
    "In this section I am helped with the paper <a href=\"https://arxiv.org/pdf/1707.03237v3.pdf\">Generalised Dice overlap as a deep learning loss function for highly unbalanced segmentations </a> of Carole H. Sudre, Wenqi Li, Tom Vercauteren, Sebastien Ourselin, and M. Jorge Cardoso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='7-5-1'></a>\n",
    "#### 7.5.1 - Weighted Sparse Categorical Cross entropy for each class\n",
    "\n",
    "Let's recall in the first part of our second problem that was in pixel accuracy and Sparse Categorical Cross entropy:\n",
    "\n",
    "They are not good for highly unbalanced segmentations.\n",
    "\n",
    "We discussed in section 7.2 about the highly unbalanced segmentation problem and why this important to solve this, and now we will solve this.\n",
    "\n",
    "Let's recall our formula for Sparse Categorical Cross entropy.\n",
    "The sparse cross entropy loss function for specific image, when P contains all the pixels coordinates in the image is:\n",
    "\n",
    "$ L = - \\sum\\limits_{\\substack{x \\in P}} \\log(p_{g(x)}(x)) $\n",
    "\n",
    "Now we want to give more importance to some classes that have very small data relative to other classes or some classes that more important for us, and we will do this with the function $w : \\{1,..., C\\} \\to \\mathbb{R}$ where C is the amount of the classes. Now, the new loss function, i.e, the Weighted Sparse Categorical Cross entropy(for each class) for specific image, when P contains all the pixels coordinates in the image is:\n",
    "\n",
    "$ L = - \\sum\\limits_{\\substack{x \\in P}} w(g(x))\\log(p_{g(x)}(x)) $\n",
    "\n",
    "In this loss function we will give different weights for different classes.\n",
    "\n",
    "Now, if our model will do mistake that related to class that have very small data relative to other classes or this class more important for us, so the loss function will be bigger than before(without w, i.e, in the original loss function), and in this way we will force our model to learn the minority classes or important classes.\n",
    "\n",
    "In this way, we solved the problem of highly unbalanced segmentations.\n",
    "\n",
    "But, how we create the w function, and know that it is a good function?\n",
    "\n",
    "So, we will pass over all the ground-truth images in our train dataset, and count the amount of the pixels that belong to each class. After this we compute w(c) in this way:\n",
    "\n",
    "$ w(c) = \\frac{Total\\hspace{0.1cm} amount\\hspace{0.1cm} of\\hspace{0.1cm} pixels\\hspace{0.1cm} in\\hspace{0.1cm} our\\hspace{0.1cm} train\\hspace{0.1cm} dataset}{Amount\\hspace{0.1cm} of\\hspace{0.1cm} pixels\\hspace{0.1cm} in\\hspace{0.1cm} our\\hspace{0.1cm} train\\hspace{0.1cm} dataset\\hspace{0.1cm} that\\hspace{0.1cm} belongs\\hspace{0.1cm} to\\hspace{0.1cm} class\\hspace{0.1cm} c} $\n",
    "\n",
    "For example if we have train dataset that contains 10 images with resultion $ 10 \\times 10 $, have 2 classes A and B, and the amount of pixels in our train dataset that belongs to class A are 200, we will get:\n",
    "\n",
    "$ w(A) = \\frac{1000}{200} = 5 $, that give more impotance to pixels that belongs to the class A.\n",
    "\n",
    "In this way we give more importance to classes that have very small data relative to other classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='7-5-2'></a>\n",
    "#### 7.5.2 - Weighted Sparse Categorical Cross entropy for each pixel\n",
    "\n",
    "Let's recall in the second part of our second problem that was in pixel accuracy and Sparse Categorical Cross entropy:\n",
    "\n",
    "They are not good when we want to give more importance to certain pixels in the image.\n",
    "\n",
    "Firstly before we will find solution to this problem, we need to understand why and when we need to give more importance to certain pixels in the image.\n",
    "In order to understand this, I will give example:\n",
    "\n",
    "In Medicine applications often, there are very small details and little borders that separate different cells.  \n",
    "Hence, we need to build a model that will be very good with predicting successfully very small details and borders, because if not, we will do big mistakes, that can lead to a wrong decision and harm to the person we are treating.\n",
    "\n",
    "For example, the <a href=\"https://arxiv.org/pdf/1505.04597.pdf\"> Unet paper </a> show this example:\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images/Weighted_sparse_categorical_cross_entropy_for_each_pixel.png\" style=\"width:200;height:200px;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 18</u></b>: HeLa cells on glass recorded with DIC (differential interference contrast) microscopy. (a) raw image. (b) overlay with ground truth segmentation. Different colors\n",
    "indicate different instances of the HeLa cells. (c) generated segmentation mask (white:\n",
    "foreground, black: background). (d) map with a pixel-wise loss weight to force the\n",
    "network to learn the border pixels.\n",
    "<a href=\"https://arxiv.org/pdf/1505.04597.pdf\">(Source)</a><br> </center></caption>\n",
    "\n",
    "We can see that very important to prdict correctly the borders that separate different cells, and hence we give the pixels of the borders more importance than other pixels.\n",
    "In this way we force our model to learn and predict succesfully borders and little diteals,\n",
    "and we see that we get good prediction for our image.\n",
    "\n",
    "So, let's summarize our problem:\n",
    "\n",
    "We want to give more importance to certain pixels in the image that more important to us.\n",
    "\n",
    "After we recall our problem, let's recall our formula for Sparse Categorical Cross entropy.\n",
    "The sparse cross entropy loss function for specific image, when P contains all the pixels coordinates in the image is:\n",
    "\n",
    "$ L = - \\sum\\limits_{\\substack{x \\in P}} \\log(p_{g(x)}(x)) $\n",
    "\n",
    "Now we want to give more importance to some pixels in the image, and we will do this with the function $w : P \\to \\mathbb{R}$ where P contains all the pixels coordinates in the image. Now, the new loss function, i.e, the Weighted Sparse Categorical Cross entropy(for each pixel) for specific image, when P contains all the pixels coordinates in the image is:\n",
    "\n",
    "$ L = - \\sum\\limits_{\\substack{x \\in P}} w(x)\\log(p_{g(x)}(x)) $\n",
    "\n",
    "In this loss function we will give different weights for different pixels.\n",
    "\n",
    "Now, if our model will do mistake that related to pixel that very important to us(like the border that separate different cells in the medicial application that we discussed before), so the loss function will be bigger than before(without w, i.e, in the original loss function), and in this way we will force our model to learn and predict succesfully borders and little diteals.\n",
    "\n",
    "The Unet paper describe how to create the w function. I don't cover this in this project because we will dont use this loss function in the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='7-5-3'></a>\n",
    "#### 7.5.3 - Dice Coefficient and soft Dice Coefficient\n",
    "\n",
    "In this section I will discuss about another solution for highly unbalanced segmentations, that is Dice Coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='7-5-3-1'></a>\n",
    "##### 7.5.3.1 - What is Precision, Recall and F1 score?\n",
    "\n",
    "Firstly, before we will talk about Dice Coefficient we need to talk about Precision, Recall and F1 score.\n",
    "\n",
    "For sake of explanation, I will use example in this section, and will explain what is Recall, Precision and F1 score with this example.\n",
    "\n",
    "Let's assume that we work on the following medical application:\n",
    "\n",
    "We need to build a model that will predict if a patient is diagnosed with cancer or not, according some features. The relevant element is patient that with cancer.\n",
    "\n",
    "When we build this model we need a way to estimate how good our model predictions.\n",
    "\n",
    "So, the most trivial way to estimate how good our model predictions is accuarcy:\n",
    "\n",
    "$ Accuarcy = \\frac{Number\\hspace{0.1cm}of\\hspace{0.1cm}correctly\\hspace{0.1cm}classified\\hspace{0.1cm}patients}{Total\\hspace{0.1cm}number\\hspace{0.1cm}of\\hspace{0.1cm}patients} $\n",
    "\n",
    "The problem of this way is that we can get very good accuarcy on our model, but in actually our model is not good. For example, if we have 1000 patients that 998 of them are actually not with cancer(let's call this group A) and 2 of them are actually with cancer(let's call this group B), and our model predict correctly on group A, but on one patient from the two patients in group B our model predict that he is not have cancer, we will get that our model accuracy is:\n",
    "\n",
    "$ Accuarcy = \\frac{Number\\hspace{0.1cm}of\\hspace{0.1cm}correctly\\hspace{0.1cm}classified\\hspace{0.1cm}patients}{Total\\hspace{0.1cm}number\\hspace{0.1cm}of\\hspace{0.1cm}patients} = \\frac{999}{1000} = 99.9$\n",
    "\n",
    "Wow! This is amazing result! Our model is the best!\n",
    "\n",
    "Not so... If we will look on the amount of the patients that have cancer, our model predict them 50% correctly. This is very bad, because you tell to patient with cancer that he not have cancer, and risking his life.\n",
    "\n",
    "Hence, accuracy is not a good way to estimate how good our model predictions, and we need to find another way to estimate how good our model predictions.\n",
    "\n",
    "That's why we come to talk about Precision, Recall and F1 score.\n",
    "\n",
    "**So what is Precision?**\n",
    "\n",
    "Precision is how good the model avoids to predict patient as diagnosed with cancer where is not has cancer, that mean how good the model avoids to predict FP.\n",
    "Another way to think about accuracy is the probabilty that if I randomly selected patient that predicted with cancer is TP.\n",
    "\n",
    "The formula of Precision is:\n",
    "\n",
    "$ Precision = \\frac{TP}{TP + FP} $ \n",
    "\n",
    "\n",
    "**So what is Recall?**\n",
    "\n",
    "Recall is how good the model avoids to predict patient that has cancer as not diagnosed with cancer, that mean how good the model avoids to predict FN.\n",
    "Another way to think about recall is the probabilty that if I randomly selected patient that labeld with cancer(the patient has actually cancer), I will predict his as diagnosed with cancer, that mean TP.\n",
    "\n",
    "The formula of Recall is:\n",
    "\n",
    "$ Recall = \\frac{TP}{TP + FN} $\n",
    "\n",
    "So, if we want that our model will work good, we need that our model will have good score both in Recall and in Precision.\n",
    "Hence, we want to find score that will care of Recall and Precision.\n",
    "\n",
    "F1 score is good solution for this.\n",
    "\n",
    "**So what is F1 score?**\n",
    "\n",
    "F1 score take into account both Recall and Precision and try to balance both of them.\n",
    "It's the hermonic mean of Recall and Precision, and thus he tend to the lowest value between Recall and Precision. This is good, becuase if we have very good precision but have bad recall, this is not good and the F1 score give us score that tend to recall, i.e, bad score.\n",
    "\n",
    "The formula of F1 score is:\n",
    "\n",
    "$ F1 = \\frac{2}{\\frac{1}{Precision} + \\frac{1}{Recall}} = \\frac{2 \\times Precision \\times Recall}{Precision + Recall}$\n",
    "\n",
    "Hence, we have good way to estimate how good our model predictions. For example, for the example that we provided before with the 1000 patients, the F1 score is:\n",
    "\n",
    "$ F1 = \\frac{2 \\times Precision \\times Recall}{Precision + Recall} = \\frac{2 \\times 1 \\times 0.5}{1 + 0.5} = \\frac{1}{1.5} = \\frac{2}{3} $\n",
    "\n",
    "And this is very good estimate for our example, compare to the accuracy method that we had use(that give us 99.9).\n",
    "\n",
    "One last thing to note is that in some cases we will want to give more importance to Recall or Precision. For example, in the example that we described Recall is more important than Precision, because this is more dangerous to mistakly predict paitents with cancer as diagnose with no cancer, than to mistakly predict paitents that have not cancer as diagnose with cancer.\n",
    "\n",
    "If we want to give more importance to Recall or Precision, we will change the F1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='7-5-3-2'></a>\n",
    "#### 7.5.3.2 - Dice Coefficient explanation and formula explanation\n",
    "\n",
    "So, we discussed in the previous section about the importance of Precision, Recall and F1 score. Now we will talk about Dice Coefficient that based on Precision, Recall and F1 score.\n",
    "\n",
    "The Dice Coefficient is evalution matric for semantic segmentation image that is actually F1 score, but suited for semantic segmentation image problem. It may not sound so clear at the moment but now we will understand it better.\n",
    "\n",
    "Firstly, let's open the formula of F1 score so that it contains only TP, FN and FP expressions.\n",
    "\n",
    "Let's recall the F1 score formula:\n",
    "\n",
    "$ F1 = \\frac{2}{\\frac{1}{Precision} + \\frac{1}{Recall}} \\xrightarrow{\\text{Precision and Recall formulas}} \\frac{2}{\\frac{TP + FP}{TP} + \\frac{TP + FN}{TP}} = \\frac{2}{\\frac{2 \\times TP + FP + FN}{TP}} = \\frac{2 \\times TP}{2 \\times TP + FP + FN}$\n",
    "\n",
    "Now let's assume that we have problem that we need to do semantic segmentation only on one relvant element, that mean that we have two classes that is relvant class and not relvant class. For example to do semantic semgentation on images and classify each pixel as cat or not cat.\n",
    "\n",
    "So we have Y that represents the ground truth for our image and $ \\hat{Y} $ that represents our model prediction for our image.The Y and $ \\hat{Y} $ are 2d arrays that in each pixel in the array has value 0 or 1(for example indicates cat or not cat). We will want that the intersection between Y and $ \\hat{Y} $ will be bigger as possible and in actually the intersection will be as close as possible to the union between Y and $ \\hat{Y} $. If we will achive this, so this mean that for the relvant class(in our example is cat) the model predictions are very well.\n",
    "\n",
    "But now the question arises as to what union and intersection between Y and $ \\hat{Y} $ actually is in terms of TP, FN and FP expressions.\n",
    "So intersection between Y and $ \\hat{Y} $ is actually TP, and union is actually TP + FN + FP, because the union include the pixels that we predict as positive(in our example cat) correctly(this is actually the intersection between Y and $ \\hat{Y} $), the pixels that we think that are negative, but we were wrong about them(pixels in Y but not in $ \\hat{Y} $), and pixels that we think that are poitive, but we were wrong about them (pixels in $ \\hat{Y} $ but not in Y). \n",
    "\n",
    "Hence, the formulas of union and intersection between Y and $ \\hat{Y} $ in terms of TP, FN and FP expressions are:\n",
    "\n",
    "$ Intersection = TP $\n",
    "\n",
    "$ Union = TP + FN + FP $ \n",
    "\n",
    "and therefore:\n",
    "\n",
    "$ F1 = \\frac{2 \\times TP}{2 \\times TP + FP + FN} = \\frac{2 \\times Intersection}{Intersection + Union} $ \n",
    "\n",
    "And like we said before Dice Coefficient is evalution matric for semantic segmentation image that is actually F1 score. Hence:\n",
    "\n",
    "$ Dice \\hspace{0.1cm} Coefficient = \\frac{2 \\times Intersection}{Intersection + Union} $ \n",
    "\n",
    "Okey, we get formula for Dice Coefficient, but we still have problem.\n",
    "In semantic segmentation $ \\hat{Y} $ is not 2d array that each pixel in the array has value 0 or 1(for example indicates cat or not cat), but 2d array that each pixel in the array has probability that the pixel will be 1 value(for our example the probability that the pixel will be cat), so we need a way to compute the Intersection and the Union, when according what we were said before:\n",
    "\n",
    "$ Intersection = TP $\n",
    "\n",
    "$ Union = TP + FN + FP $ \n",
    "\n",
    "We can compute this for Y and $ \\hat{Y} $ in this way:\n",
    "\n",
    "$ Dice \\hspace{0.1cm} Coefficient = \\frac{2 \\times Intersection}{Intersection + Union} = \\frac{2 \\times \\sum\\limits_{\\substack{y,\\hat{y} \\in P}} y \\times \\hat{y}}{\\sum\\limits_{\\substack{y,\\hat{y} \\in P}} (y + \\hat{y})}$ \n",
    "\n",
    "When P contains all the pixels coordinates in the image, and in each sum y and \\hat{y} on the same pixel coordinate. \n",
    "Pay attention that when we sum $ y + \\hat{y} $, we sum the intersecion between y and \\hat{y} twice and therefore we don't need to add the intersection.\n",
    "\n",
    "Finally, how we are use Dice Coefficient when we have semantic segmentation problem on more than one relevant class?\n",
    "\n",
    "The answer is simple: We will compute Dice Coefficient on each class and take the average. \n",
    "The formula for this is:\n",
    "\n",
    "$ Dice \\hspace{0.1cm} Coefficient \\hspace{0.1cm} for \\hspace{0.1cm} K \\hspace{0.1cm} classes = \\frac{\\sum_{k=1}^{K} Dice \\hspace{0.1cm} Coefficient \\hspace{0.1cm} for \\hspace{0.1cm} class \\hspace{0.1cm} k}{K}  $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='7-5-3-3'></a>\n",
    "#### 7.5.3.3 - Soft Dice Coefficient\n",
    "\n",
    "In the previous section we discussed about the Dice Coefficient evaluation matrix, but now the question arises as to how we can use Dice Coefficient in the loss function?\n",
    "\n",
    "So, the answer is simple:\n",
    "\n",
    "The Dice Coefficient evalution matric is between 0 and 1 inclusive, and as closer Dice Coefficient evalution matric to 1 this means our model is better, so if we want to know the loss function we can use this formula that called Soft Dice Coefficient:\n",
    "\n",
    "$ Soft \\hspace{0.1cm} Dice \\hspace{0.1cm} Coefficient = 1 - Dice \\hspace{0.1cm} Coefficient$\n",
    "\n",
    "And we can see that as closer Dice Coefficient evalution matric to 1 this means our model is better, and therefore the Soft Dice Coefficient is smaller and closer to zero, and vice versa. Of course that the Soft Dice Coefficient is between 0 and 1 inclusive.\n",
    "\n",
    "In gernal the Soft Dice Coefficient for K classes is:\n",
    "\n",
    "$ Soft \\hspace{0.1cm} Dice \\hspace{0.1cm} Coefficient \\hspace{0.1cm} for \\hspace{0.1cm} K \\hspace{0.1cm} classes = 1 - \\frac{\\sum_{k=1}^{K} Dice \\hspace{0.1cm} Coefficient \\hspace{0.1cm} for \\hspace{0.1cm} class \\hspace{0.1cm} k}{K}  $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='7-5-3-4'></a>\n",
    "#### 7.5.3.4 - Why Soft Dice Coefficient is good?\n",
    "\n",
    "The Soft Dice Coefficient is good loss function for several main reasons:\n",
    "\n",
    "* She depends on the Dice Coefficient evaluation matrix that is actually F1 score, and we discussed before why F1 score is a good way to estimate our model\n",
    "\n",
    "* She very useful for imbalanced datasets, that this is very common problem. The loss function force the model be good in all the classes, because if the model is not good in some classes, it results in a higher Soft Dice Coefficient value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='8'></a>\n",
    "## 8 - The models that we will build\n",
    "In this section I will write the models that I will build.\n",
    "\n",
    "**1.** Regular Unet model architecture with dropout layer after each max pooling layer in the encoder and after each deconvultion layer in the decoder, basic_data_augmentation function, first version of normalization and Sparse Categorical Cross entropy loss function.\n",
    "\n",
    "**2.** Regular Unet model architecture with dropout layer after each max pooling layer in the encoder and after each deconvultion layer in the decoder, basic_data_augmentation function, first version of normalization and Soft Dice Coefficient loss function.\n",
    "\n",
    "**3.** Regular Unet model architecture with dropout layer after each max pooling layer in the encoder and after each deconvultion layer in the decoder, basic_data_augmentation function, first version of normalization and Weighted Sparse Categorical Cross entroy loss function.\n",
    "\n",
    "**4.** Regular Unet model architecture without dropout layers, with basic_data_augmentation function, first version of normalization and with Sparse Categorical Cross entropy loss function.\n",
    "\n",
    "**5.** Regular Unet model architecture with dropout layer after each max pooling layer in the encoder and after each deconvultion layer in the decoder, without data augmentation, first version of normalization and with Sparse Categorical Cross entropy loss function.\n",
    "\n",
    "**6.** Regular Unet model architecture without dropout layers, without data augmentation, first version of normalization and with Sparse Categorical Cross entropy loss function.\n",
    "\n",
    "**7.** Regular Unet model architecture with dropout layer after each max pooling layer in the encoder and after each deconvultion layer in the decoder, bad_data_augmentation function, first version of normalization and Sparse Categorical Cross entropy loss function.\n",
    "\n",
    "**8.** Regular Unet model architecture with dropout layer after each max pooling layer in the encoder and after each deconvultion layer in the decoder, basic_data_augmentation function, first version of normalization, Weighted Sparse Categorical Cross entroy loss function and Learning Rate scheduler.\n",
    "\n",
    "**9.** Regular Unet model architecture with dropout layer after each max pooling layer in the encoder and after each deconvultion layer in the decoder, basic_data_augmentation function, first version of normalization, Weighted Sparse Categorical Cross entroy loss function and Dropout scheduler.\n",
    "\n",
    "**10.** Regular Unet model architecture with dropout layer after each max pooling layer in the encoder and after each deconvultion layer in the decoder, basic_data_augmentation function, first version of normalization, Weighted Sparse Categorical Cross entroy loss function and Dropout and Learning Rate scheduler.\n",
    "\n",
    "**11.** Regular Unet model architecture with dropout layer after each max pooling layer in the encoder and after each deconvultion layer in the decoder, advance_data_augmentation function, first version of normalization, Weighted Sparse Categorical Cross entroy loss function and Learning Rate scheduler.\n",
    "\n",
    "**12.** Regular Unet model architecture with dropout layer after each max pooling layer in the encoder and after each deconvultion layer in the decoder, basic_data_augmentation function,second version of normalization, Weighted Sparse Categorical Cross entroy loss function and Learning Rate scheduler.\n",
    "\n",
    "**13.** Regular Unet model architecture with dropout layer after each max pooling layer in the encoder and after each deconvultion layer in the decoder, advance_data_augmentation function, second version of normalization, Weighted Sparse Categorical Cross entroy loss function and Learning Rate scheduler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='9'></a>\n",
    "## 9 - Let's build Unet encoders and decoders\n",
    "In this section we will build Unet encoders and decoders.\n",
    "\n",
    "They will be used to build the models in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='9-1'></a>\n",
    "### 9.1 - Let's build Unet encoder\n",
    "In this section we will build Unet encoder.\n",
    "\n",
    "Let's recall the structure of Unet encoder:\n",
    "\n",
    "There is 4 levels in the ecnoder, where that each level repated by two $ 3\\times3 $ conv(with padding) + Relu + Batch normalzation layers, and one $ 2\\times2 $ max pooling layer to downsample the resolution of the image.\n",
    "\n",
    "For simplicity, we will implement the bottleneck part in the encoder part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we will build the encoder of Unet, we need to build a function that will represent each level in the encoder.\n",
    "This function will implement the two $ 3\\times3 $ conv(with padding) + Relu + Batch normalzation layers, and one $ 2\\times2 $ max pooling layer to downsample the resolution of the image(This is optional. In the boottleneck part we will not use max pooling layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsampling_block(input, n_filters, dropout_prob = 0, max_pooling=True):\n",
    "    \"\"\"\n",
    "    Down sampling block that contains two 3*3 conv(with padding) + Relu + Batch normalzation layers,\n",
    "    and one optional 2*2 max pooling layer. \n",
    "\n",
    "    Arguments:\n",
    "        input - Input tensor\n",
    "        n_filters - Number of the filters in the block\n",
    "        dropout_prob - Dropout probability. If equal to 0, so we not use dropout.\n",
    "        max_pooling - Bool value that indicates if we need to do max pooling in this block\n",
    "\n",
    "    Returns:\n",
    "        outputs_dic - Dictionary that contains the next layer and skip connection outputs\n",
    "    \"\"\" \n",
    "\n",
    "    conv = Conv2D(n_filters, 3, activation='relu', padding='same', kernel_initializer='he_normal')(input)\n",
    "    conv = tf.keras.layers.BatchNormalization()(conv)\n",
    "    conv = Conv2D(n_filters, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv)\n",
    "    conv = tf.keras.layers.BatchNormalization()(conv)\n",
    "    \n",
    "    if max_pooling:\n",
    "        next_layer = MaxPooling2D(2, strides=2)(conv)\n",
    "    else:\n",
    "        next_layer = conv\n",
    "        \n",
    "    if dropout_prob > 0:\n",
    "        next_layer = Dropout(dropout_prob)(next_layer)\n",
    "        \n",
    "    skip_connection = conv\n",
    "\n",
    "    outputs_dic = {\"next_layer\":next_layer, \"skip_connection\":skip_connection}\n",
    "\n",
    "    return outputs_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_encoder(input, start_n_filters = 64, dropout_prob = 0.3):\n",
    "    \"\"\"\n",
    "    Unet encoder\n",
    "\n",
    "    Arguments:\n",
    "        input - Input tensor\n",
    "        start_n_filters - Number of the filters in the first level of the encoder\n",
    "        dropout_prob - Dropout probability. If equal to 0, so we not use dropout.\n",
    "\n",
    "    Returns:\n",
    "        Outputs that we will need in the decoder.\n",
    "        Outputs(feature maps) of the encoder that we need for the skip connections in this order:\n",
    "        first skip connection, ..., fourth skip connection  \n",
    "        And the output(feature map) of the last level in the encoder for the first level in the decoder.\n",
    "    \"\"\" \n",
    "\n",
    "    dblock1 = downsampling_block(input, start_n_filters, dropout_prob)\n",
    "    dblock2 = downsampling_block(dblock1[\"next_layer\"], start_n_filters*2, dropout_prob)\n",
    "    dblock3 = downsampling_block(dblock2[\"next_layer\"], start_n_filters*4, dropout_prob)\n",
    "    dblock4 = downsampling_block(dblock3[\"next_layer\"], start_n_filters*8, dropout_prob)\n",
    "    dblock5 = downsampling_block(dblock4[\"next_layer\"], start_n_filters*16, max_pooling=False)\n",
    "\n",
    "    return dblock1[\"skip_connection\"], dblock2[\"skip_connection\"], \\\n",
    "    dblock3[\"skip_connection\"], dblock4[\"skip_connection\"], dblock5[\"next_layer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='9-2'></a>\n",
    "### 9.2 - Let's build Unet decoder\n",
    "In this section we will build Unet decoder.\n",
    "\n",
    "Let's recall the structure of Unet decoder:\n",
    "\n",
    "There is 4 levels in the decoder, where the first 3 levels repated by two $ 3\\times3 $ conv(with padding) + Relu + Batch normalzation layers, and one $ 2\\times2 $ Transpose conv/Deconvultion layer to upsample the resolution of the image\n",
    "\n",
    "In the final level we do two $ 3\\times3 $ conv(with padding) + Relu + Batch normalzation layers, and one $ 1\\times1 $ conv(with padding)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we will build the decoder of Unet, we need to build a function that will represent each level in the decoder.\n",
    "\n",
    "This function will implement the one $ 2\\times2 $ Transpose conv/Deconvultion layer and two $ 3\\times 3 $ conv(with padding) + Relu + Batch normalzation layers.\n",
    "\n",
    "The $ 1\\times 1 $ conv(with padding) layer not include in this function and we will implement this layer in the decoder function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsampling_block(input, skip_connection_input, n_filters, dropout_prob = 0):\n",
    "    \"\"\"\n",
    "    Up sampling block that contains one 2*2 Transpose conv/Deconvultion layer, and \n",
    "    two 3*3 conv(with padding) + Relu + Batch normalzation layers. \n",
    "\n",
    "    Arguments:\n",
    "        input - Input tensor from the last block\n",
    "        skip_connection_input - Skip connection input tensor\n",
    "        n_filters - Number of the filters in the block\n",
    "        dropout_prob - Dropout probability. If equal to 0, so we not use dropout.\n",
    "\n",
    "    Returns:\n",
    "        output - Output tensor  \n",
    "    \"\"\" \n",
    "    \n",
    "    upsample_input = Conv2DTranspose(n_filters, kernel_size=3, strides=2,\n",
    "                                      padding='same')(input)\n",
    "    \n",
    "    if dropout_prob > 0:\n",
    "        upsample_input = Dropout(dropout_prob)(upsample_input)\n",
    "    merge = concatenate([upsample_input, skip_connection_input], axis=-1)\n",
    "    output = Conv2D(n_filters, 3, activation='relu', padding='same',\n",
    "                     kernel_initializer='he_normal')(merge)\n",
    "    output = tf.keras.layers.BatchNormalization()(output)\n",
    "    output = Conv2D(n_filters, 3, activation='relu', padding='same',\n",
    "                     kernel_initializer='he_normal')(output)\n",
    "    output = tf.keras.layers.BatchNormalization()(output)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_decoder(encoder_output, n_classes, start_n_filters = 512, dropout_prob = 0.3):\n",
    "    \"\"\"\n",
    "    Unet decoder\n",
    "\n",
    "    Arguments:\n",
    "        encoder_output - The output that we get from the encoder function.\n",
    "        n_classes - Number of classes in the dataset\n",
    "        start_n_filters - Number of the filters in the first level of the decoder\n",
    "        dropout_prob - Dropout probability. If equal to 0, so we not use dropout.\n",
    "        \n",
    "    Returns:\n",
    "        output - The output of the decoder\n",
    "    \"\"\" \n",
    "\n",
    "    skip_connection_1, skip_connection_2, skip_connection_3, skip_connection_4, \\\n",
    "    input_for_decoder_first_level = encoder_output\n",
    "\n",
    "    ublock1 = upsampling_block(input_for_decoder_first_level, skip_connection_4, start_n_filters, dropout_prob)\n",
    "    ublock2 = upsampling_block(ublock1, skip_connection_3, start_n_filters/2, dropout_prob)\n",
    "    ublock3 = upsampling_block(ublock2, skip_connection_2, start_n_filters/4, dropout_prob)\n",
    "    ublock4 = upsampling_block(ublock3, skip_connection_1, start_n_filters/8, dropout_prob)\n",
    "\n",
    "    output = Conv2D(filters=n_classes, kernel_size=1, padding='same', activation = \"softmax\")(ublock4)\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='10'></a>\n",
    "## 10 - Let's build genral unet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genral_unet_model(input_size, data_augmentation_func, encoder_func, decoder_func, data_augmentation_prob = 0.5, dropout_prob = 0.3, start_n_filters = 64, n_classes = NUM_CLASSES):\n",
    "    \"\"\"\n",
    "    Unet model\n",
    "\n",
    "    Arguments:\n",
    "        input_size - Input shape\n",
    "        data_augmentation_func - Data augmentation function that we will use\n",
    "        data_augmentation_prob - The probabilty for data augmentation. \n",
    "        We will do this in order that not always will be data augmentation,\n",
    "        and our model also for some iterations will learn our original images.\n",
    "        If equal to 0, so we not use data augmentation\n",
    "\n",
    "        encoder_func - Encoder function that we will use\n",
    "        decoder_func - Decoder function that we will use\n",
    "        dropout_prob - Dropout probability. If equal to 0, so we not use dropout.\n",
    "        start_n_filters - Number of the filters in the first level of the encoder\n",
    "        n_classes - Number of classes in the dataset\n",
    "        \n",
    "    Returns:\n",
    "        model - tf.keras.Model\n",
    "    \"\"\"\n",
    "\n",
    "    input = Input(input_size)\n",
    "    data_augmentation_layer = DataAugmentationLayer(data_augmentation_func, data_augmentation_prob)\n",
    "    augmented_input = data_augmentation_layer(input)\n",
    "\n",
    "    encoder_output = encoder_func(augmented_input, start_n_filters, dropout_prob)\n",
    "    decoder_output = decoder_func(encoder_output, n_classes, start_n_filters*8) \n",
    "\n",
    "    model = tf.keras.Model(input, decoder_output)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='11'></a>\n",
    "## 11 - Let's code the cost functions and evaluation metrics that we need for the models\n",
    "\n",
    "In section 7 we discussed about cost functions and evaluation metrics for Unet.\n",
    "\n",
    "In this project we will use Sparse Categorical Cross entropy, Weighted Sparse Categorical Cross entropy for each class and Soft Dice Coefficient as loss functions, and our evalution matric will include:\n",
    "\n",
    "1. Dice Coefficient\n",
    "2. Pixel accuracy\n",
    "\n",
    "We include Pixel accuracy in our evaluation matrix, in order to see how Pixel accuracy don't indicates if our model is good, even if Pixel accuracy has a high value.\n",
    "\n",
    "We can include only some classes in the evaluation matric and some classes not include, but in this project I want to include all of them, and challange myself to build a model that give good performance to all the classes.\n",
    "In industery applications this is will be more smart to include somtimes not all the classes, but only the classes that are the most important to us.\n",
    "\n",
    "Now, we will implement Weighted Sparse Categorical Cross entropy for each class and Soft Dice Coefficient loss functions, and Dice Coefficient evaluation matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's implement Dice Coefficient:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coefficient_for_one_class(y_true, y_pred, class_id):\n",
    "    y_true = tf.math.reduce_max(y_true, axis=-1, keepdims=False)\n",
    "    y_true = tf.cast(tf.equal(y_true, class_id), tf.float32)\n",
    "    y_pred = y_pred[..., class_id]\n",
    "    \n",
    "    intersection = tf.reduce_sum(y_true * y_pred)\n",
    "    union = tf.reduce_sum(y_true + y_pred)\n",
    "    \n",
    "    return (2.0 * intersection) / (union)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**And now let's implement Dice Coefficient and Soft Dice Coefficient for K classes:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coefficient(y_true, y_pred):\n",
    "    dice = 0.0\n",
    "    for class_id in range(NUM_CLASSES):\n",
    "        dice += dice_coefficient_for_one_class(y_true, y_pred, class_id)\n",
    "    return dice / NUM_CLASSES\n",
    "\n",
    "def soft_dice_coefficient(y_true, y_pred):\n",
    "    dice = 0.0\n",
    "    for class_id in range(NUM_CLASSES):\n",
    "        dice += dice_coefficient_for_one_class(y_true, y_pred, class_id)\n",
    "    return 1.0 - dice / NUM_CLASSES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**And now let's implement Weighted Sparse Categorical Cross entropy for each class:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_amount_of_pixels_that_belong_to_class(example, class_id):\n",
    "    ground_truth = example[1]\n",
    "    ground_truth = tf.math.reduce_max(ground_truth, axis=-1, keepdims=False)\n",
    "    count = tf.reduce_sum(tf.cast(tf.equal(ground_truth, class_id), tf.float32))\n",
    "    return count\n",
    "\n",
    "\n",
    "def calculate_class_frequency(train_dataset):\n",
    "    total_amount_of_pixels_in_train_dataset = TOTAL_TRAIN_FILES * TARGET_SIZE_TO_RESIZE[0] * TARGET_SIZE_TO_RESIZE[1]\n",
    "    class_frequency = tf.Variable(tf.zeros(NUM_CLASSES, dtype=tf.float32))\n",
    "    \n",
    "    for class_id in range(NUM_CLASSES):\n",
    "        amount_of_pixels_that_belong_to_class = train_dataset.reduce(\n",
    "            0.0, lambda x, example: x + compute_amount_of_pixels_that_belong_to_class(example, class_id))\n",
    "        \n",
    "        class_frequency[class_id].assign(amount_of_pixels_that_belong_to_class)\n",
    "        \n",
    "    return class_frequency \n",
    "\n",
    "class_frequency = calculate_class_frequency(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's assign more importance to the classes that are minority, i.e, that don't show up much in the train dataset.\n",
    "To the 2 classes that have the most little frequency we will give the higest importance and so we will continue and bring less and less importance as we go to classes that have higer frequency. The classes that have good frequency in the train dataset we will give regular importance, i.e, 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = tf.Variable(tf.zeros(NUM_CLASSES, dtype=tf.float32))\n",
    "\n",
    "_, minority_indices = tf.math.top_k(-class_frequency, k=15)\n",
    "\n",
    "minority_indices = minority_indices.numpy()\n",
    "\n",
    "for class_id in range(NUM_CLASSES):\n",
    "    class_weights[class_id].assign(1.0)\n",
    "    \n",
    "for index, value in enumerate(minority_indices):\n",
    "    if index == 0:\n",
    "        class_weights[value].assign(6.0)\n",
    "    elif index < 3:\n",
    "        class_weights[value].assign(5.0)\n",
    "    elif index < 5:\n",
    "        class_weights[value].assign(4.0)\n",
    "    elif index < 7:\n",
    "        class_weights[value].assign(3.0)\n",
    "    else:\n",
    "        class_weights[value].assign(2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_sparse_categorical_crossentropy(y_true, y_pred):\n",
    "    y_pred_with_class_weights = tf.math.pow(y_pred, class_weights)\n",
    "    weighted_cross_entropy = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred_with_class_weights, from_logits=False)\n",
    "    return weighted_cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='12'></a>\n",
    "## 12 - Let's implement plot function for plot history of the model\n",
    "\n",
    "In this section we will implement the plot function for plotting history of the model, because after we will finish train our model we need to plot the history of the model.\n",
    "\n",
    "We will plot the history of the model for the train, dev and test datasets in the loss function and our evaluation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(model_history):\n",
    "  fig, axes = plt.subplots(nrows = 1, ncols = 3, figsize=(20, 7))\n",
    "\n",
    "  # Training\n",
    "  sns.lineplot(range(1, len(model_history.history[\"loss\"]) + 1), model_history.history[\"loss\"], ax = axes[0], label=\"Training Loss\")\n",
    "  sns.lineplot(range(1, len(model_history.history[\"loss\"]) + 1), model_history.history[\"accuracy\"], ax = axes[1], label=\"Training Accuracy\")\n",
    "  sns.lineplot(range(1, len(model_history.history[\"loss\"]) + 1), model_history.history[\"dice_coefficient\"], ax = axes[2], label=\"Training Dice Coefficient\")\n",
    "\n",
    "  # Validation\n",
    "  sns.lineplot(range(1, len(model_history.history[\"loss\"]) + 1), model_history.history[\"val_loss\"], ax = axes[0], label=\"Validation Loss\")\n",
    "  sns.lineplot(range(1, len(model_history.history[\"loss\"]) + 1), model_history.history[\"val_accuracy\"], ax = axes[1], label=\"Validation Accuracy\")\n",
    "  sns.lineplot(range(1, len(model_history.history[\"loss\"]) + 1), model_history.history[\"val_dice_coefficient\"], ax = axes[2], label=\"Validation Dice Coefficient\")\n",
    "  \n",
    "  axes[0].set_title(\"Loss\", fontdict = {'fontsize': 15})\n",
    "  axes[0].set_xlabel(\"Epoch\")\n",
    "  axes[0].set_ylabel(\"Loss\")\n",
    "\n",
    "  axes[1].set_title(\"Accuracy\", fontdict = {'fontsize': 15})\n",
    "  axes[1].set_xlabel(\"Epoch\")\n",
    "  axes[1].set_ylabel(\"Accuracy\")\n",
    "\n",
    "  axes[2].set_title(\"Dice Coefficient\", fontdict = {'fontsize': 15})\n",
    "  axes[2].set_xlabel(\"Epoch\")\n",
    "  axes[2].set_ylabel(\"Dice Coefficient\")\n",
    "  plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='13'></a>\n",
    "## 13 - Let's implement functions for shows model predictions\n",
    "\n",
    "In this section we will implement the functions for shows model predictions after we trained the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIRST_EXAMPLE_IN_BATCH = 0\n",
    "\n",
    "def create_mask(pred):\n",
    "    mask = tf.argmax(pred, axis=-1)\n",
    "    mask = mask[..., tf.newaxis]\n",
    "    return mask\n",
    "\n",
    "\n",
    "def plot_prediction(ground_troth, pred):\n",
    "    fig, arr = plt.subplots(1, 2)\n",
    "    arr[0].imshow(ground_troth)\n",
    "    arr[0].set_title('Ground truth')\n",
    "    arr[1].imshow(pred)\n",
    "    arr[1].set_title('Prediction')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_predictions(model, dataset, num=1):\n",
    "    \"\"\"\n",
    "    Displays the first image of each of the num batches\n",
    "    \"\"\"\n",
    "    for img, ground_truth in dataset.take(num):\n",
    "        batch_pred = model.predict(img)\n",
    "        mask = create_mask(batch_pred[FIRST_EXAMPLE_IN_BATCH])\n",
    "        mask_vis = convert_mask_to_visualization_mask(mask, labels)\n",
    "        groud_truth_vis = convert_mask_to_visualization_mask(ground_truth[FIRST_EXAMPLE_IN_BATCH], labels)\n",
    "        plot_prediction(groud_truth_vis, mask_vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='14'></a>\n",
    "## 14 - Let's implement visualization callbacks functions\n",
    "\n",
    "In this section we will implement visualization callbacks functions, that let us see the model prediction on spesific example in real time when the model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_HEIGHT, INPUT_WIDTH, INPUT_CHANNELS = INPUT_SHAPE\n",
    "\n",
    "class VizCallback(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, model, img, ground_truth, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.model = model\n",
    "        self.img = tf.reshape(img, [1, INPUT_HEIGHT, INPUT_WIDTH, INPUT_CHANNELS])\n",
    "        self.ground_truth = ground_truth\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        pred = self.model.predict(self.img)\n",
    "        mask = create_mask(pred[FIRST_EXAMPLE_IN_BATCH])\n",
    "        mask_vis = convert_mask_to_visualization_mask(mask, labels)\n",
    "        groud_truth_vis = convert_mask_to_visualization_mask(self.ground_truth, labels)\n",
    "        plot_prediction(groud_truth_vis, mask_vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='15'></a>\n",
    "## 15 - Let's implement schedulers\n",
    "\n",
    "In this section we will implement schedulers for our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='15-1'></a>\n",
    "### 15.1 - Let's implement Dropout scheduler\n",
    "\n",
    "In this section we will implement Dropout scheduler for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropoutScheduler(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, drop_schedule):\n",
    "        \"\"\"\n",
    "        drop_schedule is a dictionary that have keys of epochs and values of new dropout probabilities for them\n",
    "        \"\"\"\n",
    "        super(DropoutScheduler, self).__init__()\n",
    "        self.drop_schedule = drop_schedule\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch in self.drop_schedule:\n",
    "            new_prob = self.drop_schedule[epoch]\n",
    "            for layer in self.model.layers:\n",
    "                if isinstance(layer, tf.keras.layers.Dropout):\n",
    "                    layer.rate = new_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='15-2'></a>\n",
    "### 15.2 - Let's implement Learning Rate scheduler\n",
    "\n",
    "In this section we will implement Learning Rate scheduler for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_scheduler(epoch, initial_lr, new_lr, num_of_epochs_for_initial_lr):\n",
    "    if epoch < num_of_epochs_for_initial_lr:\n",
    "        return initial_lr\n",
    "    else:\n",
    "        return new_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='16'></a>\n",
    "## 16 - Regular Unet model\n",
    "\n",
    "In this section we will create models that are versions of regular unet model(Unet model that we described in section 6), and we will research there performance, when we do some changes like change the data augmentation, hyperparameters, regulzation and loss functions and more.\n",
    "\n",
    "One important note is that I not run on my computer the model training process, beacause I have not good enough hardware. Instead, I run my notebook on kaggle, and I will provide in this notebook screenshots that will desribe my results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='16-1'></a>\n",
    "### 16.1 - First model version\n",
    "\n",
    "In this section we will create model that has the regular Unet model architecture with dropout layer after each max pooling layer in the encoder and after each deconvultion layer in the decoder, basic_data_augmentation function and Sparse Categorical Cross entropy loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='16-1-1'></a>\n",
    "#### 16.1.1 - Create the model\n",
    "\n",
    "In this section we will create the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_version_model  = genral_unet_model(INPUT_SHAPE, basic_data_augmentation, unet_encoder, unet_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, 128, 128, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " data_augmentation_layer_1   (None, 128, 128, 3)          0         ['input_2[0][0]']             \n",
      " (DataAugmentationLayer)                                                                          \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)          (None, 128, 128, 64)         1792      ['data_augmentation_layer_1[0]\n",
      "                                                                    [0]']                         \n",
      "                                                                                                  \n",
      " batch_normalization_18 (Ba  (None, 128, 128, 64)         256       ['conv2d_19[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)          (None, 128, 128, 64)         36928     ['batch_normalization_18[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_19 (Ba  (None, 128, 128, 64)         256       ['conv2d_20[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_4 (MaxPoolin  (None, 64, 64, 64)           0         ['batch_normalization_19[0][0]\n",
      " g2D)                                                               ']                            \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)         (None, 64, 64, 64)           0         ['max_pooling2d_4[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)          (None, 64, 64, 128)          73856     ['dropout_8[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_20 (Ba  (None, 64, 64, 128)          512       ['conv2d_21[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)          (None, 64, 64, 128)          147584    ['batch_normalization_20[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_21 (Ba  (None, 64, 64, 128)          512       ['conv2d_22[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_5 (MaxPoolin  (None, 32, 32, 128)          0         ['batch_normalization_21[0][0]\n",
      " g2D)                                                               ']                            \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)         (None, 32, 32, 128)          0         ['max_pooling2d_5[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)          (None, 32, 32, 256)          295168    ['dropout_9[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_22 (Ba  (None, 32, 32, 256)          1024      ['conv2d_23[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)          (None, 32, 32, 256)          590080    ['batch_normalization_22[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_23 (Ba  (None, 32, 32, 256)          1024      ['conv2d_24[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_6 (MaxPoolin  (None, 16, 16, 256)          0         ['batch_normalization_23[0][0]\n",
      " g2D)                                                               ']                            \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)        (None, 16, 16, 256)          0         ['max_pooling2d_6[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)          (None, 16, 16, 512)          1180160   ['dropout_10[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_24 (Ba  (None, 16, 16, 512)          2048      ['conv2d_25[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)          (None, 16, 16, 512)          2359808   ['batch_normalization_24[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_25 (Ba  (None, 16, 16, 512)          2048      ['conv2d_26[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_7 (MaxPoolin  (None, 8, 8, 512)            0         ['batch_normalization_25[0][0]\n",
      " g2D)                                                               ']                            \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)        (None, 8, 8, 512)            0         ['max_pooling2d_7[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)          (None, 8, 8, 1024)           4719616   ['dropout_11[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_26 (Ba  (None, 8, 8, 1024)           4096      ['conv2d_27[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)          (None, 8, 8, 1024)           9438208   ['batch_normalization_26[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_27 (Ba  (None, 8, 8, 1024)           4096      ['conv2d_28[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_transpose_4 (Conv2D  (None, 16, 16, 512)          4719104   ['batch_normalization_27[0][0]\n",
      " Transpose)                                                         ']                            \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)        (None, 16, 16, 512)          0         ['conv2d_transpose_4[0][0]']  \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate  (None, 16, 16, 1024)         0         ['dropout_12[0][0]',          \n",
      " )                                                                   'batch_normalization_25[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)          (None, 16, 16, 512)          4719104   ['concatenate_4[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_28 (Ba  (None, 16, 16, 512)          2048      ['conv2d_29[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)          (None, 16, 16, 512)          2359808   ['batch_normalization_28[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_29 (Ba  (None, 16, 16, 512)          2048      ['conv2d_30[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_transpose_5 (Conv2D  (None, 32, 32, 256)          1179904   ['batch_normalization_29[0][0]\n",
      " Transpose)                                                         ']                            \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)        (None, 32, 32, 256)          0         ['conv2d_transpose_5[0][0]']  \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate  (None, 32, 32, 512)          0         ['dropout_13[0][0]',          \n",
      " )                                                                   'batch_normalization_23[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)          (None, 32, 32, 256)          1179904   ['concatenate_5[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_30 (Ba  (None, 32, 32, 256)          1024      ['conv2d_31[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)          (None, 32, 32, 256)          590080    ['batch_normalization_30[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_31 (Ba  (None, 32, 32, 256)          1024      ['conv2d_32[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_transpose_6 (Conv2D  (None, 64, 64, 128)          295040    ['batch_normalization_31[0][0]\n",
      " Transpose)                                                         ']                            \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)        (None, 64, 64, 128)          0         ['conv2d_transpose_6[0][0]']  \n",
      "                                                                                                  \n",
      " concatenate_6 (Concatenate  (None, 64, 64, 256)          0         ['dropout_14[0][0]',          \n",
      " )                                                                   'batch_normalization_21[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_33 (Conv2D)          (None, 64, 64, 128)          295040    ['concatenate_6[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_32 (Ba  (None, 64, 64, 128)          512       ['conv2d_33[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_34 (Conv2D)          (None, 64, 64, 128)          147584    ['batch_normalization_32[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_33 (Ba  (None, 64, 64, 128)          512       ['conv2d_34[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_transpose_7 (Conv2D  (None, 128, 128, 64)         73792     ['batch_normalization_33[0][0]\n",
      " Transpose)                                                         ']                            \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)        (None, 128, 128, 64)         0         ['conv2d_transpose_7[0][0]']  \n",
      "                                                                                                  \n",
      " concatenate_7 (Concatenate  (None, 128, 128, 128)        0         ['dropout_15[0][0]',          \n",
      " )                                                                   'batch_normalization_19[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_35 (Conv2D)          (None, 128, 128, 64)         73792     ['concatenate_7[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_34 (Ba  (None, 128, 128, 64)         256       ['conv2d_35[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_36 (Conv2D)          (None, 128, 128, 64)         36928     ['batch_normalization_34[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_35 (Ba  (None, 128, 128, 64)         256       ['conv2d_36[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_37 (Conv2D)          (None, 128, 128, 34)         2210      ['batch_normalization_35[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 34539042 (131.76 MB)\n",
      "Trainable params: 34527266 (131.71 MB)\n",
      "Non-trainable params: 11776 (46.00 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "first_version_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compile the model with the Sparse Categorical Cross entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_version_model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              metrics=['accuracy', dice_coefficient])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create visualization callback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, ground_truth in train_dataset.take(1):\n",
    "    first_version_viz_callback = VizCallback(first_version_model, img[FIRST_EXAMPLE_IN_BATCH], ground_truth[FIRST_EXAMPLE_IN_BATCH])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='16-1-2'></a>\n",
    "#### 16.1.2 - Train the model and evaluate him on train and dev datasets\n",
    "\n",
    "In this section we will train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we will train our model for 40 epochs, and we will see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIRST_VERSION_EPOCHS = 40\n",
    "first_version_history = first_version_model.fit(train_dataset, epochs=FIRST_VERSION_EPOCHS, validation_data=dev_dataset, callbacks=[first_version_viz_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the history of this training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(first_version_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The history of this training process is:\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\first_version_first_run_history_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 19</u></b>: History result of the first version model in the first training run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see the results of the model in the second epoch and in the last epoch:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\first_version_first_run_second_epoch_result.png\" style=\"width:200;height:200;\">\n",
    "    <img src=\"Images\\first_version_first_run_last_epoch_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 20</u></b>: Results of the first version model in the first training run in the second epoch and in the last epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions:**\n",
    "\n",
    "* We can see in the history result, that the loss, Accuracy and Dice Cooefficient on the train dataset converge in a pretty good way, but on the dev dataset they are less converging but more moving in a zigzag pattern.\n",
    "\n",
    "* We get good results for first version of our model(i.e, the first idea that we try).\n",
    "We can see that the model accuracy is preety high, but the Dice Cooefficient score not high, and the reason for this is that our model less good in predict classes that have small amount of data(we talked about this in section 7).\n",
    "\n",
    "* We can see that our model little overfit the loss and Accuracy, but in the Dice Coefficient the model do much more overfiting. A logical reason for this that maybee the train dataset and the dev dataset has differernt classes that are minority and maybe there are classes in the dev dataset that in the train dataset these classes not have many examples, so the model not trained on them like the other classes.\n",
    "\n",
    "In conclusion, we need that our model will be more genral and less overfitt the train dataset.\n",
    "In addition, we need that the loss, Accuracy and Dice Cooefficient on the dev dataset will more converge and will not move in a zigzag pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try train the model more in order to see if we can get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIRST_VERSION_SECOND_RUN_EPOCHS = 40\n",
    "first_version_second_run_history = first_version_model.fit(train_dataset, epochs=FIRST_VERSION_SECOND_RUN_EPOCHS, validation_data=dev_dataset, callbacks=[first_version_viz_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the history of this training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(first_version_second_run_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The history of this training process is:\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\first_version_second_run_history_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 21</u></b>: History result of second training run in the first version model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can compare the results of the model in the last epoch of the first training run and in the last epoch of the second training:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\first_version_first_run_last_epoch_result.png\" style=\"width:200;height:200;\">\n",
    "    <img src=\"Images\\first_version_second_run_last_epoch_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 22</u></b>: Results of the first version model in the first training run in the last epoch and in the second training run in the last epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions:**\n",
    "\n",
    "* We can see in the history result, that the loss, Accuracy and Dice Cooefficient on the train dataset converge in a pretty good way, but on the dev dataset our model become less stable, and moving in a zigzag pattern.\n",
    "\n",
    "* We get better results in the last epoch in the second running compare to the last epoch in the first running. Now our model can predict correctly more little detials in the image.\n",
    "We can see that our model much improved in Accuracy and Dice Cooefficient.\n",
    "\n",
    "* We can see that our model more overfit the train dataset espacially in the Dice Cooefficient.\n",
    "\n",
    "In overall, the model performance improved on the train dataset, and the performance on the the dev dataset didn't get any worse, but our model overfit the train dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try train the model little more in order to see if we can get better results in the dev dataset and maybee less overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIRST_VERSION_THIRD_RUN_EPOCHS = 25\n",
    "first_version_third_run_history = first_version_model.fit(train_dataset, epochs=FIRST_VERSION_THIRD_RUN_EPOCHS, validation_data=dev_dataset, callbacks=[first_version_viz_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the history of this training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(first_version_third_run_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The history of this training process is:\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\first_version_third_run_history_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 23</u></b>: History result of second training run in the first version model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can compare the results of the model in the last epoch of the second training run and in the last epoch of the third training:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\first_version_second_run_last_epoch_result.png\" style=\"width:200;height:200;\">\n",
    "    <img src=\"Images\\first_version_third_run_last_epoch_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 24</u></b>: Results of the first version model in the second training run in the last epoch and in the third training run in the last epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions:**\n",
    "\n",
    "* We can see in the history result, that the loss, Accuracy and Dice Cooefficient on the train dataset converge in a pretty good way(in the Dice Cooefficient less converges), but on the dev dataset our model become much less stable, and moving in a zigzag pattern(espacially in the 5-10 epochs).\n",
    "\n",
    "* We get better results in the last epoch in the third running compare to the last epoch in the second running in the train dataset, but our model performance getting worse on the dev dataset compare to the second running.\n",
    "\n",
    "* We can see that our model much more overfit the train dataset espacially in the Dice Cooefficient.\n",
    "\n",
    "In overall, the model performance improved on the train dataset, and the model performance on the the dev dataset gets worse, and our model overfit the train dataset.\n",
    "\n",
    "Hence, we will use the first version model in the second running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='16-1-3'></a>\n",
    "#### 16.1.3 - Model's predictions on the train and dev datasets\n",
    "\n",
    "In this section we will show examples of model's predictions on the train and dev datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the model's predictions on train dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_predictions(first_version_model, train_dataset, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Six examples of model's predictions on the train dataset are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\first_version_predictions_train_part1.png\" style=\"width:200;height:200;\">\n",
    "    <img src=\"Images\\first_version_predictions_train_part2.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 25</u></b>: Model's predictions on the train dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the model's predictions on dev dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_predictions(first_version_model, dev_dataset, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Six examples of model's predictions on the dev dataset are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\first_version_predictions_dev_part1.png\" style=\"width:200;height:200;\">\n",
    "    <img src=\"Images\\first_version_predictions_dev_part2.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 26</u></b>: Model's predictions on the dev dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, we get very good results for first version of our model(i.e, the first idea that we try).\n",
    "We need to improve the model performance in the dev dataset and his performance on minority classes.\n",
    "\n",
    "In addition, we need that our model will be more genral and less overfitt the train dataset, and the loss, Accuracy and Dice Cooefficient on the dev dataset will more converge and will not move in a zigzag pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='16-2'></a>\n",
    "### 16.2 - Second model version\n",
    "\n",
    "In this section we will create model that has the regular Unet model architecture with dropout layer after each max pooling layer in the encoder and after each deconvultion layer in the decoder, basic_data_augmentation function and Soft Dice Coefficient loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='16-2-1'></a>\n",
    "#### 16.2.1 - Create the model\n",
    "\n",
    "In this section we will create the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_version_model  = genral_unet_model(INPUT_SHAPE, basic_data_augmentation, unet_encoder, unet_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)        [(None, 128, 128, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " data_augmentation_layer_2   (None, 128, 128, 3)          0         ['input_3[0][0]']             \n",
      " (DataAugmentationLayer)                                                                          \n",
      "                                                                                                  \n",
      " conv2d_38 (Conv2D)          (None, 128, 128, 64)         1792      ['data_augmentation_layer_2[0]\n",
      "                                                                    [0]']                         \n",
      "                                                                                                  \n",
      " batch_normalization_36 (Ba  (None, 128, 128, 64)         256       ['conv2d_38[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_39 (Conv2D)          (None, 128, 128, 64)         36928     ['batch_normalization_36[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_37 (Ba  (None, 128, 128, 64)         256       ['conv2d_39[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_8 (MaxPoolin  (None, 64, 64, 64)           0         ['batch_normalization_37[0][0]\n",
      " g2D)                                                               ']                            \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)        (None, 64, 64, 64)           0         ['max_pooling2d_8[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_40 (Conv2D)          (None, 64, 64, 128)          73856     ['dropout_16[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_38 (Ba  (None, 64, 64, 128)          512       ['conv2d_40[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_41 (Conv2D)          (None, 64, 64, 128)          147584    ['batch_normalization_38[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_39 (Ba  (None, 64, 64, 128)          512       ['conv2d_41[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_9 (MaxPoolin  (None, 32, 32, 128)          0         ['batch_normalization_39[0][0]\n",
      " g2D)                                                               ']                            \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)        (None, 32, 32, 128)          0         ['max_pooling2d_9[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_42 (Conv2D)          (None, 32, 32, 256)          295168    ['dropout_17[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_40 (Ba  (None, 32, 32, 256)          1024      ['conv2d_42[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_43 (Conv2D)          (None, 32, 32, 256)          590080    ['batch_normalization_40[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_41 (Ba  (None, 32, 32, 256)          1024      ['conv2d_43[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_10 (MaxPooli  (None, 16, 16, 256)          0         ['batch_normalization_41[0][0]\n",
      " ng2D)                                                              ']                            \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)        (None, 16, 16, 256)          0         ['max_pooling2d_10[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_44 (Conv2D)          (None, 16, 16, 512)          1180160   ['dropout_18[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_42 (Ba  (None, 16, 16, 512)          2048      ['conv2d_44[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_45 (Conv2D)          (None, 16, 16, 512)          2359808   ['batch_normalization_42[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_43 (Ba  (None, 16, 16, 512)          2048      ['conv2d_45[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_11 (MaxPooli  (None, 8, 8, 512)            0         ['batch_normalization_43[0][0]\n",
      " ng2D)                                                              ']                            \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)        (None, 8, 8, 512)            0         ['max_pooling2d_11[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_46 (Conv2D)          (None, 8, 8, 1024)           4719616   ['dropout_19[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_44 (Ba  (None, 8, 8, 1024)           4096      ['conv2d_46[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_47 (Conv2D)          (None, 8, 8, 1024)           9438208   ['batch_normalization_44[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_45 (Ba  (None, 8, 8, 1024)           4096      ['conv2d_47[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_transpose_8 (Conv2D  (None, 16, 16, 512)          4719104   ['batch_normalization_45[0][0]\n",
      " Transpose)                                                         ']                            \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)        (None, 16, 16, 512)          0         ['conv2d_transpose_8[0][0]']  \n",
      "                                                                                                  \n",
      " concatenate_8 (Concatenate  (None, 16, 16, 1024)         0         ['dropout_20[0][0]',          \n",
      " )                                                                   'batch_normalization_43[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_48 (Conv2D)          (None, 16, 16, 512)          4719104   ['concatenate_8[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_46 (Ba  (None, 16, 16, 512)          2048      ['conv2d_48[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_49 (Conv2D)          (None, 16, 16, 512)          2359808   ['batch_normalization_46[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_47 (Ba  (None, 16, 16, 512)          2048      ['conv2d_49[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_transpose_9 (Conv2D  (None, 32, 32, 256)          1179904   ['batch_normalization_47[0][0]\n",
      " Transpose)                                                         ']                            \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)        (None, 32, 32, 256)          0         ['conv2d_transpose_9[0][0]']  \n",
      "                                                                                                  \n",
      " concatenate_9 (Concatenate  (None, 32, 32, 512)          0         ['dropout_21[0][0]',          \n",
      " )                                                                   'batch_normalization_41[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_50 (Conv2D)          (None, 32, 32, 256)          1179904   ['concatenate_9[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_48 (Ba  (None, 32, 32, 256)          1024      ['conv2d_50[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_51 (Conv2D)          (None, 32, 32, 256)          590080    ['batch_normalization_48[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_49 (Ba  (None, 32, 32, 256)          1024      ['conv2d_51[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_transpose_10 (Conv2  (None, 64, 64, 128)          295040    ['batch_normalization_49[0][0]\n",
      " DTranspose)                                                        ']                            \n",
      "                                                                                                  \n",
      " dropout_22 (Dropout)        (None, 64, 64, 128)          0         ['conv2d_transpose_10[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_10 (Concatenat  (None, 64, 64, 256)          0         ['dropout_22[0][0]',          \n",
      " e)                                                                  'batch_normalization_39[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_52 (Conv2D)          (None, 64, 64, 128)          295040    ['concatenate_10[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_50 (Ba  (None, 64, 64, 128)          512       ['conv2d_52[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_53 (Conv2D)          (None, 64, 64, 128)          147584    ['batch_normalization_50[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_51 (Ba  (None, 64, 64, 128)          512       ['conv2d_53[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_transpose_11 (Conv2  (None, 128, 128, 64)         73792     ['batch_normalization_51[0][0]\n",
      " DTranspose)                                                        ']                            \n",
      "                                                                                                  \n",
      " dropout_23 (Dropout)        (None, 128, 128, 64)         0         ['conv2d_transpose_11[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_11 (Concatenat  (None, 128, 128, 128)        0         ['dropout_23[0][0]',          \n",
      " e)                                                                  'batch_normalization_37[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_54 (Conv2D)          (None, 128, 128, 64)         73792     ['concatenate_11[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_52 (Ba  (None, 128, 128, 64)         256       ['conv2d_54[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_55 (Conv2D)          (None, 128, 128, 64)         36928     ['batch_normalization_52[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_53 (Ba  (None, 128, 128, 64)         256       ['conv2d_55[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_56 (Conv2D)          (None, 128, 128, 34)         2210      ['batch_normalization_53[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 34539042 (131.76 MB)\n",
      "Trainable params: 34527266 (131.71 MB)\n",
      "Non-trainable params: 11776 (46.00 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "second_version_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compile the model with the Soft Dice Coefficient loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_version_model.compile(optimizer='adam',\n",
    "              loss=soft_dice_coefficient,\n",
    "              metrics=['accuracy', dice_coefficient])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create visualization callback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, ground_truth in train_dataset.take(1):\n",
    "    second_version_viz_callback = VizCallback(second_version_model, img[FIRST_EXAMPLE_IN_BATCH], ground_truth[FIRST_EXAMPLE_IN_BATCH])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='16-2-2'></a>\n",
    "#### 16.2.2 - Train the model and evaluate him on train and dev datasets\n",
    "\n",
    "In this section we will train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we will train our model for 40 epochs, and we will see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SECOND_VERSION_EPOCHS = 40\n",
    "second_version_history = second_version_model.fit(train_dataset, epochs=SECOND_VERSION_EPOCHS, validation_data=dev_dataset, callbacks=[second_version_viz_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the history of this training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(second_version_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The history of this training process is:\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\second_version_history_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 27</u></b>: History result of the second version model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see the results of the model in the second epoch and in the last epoch:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\second_version_second_epoch_result.png\" style=\"width:200;height:200;\">\n",
    "    <img src=\"Images\\second_version_last_epoch_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 28</u></b>: Results of the second version model in the second epoch and in the 25 epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions:**\n",
    "\n",
    "* We can see in the history result, that the loss, Accuracy and Dice Coefficient on the train dataset converge in a pretty good way, but on the dev dataset they are not converging but more moving in a zigzag pattern, i.e, the model is really unstable on the dev dataset, and this not good.\n",
    "\n",
    "* We get not good results compared to the first version model in the first running both in train dataset and dev dataset. We get better result in the Dice Cooefficient on the train dataset compared to the first version model in the first running, but this is because we use Soft Dice Cooefficient loss function.\n",
    "\n",
    "* We can see that our model overfit the loss, Accuracy and Dice Coefficient in the train dataset. \n",
    "\n",
    "In conclusion, we can see that Soft Dice Coefficient loss function not good loss function for our problem.\n",
    "She gave us better result in the Dice Cooefficient on the train dataset compared to the first version model in the first running, but much worsere results on the other things. In addition we can see that the learning process with this loss function is much more slower(I assume that one of the causes of this is because the scale of the loss function), and not stable in the dev dataset(A logical reason for this that maybee the train dataset and the dev dataset has differernt classes that are minority and while the Soft Dice Cooefficient loss function forces the model learn the minority of classes in train dataset, he not learned the minority of classes in the dev dataset).\n",
    "\n",
    "For this model version we will not show the model's predictions, because this version not good compared to the first version model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='16-3'></a>\n",
    "### 16.3 - Third model version\n",
    "\n",
    "In this section we will create model that has the regular Unet model architecture with dropout layer after each max pooling layer in the encoder and after each deconvultion layer in the decoder, basic_data_augmentation function and Weighted Sparse Categorical Cross entropy for each class loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='16-3-1'></a>\n",
    "#### 16.3.1 - Create the model\n",
    "\n",
    "In this section we will create the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_version_model  = genral_unet_model(INPUT_SHAPE, basic_data_augmentation, unet_encoder, unet_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)        [(None, 128, 128, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " data_augmentation_layer_3   (None, 128, 128, 3)          0         ['input_4[0][0]']             \n",
      " (DataAugmentationLayer)                                                                          \n",
      "                                                                                                  \n",
      " conv2d_57 (Conv2D)          (None, 128, 128, 64)         1792      ['data_augmentation_layer_3[0]\n",
      "                                                                    [0]']                         \n",
      "                                                                                                  \n",
      " batch_normalization_54 (Ba  (None, 128, 128, 64)         256       ['conv2d_57[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_58 (Conv2D)          (None, 128, 128, 64)         36928     ['batch_normalization_54[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_55 (Ba  (None, 128, 128, 64)         256       ['conv2d_58[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_12 (MaxPooli  (None, 64, 64, 64)           0         ['batch_normalization_55[0][0]\n",
      " ng2D)                                                              ']                            \n",
      "                                                                                                  \n",
      " dropout_24 (Dropout)        (None, 64, 64, 64)           0         ['max_pooling2d_12[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_59 (Conv2D)          (None, 64, 64, 128)          73856     ['dropout_24[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_56 (Ba  (None, 64, 64, 128)          512       ['conv2d_59[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_60 (Conv2D)          (None, 64, 64, 128)          147584    ['batch_normalization_56[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_57 (Ba  (None, 64, 64, 128)          512       ['conv2d_60[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_13 (MaxPooli  (None, 32, 32, 128)          0         ['batch_normalization_57[0][0]\n",
      " ng2D)                                                              ']                            \n",
      "                                                                                                  \n",
      " dropout_25 (Dropout)        (None, 32, 32, 128)          0         ['max_pooling2d_13[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_61 (Conv2D)          (None, 32, 32, 256)          295168    ['dropout_25[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_58 (Ba  (None, 32, 32, 256)          1024      ['conv2d_61[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_62 (Conv2D)          (None, 32, 32, 256)          590080    ['batch_normalization_58[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_59 (Ba  (None, 32, 32, 256)          1024      ['conv2d_62[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_14 (MaxPooli  (None, 16, 16, 256)          0         ['batch_normalization_59[0][0]\n",
      " ng2D)                                                              ']                            \n",
      "                                                                                                  \n",
      " dropout_26 (Dropout)        (None, 16, 16, 256)          0         ['max_pooling2d_14[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_63 (Conv2D)          (None, 16, 16, 512)          1180160   ['dropout_26[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_60 (Ba  (None, 16, 16, 512)          2048      ['conv2d_63[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_64 (Conv2D)          (None, 16, 16, 512)          2359808   ['batch_normalization_60[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_61 (Ba  (None, 16, 16, 512)          2048      ['conv2d_64[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_15 (MaxPooli  (None, 8, 8, 512)            0         ['batch_normalization_61[0][0]\n",
      " ng2D)                                                              ']                            \n",
      "                                                                                                  \n",
      " dropout_27 (Dropout)        (None, 8, 8, 512)            0         ['max_pooling2d_15[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_65 (Conv2D)          (None, 8, 8, 1024)           4719616   ['dropout_27[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_62 (Ba  (None, 8, 8, 1024)           4096      ['conv2d_65[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_66 (Conv2D)          (None, 8, 8, 1024)           9438208   ['batch_normalization_62[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_63 (Ba  (None, 8, 8, 1024)           4096      ['conv2d_66[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_transpose_12 (Conv2  (None, 16, 16, 512)          4719104   ['batch_normalization_63[0][0]\n",
      " DTranspose)                                                        ']                            \n",
      "                                                                                                  \n",
      " dropout_28 (Dropout)        (None, 16, 16, 512)          0         ['conv2d_transpose_12[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_12 (Concatenat  (None, 16, 16, 1024)         0         ['dropout_28[0][0]',          \n",
      " e)                                                                  'batch_normalization_61[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_67 (Conv2D)          (None, 16, 16, 512)          4719104   ['concatenate_12[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_64 (Ba  (None, 16, 16, 512)          2048      ['conv2d_67[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_68 (Conv2D)          (None, 16, 16, 512)          2359808   ['batch_normalization_64[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_65 (Ba  (None, 16, 16, 512)          2048      ['conv2d_68[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_transpose_13 (Conv2  (None, 32, 32, 256)          1179904   ['batch_normalization_65[0][0]\n",
      " DTranspose)                                                        ']                            \n",
      "                                                                                                  \n",
      " dropout_29 (Dropout)        (None, 32, 32, 256)          0         ['conv2d_transpose_13[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_13 (Concatenat  (None, 32, 32, 512)          0         ['dropout_29[0][0]',          \n",
      " e)                                                                  'batch_normalization_59[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_69 (Conv2D)          (None, 32, 32, 256)          1179904   ['concatenate_13[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_66 (Ba  (None, 32, 32, 256)          1024      ['conv2d_69[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_70 (Conv2D)          (None, 32, 32, 256)          590080    ['batch_normalization_66[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_67 (Ba  (None, 32, 32, 256)          1024      ['conv2d_70[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_transpose_14 (Conv2  (None, 64, 64, 128)          295040    ['batch_normalization_67[0][0]\n",
      " DTranspose)                                                        ']                            \n",
      "                                                                                                  \n",
      " dropout_30 (Dropout)        (None, 64, 64, 128)          0         ['conv2d_transpose_14[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_14 (Concatenat  (None, 64, 64, 256)          0         ['dropout_30[0][0]',          \n",
      " e)                                                                  'batch_normalization_57[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_71 (Conv2D)          (None, 64, 64, 128)          295040    ['concatenate_14[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_68 (Ba  (None, 64, 64, 128)          512       ['conv2d_71[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_72 (Conv2D)          (None, 64, 64, 128)          147584    ['batch_normalization_68[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_69 (Ba  (None, 64, 64, 128)          512       ['conv2d_72[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_transpose_15 (Conv2  (None, 128, 128, 64)         73792     ['batch_normalization_69[0][0]\n",
      " DTranspose)                                                        ']                            \n",
      "                                                                                                  \n",
      " dropout_31 (Dropout)        (None, 128, 128, 64)         0         ['conv2d_transpose_15[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_15 (Concatenat  (None, 128, 128, 128)        0         ['dropout_31[0][0]',          \n",
      " e)                                                                  'batch_normalization_55[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_73 (Conv2D)          (None, 128, 128, 64)         73792     ['concatenate_15[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_70 (Ba  (None, 128, 128, 64)         256       ['conv2d_73[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_74 (Conv2D)          (None, 128, 128, 64)         36928     ['batch_normalization_70[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_71 (Ba  (None, 128, 128, 64)         256       ['conv2d_74[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_75 (Conv2D)          (None, 128, 128, 34)         2210      ['batch_normalization_71[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 34539042 (131.76 MB)\n",
      "Trainable params: 34527266 (131.71 MB)\n",
      "Non-trainable params: 11776 (46.00 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "third_version_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compile the model with the Weighted Sparse Categorical Cross entropy for each class loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_version_model.compile(optimizer='adam',\n",
    "              loss=weighted_sparse_categorical_crossentropy,\n",
    "              metrics=['accuracy', dice_coefficient])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create visualization callback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, ground_truth in train_dataset.take(1):\n",
    "    third_version_viz_callback = VizCallback(third_version_model, img[FIRST_EXAMPLE_IN_BATCH], ground_truth[FIRST_EXAMPLE_IN_BATCH])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='16-3-2'></a>\n",
    "#### 16.3.2 - Train the model and evaluate him on train and dev datasets\n",
    "\n",
    "In this section we will train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we will train our model for 40 epochs, and we will see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THIRD_VERSION_EPOCHS = 40\n",
    "third_version_history = third_version_model.fit(train_dataset, epochs=THIRD_VERSION_EPOCHS, validation_data=dev_dataset, callbacks=[third_version_viz_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the history of this training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(third_version_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The history of this training process is:\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\third_version_first_run_history_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 29</u></b>: History result of the third version model in the first training run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see the results of the model in the second epoch and in the last epoch:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\third_version_first_run_second_epoch_result.png\" style=\"width:200;height:200;\">\n",
    "    <img src=\"Images\\third_version_first_run_last_epoch_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 30</u></b>: Results of the third version model in the first training run in the second epoch and in the last epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions:**\n",
    "\n",
    "* We can see in the history result, that the loss, Accuracy and Dice Cooefficient on the train dataset converge in a pretty good way, but on the dev dataset they are less converging but more moving in a zigzag pattern(espcially in 15 epoch). Compare to the first version model, this model much less converging in the loss, Accuracy and Dice Cooefficient on the dev dataset.\n",
    "\n",
    "* We can see that the model accuracy is preety high, but the Dice Cooefficient score not high, and the reson for this is that our model less good in predict classes that have small amount of data(we talked about this in section 7).\n",
    "\n",
    "* We can see that our model little overfit the loss and Accuracy, but in the Dice Coefficient the model do more overfit. Compare to the first version model, this model less overfit the train dataset.\n",
    "\n",
    "In conclusion, we need that the loss, Accuracy and Dice Cooefficient on the dev dataset will more converge and will not move in a zigzag pattern.\n",
    "In addition we want to get a higer performance on the train and dev datasets, while not overfitt the train dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try train the model more in order to see if we can get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THIRD_VERSION_SECOND_RUN_EPOCHS = 40\n",
    "third_version_second_run_history = third_version_model.fit(train_dataset, epochs=THIRD_VERSION_SECOND_RUN_EPOCHS, validation_data=dev_dataset, callbacks=[third_version_viz_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the history of this training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(third_version_second_run_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The history of this training process is:\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\third_version_second_run_history_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 31</u></b>: History result of the third version model in the second training run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can compare the results of the model in the last epoch of the first training run and in the last epoch of the second training:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\third_version_first_run_last_epoch_result.png\" style=\"width:200;height:200;\">\n",
    "    <img src=\"Images\\third_version_second_run_last_epoch_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 32</u></b>: Results of the third version model in the first training run in the last epoch and in the second training run in the last epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions:**\n",
    "\n",
    "* We can see in the history result, that the loss, Accuracy and Dice Cooefficient on the train dataset converge worse compare to the first training run, and on the dev dataset our model become less stable, and moving in a zigzag pattern.\n",
    "\n",
    "* We get better results in the last epoch in the second running compare to the last epoch in the first running. Now our model can predict correctly more little detials in the image.\n",
    "We can see that our model much improved in Accuracy and Dice Cooefficient.\n",
    "\n",
    "* We can see that our model more overfit the train dataset.\n",
    "\n",
    "In overall, the model performance improved on the train dataset, and the performance on the the dev dataset didn't get any worse in Accuracy and Dice Cooefficient, but our model overfit the train dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try train the model little more in order to see if we can get better results in the dev dataset and maybee less overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THIRD_VERSION_THIRD_RUN_EPOCHS = 25\n",
    "third_version_third_run_history = third_version_model.fit(train_dataset, epochs=THIRD_VERSION_THIRD_RUN_EPOCHS, validation_data=dev_dataset, callbacks=[third_version_viz_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the history of this training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(third_version_third_run_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The history of this training process is:\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\third_version_third_run_history_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 33</u></b>: History result of the third version model in the third training run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can compare the results of the model in the last epoch of the first training run and in the last epoch of the second training:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\third_version_second_run_last_epoch_result.png\" style=\"width:200;height:200;\">\n",
    "    <img src=\"Images\\third_version_third_run_last_epoch_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 34</u></b>: Results of the third version model in the second training run in the last epoch and in the third training run in the last epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions:**\n",
    "\n",
    "* We can see in the history result, that the loss, Accuracy and Dice Cooefficient on the train dataset converge in a pretty good way(in the Dice Cooefficient less converges), but on the dev dataset our model become much less stable, and moving in a zigzag pattern(espacially in the fifth epoch).\n",
    "\n",
    "* We get better results in the last epoch in the third running compare to the last epoch in the second running in the train dataset, but our model performance getting more worse on the dev dataset compare to the second running.\n",
    "\n",
    "* We can see that our model much more overfit the train dataset.\n",
    "\n",
    "In overall, the model performance improved on the train dataset, but the model performance on the the dev dataset gets worse, and our model overfit the train dataset.\n",
    "\n",
    "Hence, we will use the third version model in the second running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='16-3-3'></a>\n",
    "#### 16.3.3 - Model's predictions on the train and dev datasets\n",
    "\n",
    "In this section we will show examples of model's predictions on the train and dev datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the model's predictions on train dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_predictions(third_version_model, train_dataset, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Six examples of model's predictions on the train dataset are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\third_version_predictions_train_part1.png\" style=\"width:200;height:200;\">\n",
    "    <img src=\"Images\\third_version_predictions_train_part2.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 35</u></b>: Model's predictions on the train dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the model's predictions on dev dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_predictions(third_version_model, dev_dataset, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Six examples of model's predictions on the dev dataset are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\third_version_predictions_dev_part1.png\" style=\"width:200;height:200;\">\n",
    "    <img src=\"Images\\third_version_predictions_dev_part2.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 36</u></b>: Model's predictions on the dev dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, we get good results. We saw that the model predictions on minority classes improve, because we use Weighted Sparse Categorical Cross entropy.\n",
    "We need to try to improve the model performance espically in the dev dataset, i.e, we need that our model will be more genral and less overfitt the train dataset.\n",
    "\n",
    "In addition we need that the loss, Accuracy and Dice Cooefficient on the dev dataset will more converge and will not move in a zigzag pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='16-4'></a>\n",
    "### 16.4 - Fourth model version\n",
    "\n",
    "In this section we will create model that has the regular Unet model architecture without dropout layers,with basic_data_augmentation function and Sparse Categorical Cross entropy loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='16-4-1'></a>\n",
    "#### 16.4.1 - Create the model\n",
    "\n",
    "In this section we will create the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourth_version_model  = genral_unet_model(INPUT_SHAPE, basic_data_augmentation, unet_encoder, unet_decoder, dropout_prob = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)        [(None, 128, 128, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " data_augmentation_layer_4   (None, 128, 128, 3)          0         ['input_5[0][0]']             \n",
      " (DataAugmentationLayer)                                                                          \n",
      "                                                                                                  \n",
      " conv2d_76 (Conv2D)          (None, 128, 128, 64)         1792      ['data_augmentation_layer_4[0]\n",
      "                                                                    [0]']                         \n",
      "                                                                                                  \n",
      " batch_normalization_72 (Ba  (None, 128, 128, 64)         256       ['conv2d_76[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_77 (Conv2D)          (None, 128, 128, 64)         36928     ['batch_normalization_72[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_73 (Ba  (None, 128, 128, 64)         256       ['conv2d_77[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_16 (MaxPooli  (None, 64, 64, 64)           0         ['batch_normalization_73[0][0]\n",
      " ng2D)                                                              ']                            \n",
      "                                                                                                  \n",
      " conv2d_78 (Conv2D)          (None, 64, 64, 128)          73856     ['max_pooling2d_16[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_74 (Ba  (None, 64, 64, 128)          512       ['conv2d_78[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_79 (Conv2D)          (None, 64, 64, 128)          147584    ['batch_normalization_74[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_75 (Ba  (None, 64, 64, 128)          512       ['conv2d_79[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_17 (MaxPooli  (None, 32, 32, 128)          0         ['batch_normalization_75[0][0]\n",
      " ng2D)                                                              ']                            \n",
      "                                                                                                  \n",
      " conv2d_80 (Conv2D)          (None, 32, 32, 256)          295168    ['max_pooling2d_17[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_76 (Ba  (None, 32, 32, 256)          1024      ['conv2d_80[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_81 (Conv2D)          (None, 32, 32, 256)          590080    ['batch_normalization_76[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_77 (Ba  (None, 32, 32, 256)          1024      ['conv2d_81[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_18 (MaxPooli  (None, 16, 16, 256)          0         ['batch_normalization_77[0][0]\n",
      " ng2D)                                                              ']                            \n",
      "                                                                                                  \n",
      " conv2d_82 (Conv2D)          (None, 16, 16, 512)          1180160   ['max_pooling2d_18[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_78 (Ba  (None, 16, 16, 512)          2048      ['conv2d_82[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_83 (Conv2D)          (None, 16, 16, 512)          2359808   ['batch_normalization_78[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_79 (Ba  (None, 16, 16, 512)          2048      ['conv2d_83[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_19 (MaxPooli  (None, 8, 8, 512)            0         ['batch_normalization_79[0][0]\n",
      " ng2D)                                                              ']                            \n",
      "                                                                                                  \n",
      " conv2d_84 (Conv2D)          (None, 8, 8, 1024)           4719616   ['max_pooling2d_19[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_80 (Ba  (None, 8, 8, 1024)           4096      ['conv2d_84[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_85 (Conv2D)          (None, 8, 8, 1024)           9438208   ['batch_normalization_80[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_81 (Ba  (None, 8, 8, 1024)           4096      ['conv2d_85[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_transpose_16 (Conv2  (None, 16, 16, 512)          4719104   ['batch_normalization_81[0][0]\n",
      " DTranspose)                                                        ']                            \n",
      "                                                                                                  \n",
      " dropout_32 (Dropout)        (None, 16, 16, 512)          0         ['conv2d_transpose_16[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_16 (Concatenat  (None, 16, 16, 1024)         0         ['dropout_32[0][0]',          \n",
      " e)                                                                  'batch_normalization_79[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_86 (Conv2D)          (None, 16, 16, 512)          4719104   ['concatenate_16[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_82 (Ba  (None, 16, 16, 512)          2048      ['conv2d_86[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_87 (Conv2D)          (None, 16, 16, 512)          2359808   ['batch_normalization_82[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_83 (Ba  (None, 16, 16, 512)          2048      ['conv2d_87[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_transpose_17 (Conv2  (None, 32, 32, 256)          1179904   ['batch_normalization_83[0][0]\n",
      " DTranspose)                                                        ']                            \n",
      "                                                                                                  \n",
      " dropout_33 (Dropout)        (None, 32, 32, 256)          0         ['conv2d_transpose_17[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_17 (Concatenat  (None, 32, 32, 512)          0         ['dropout_33[0][0]',          \n",
      " e)                                                                  'batch_normalization_77[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_88 (Conv2D)          (None, 32, 32, 256)          1179904   ['concatenate_17[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_84 (Ba  (None, 32, 32, 256)          1024      ['conv2d_88[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_89 (Conv2D)          (None, 32, 32, 256)          590080    ['batch_normalization_84[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_85 (Ba  (None, 32, 32, 256)          1024      ['conv2d_89[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_transpose_18 (Conv2  (None, 64, 64, 128)          295040    ['batch_normalization_85[0][0]\n",
      " DTranspose)                                                        ']                            \n",
      "                                                                                                  \n",
      " dropout_34 (Dropout)        (None, 64, 64, 128)          0         ['conv2d_transpose_18[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_18 (Concatenat  (None, 64, 64, 256)          0         ['dropout_34[0][0]',          \n",
      " e)                                                                  'batch_normalization_75[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_90 (Conv2D)          (None, 64, 64, 128)          295040    ['concatenate_18[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_86 (Ba  (None, 64, 64, 128)          512       ['conv2d_90[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_91 (Conv2D)          (None, 64, 64, 128)          147584    ['batch_normalization_86[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_87 (Ba  (None, 64, 64, 128)          512       ['conv2d_91[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_transpose_19 (Conv2  (None, 128, 128, 64)         73792     ['batch_normalization_87[0][0]\n",
      " DTranspose)                                                        ']                            \n",
      "                                                                                                  \n",
      " dropout_35 (Dropout)        (None, 128, 128, 64)         0         ['conv2d_transpose_19[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_19 (Concatenat  (None, 128, 128, 128)        0         ['dropout_35[0][0]',          \n",
      " e)                                                                  'batch_normalization_73[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_92 (Conv2D)          (None, 128, 128, 64)         73792     ['concatenate_19[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_88 (Ba  (None, 128, 128, 64)         256       ['conv2d_92[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_93 (Conv2D)          (None, 128, 128, 64)         36928     ['batch_normalization_88[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_89 (Ba  (None, 128, 128, 64)         256       ['conv2d_93[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_94 (Conv2D)          (None, 128, 128, 34)         2210      ['batch_normalization_89[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 34539042 (131.76 MB)\n",
      "Trainable params: 34527266 (131.71 MB)\n",
      "Non-trainable params: 11776 (46.00 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "fourth_version_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compile the model with the Sparse Categorical Cross entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourth_version_model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              metrics=['accuracy', dice_coefficient])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create visualization callback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, ground_truth in train_dataset.take(1):\n",
    "    fourth_version_viz_callback = VizCallback(fourth_version_model, img[FIRST_EXAMPLE_IN_BATCH], ground_truth[FIRST_EXAMPLE_IN_BATCH])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='16-4-2'></a>\n",
    "#### 16.4.2 - Train the model and evaluate him on train and dev datasets\n",
    "\n",
    "In this section we will train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we will train our model for 40 epochs, and we will see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOURTH_VERSION_EPOCHS = 40\n",
    "fourth_version_history = fourth_version_model.fit(train_dataset, epochs=FOURTH_VERSION_EPOCHS, validation_data=dev_dataset, callbacks=[fourth_version_viz_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the history of this training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(fourth_version_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The history of this training process is:\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\fourth_version_first_run_history_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 37</u></b>: History result of the fourth version model in the first training run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions:**\n",
    "\n",
    "* We can see in the history result, that the loss, Accuracy and Dice Cooefficient on the train dataset converge in a pretty good way, but on the dev dataset they are less converging and little moving in a zigzag pattern. Compare to the previous models, this model much more converging in the loss, Accuracy and Dice Cooefficient on the dev dataset.\n",
    "\n",
    "* We can see that the model accuracy is preety high, but the Dice Cooefficient score not high, and the reason for this is that our model less good in predict classes that have small amount of data(we talked about this in section 7). We want get a higher results because this still not good results compare to previous models.\n",
    "\n",
    "* We can see that our model little overfit the loss and Accuracy, but in the Dice Coefficient the model do more overfit. Because this model has not dropout layers, we will see that as long as we train more our model ,our model will overfit more the train dataset. \n",
    "\n",
    "In conclusion, we want to get a higer performance on the train and dev datasets, while not overfitt the train dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try train the model more in order to see if we can get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOURTH_VERSION_SECOND_RUN_EPOCHS = 40\n",
    "fourth_version_second_run_history = fourth_version_model.fit(train_dataset, epochs=FOURTH_VERSION_SECOND_RUN_EPOCHS, validation_data=dev_dataset, callbacks=[fourth_version_viz_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the history of this training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(fourth_version_second_run_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The history of this training process is:\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\fourth_version_second_run_history_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 38</u></b>: History result of the fourth version model in the second training run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions:**\n",
    "\n",
    "* We can see in the history result, that the loss, Accuracy and Dice Coefficient on the train dataset converge in a pretty good way(in the Dice Cooefficient less converges), but on the dev dataset they are not converging but more moving in a zigzag pattern, i.e, the model is really unstable on the dev dataset, and this not good.\n",
    "\n",
    "* We can see that our model overfit the loss, Accuracy and Dice Coefficient in the train dataset.  The model performance in loss, Accuracy and Dice Coefficient on the dev dataset become worser. This is because we not use dropout layers, and hence our model overfit the the train dataset.\n",
    "\n",
    "In conclusion, we can see that dropout layers are necessary for get good performance on the dev dataset, and in order our model can generalize to new examples.\n",
    "\n",
    "For this model version we will not show the model's predictions, because this version not good compared to the previous models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='16-5'></a>\n",
    "### 16.5 - Fifth model version\n",
    "\n",
    "In this section we will create model that has the regular Unet model architecture with dropout layer after each max pooling layer in the encoder and after each deconvultion layer in the decoder, without data augmentation and with Sparse Categorical Cross entropy loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='16-5-1'></a>\n",
    "#### 16.5.1 - Create the model\n",
    "\n",
    "In this section we will create the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "fifth_version_model  = genral_unet_model(INPUT_SHAPE, basic_data_augmentation, unet_encoder, unet_decoder, data_augmentation_prob = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_6 (InputLayer)        [(None, 128, 128, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " data_augmentation_layer_5   (None, 128, 128, 3)          0         ['input_6[0][0]']             \n",
      " (DataAugmentationLayer)                                                                          \n",
      "                                                                                                  \n",
      " conv2d_95 (Conv2D)          (None, 128, 128, 64)         1792      ['data_augmentation_layer_5[0]\n",
      "                                                                    [0]']                         \n",
      "                                                                                                  \n",
      " batch_normalization_90 (Ba  (None, 128, 128, 64)         256       ['conv2d_95[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_96 (Conv2D)          (None, 128, 128, 64)         36928     ['batch_normalization_90[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_91 (Ba  (None, 128, 128, 64)         256       ['conv2d_96[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_20 (MaxPooli  (None, 64, 64, 64)           0         ['batch_normalization_91[0][0]\n",
      " ng2D)                                                              ']                            \n",
      "                                                                                                  \n",
      " dropout_36 (Dropout)        (None, 64, 64, 64)           0         ['max_pooling2d_20[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_97 (Conv2D)          (None, 64, 64, 128)          73856     ['dropout_36[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_92 (Ba  (None, 64, 64, 128)          512       ['conv2d_97[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_98 (Conv2D)          (None, 64, 64, 128)          147584    ['batch_normalization_92[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_93 (Ba  (None, 64, 64, 128)          512       ['conv2d_98[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_21 (MaxPooli  (None, 32, 32, 128)          0         ['batch_normalization_93[0][0]\n",
      " ng2D)                                                              ']                            \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)        (None, 32, 32, 128)          0         ['max_pooling2d_21[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_99 (Conv2D)          (None, 32, 32, 256)          295168    ['dropout_37[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_94 (Ba  (None, 32, 32, 256)          1024      ['conv2d_99[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_100 (Conv2D)         (None, 32, 32, 256)          590080    ['batch_normalization_94[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_95 (Ba  (None, 32, 32, 256)          1024      ['conv2d_100[0][0]']          \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_22 (MaxPooli  (None, 16, 16, 256)          0         ['batch_normalization_95[0][0]\n",
      " ng2D)                                                              ']                            \n",
      "                                                                                                  \n",
      " dropout_38 (Dropout)        (None, 16, 16, 256)          0         ['max_pooling2d_22[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_101 (Conv2D)         (None, 16, 16, 512)          1180160   ['dropout_38[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_96 (Ba  (None, 16, 16, 512)          2048      ['conv2d_101[0][0]']          \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_102 (Conv2D)         (None, 16, 16, 512)          2359808   ['batch_normalization_96[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_97 (Ba  (None, 16, 16, 512)          2048      ['conv2d_102[0][0]']          \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_23 (MaxPooli  (None, 8, 8, 512)            0         ['batch_normalization_97[0][0]\n",
      " ng2D)                                                              ']                            \n",
      "                                                                                                  \n",
      " dropout_39 (Dropout)        (None, 8, 8, 512)            0         ['max_pooling2d_23[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_103 (Conv2D)         (None, 8, 8, 1024)           4719616   ['dropout_39[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_98 (Ba  (None, 8, 8, 1024)           4096      ['conv2d_103[0][0]']          \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_104 (Conv2D)         (None, 8, 8, 1024)           9438208   ['batch_normalization_98[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_99 (Ba  (None, 8, 8, 1024)           4096      ['conv2d_104[0][0]']          \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_transpose_20 (Conv2  (None, 16, 16, 512)          4719104   ['batch_normalization_99[0][0]\n",
      " DTranspose)                                                        ']                            \n",
      "                                                                                                  \n",
      " dropout_40 (Dropout)        (None, 16, 16, 512)          0         ['conv2d_transpose_20[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_20 (Concatenat  (None, 16, 16, 1024)         0         ['dropout_40[0][0]',          \n",
      " e)                                                                  'batch_normalization_97[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_105 (Conv2D)         (None, 16, 16, 512)          4719104   ['concatenate_20[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_100 (B  (None, 16, 16, 512)          2048      ['conv2d_105[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_106 (Conv2D)         (None, 16, 16, 512)          2359808   ['batch_normalization_100[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_101 (B  (None, 16, 16, 512)          2048      ['conv2d_106[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_transpose_21 (Conv2  (None, 32, 32, 256)          1179904   ['batch_normalization_101[0][0\n",
      " DTranspose)                                                        ]']                           \n",
      "                                                                                                  \n",
      " dropout_41 (Dropout)        (None, 32, 32, 256)          0         ['conv2d_transpose_21[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_21 (Concatenat  (None, 32, 32, 512)          0         ['dropout_41[0][0]',          \n",
      " e)                                                                  'batch_normalization_95[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_107 (Conv2D)         (None, 32, 32, 256)          1179904   ['concatenate_21[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_102 (B  (None, 32, 32, 256)          1024      ['conv2d_107[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_108 (Conv2D)         (None, 32, 32, 256)          590080    ['batch_normalization_102[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_103 (B  (None, 32, 32, 256)          1024      ['conv2d_108[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_transpose_22 (Conv2  (None, 64, 64, 128)          295040    ['batch_normalization_103[0][0\n",
      " DTranspose)                                                        ]']                           \n",
      "                                                                                                  \n",
      " dropout_42 (Dropout)        (None, 64, 64, 128)          0         ['conv2d_transpose_22[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_22 (Concatenat  (None, 64, 64, 256)          0         ['dropout_42[0][0]',          \n",
      " e)                                                                  'batch_normalization_93[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_109 (Conv2D)         (None, 64, 64, 128)          295040    ['concatenate_22[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_104 (B  (None, 64, 64, 128)          512       ['conv2d_109[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_110 (Conv2D)         (None, 64, 64, 128)          147584    ['batch_normalization_104[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_105 (B  (None, 64, 64, 128)          512       ['conv2d_110[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_transpose_23 (Conv2  (None, 128, 128, 64)         73792     ['batch_normalization_105[0][0\n",
      " DTranspose)                                                        ]']                           \n",
      "                                                                                                  \n",
      " dropout_43 (Dropout)        (None, 128, 128, 64)         0         ['conv2d_transpose_23[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_23 (Concatenat  (None, 128, 128, 128)        0         ['dropout_43[0][0]',          \n",
      " e)                                                                  'batch_normalization_91[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_111 (Conv2D)         (None, 128, 128, 64)         73792     ['concatenate_23[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_106 (B  (None, 128, 128, 64)         256       ['conv2d_111[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_112 (Conv2D)         (None, 128, 128, 64)         36928     ['batch_normalization_106[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_107 (B  (None, 128, 128, 64)         256       ['conv2d_112[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_113 (Conv2D)         (None, 128, 128, 34)         2210      ['batch_normalization_107[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 34539042 (131.76 MB)\n",
      "Trainable params: 34527266 (131.71 MB)\n",
      "Non-trainable params: 11776 (46.00 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "fifth_version_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compile the model with the Sparse Categorical Cross entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fifth_version_model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              metrics=['accuracy', dice_coefficient])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create visualization callback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, ground_truth in train_dataset.take(1):\n",
    "    fifth_version_viz_callback = VizCallback(fifth_version_model, img[FIRST_EXAMPLE_IN_BATCH], ground_truth[FIRST_EXAMPLE_IN_BATCH])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='16-5-2'></a>\n",
    "#### 16.5.2 - Train the model and evaluate him on train and dev datasets\n",
    "\n",
    "In this section we will train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we will train our model for 40 epochs, and we will see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIFTH_VERSION_EPOCHS = 40\n",
    "fifth_version_history = fifth_version_model.fit(train_dataset, epochs=FIFTH_VERSION_EPOCHS, validation_data=dev_dataset, callbacks=[fifth_version_viz_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the history of this training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(fifth_version_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The history of this training process is:\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\fifth_version_first_run_history_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 39</u></b>: History result of the fifth version model in the first training run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions:**\n",
    "\n",
    "* We can see in the history result, that the loss, Accuracy and Dice Coefficient on the train dataset converge in a pretty good way(in the Dice Cooefficient less converges good in the end), but on the dev dataset they are not converging but more moving in a zigzag pattern, i.e, the model is really unstable on the dev dataset, and this not good.\n",
    "\n",
    "* We can see that our model overfit the loss, Accuracy and Dice Coefficient in the train dataset. This is because we not use data augmentation, and hence our model overfit the the train dataset.\n",
    "\n",
    "In conclusion, we can see that data augmentation is necessary for get good performance on the dev dataset, and in order our model can generalize to new examples.\n",
    "\n",
    "For this model version we will not show the model's predictions, because this version not good compared to the previous models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='16-6'></a>\n",
    "### 16.6 - Sixth model version\n",
    "\n",
    "In this section we will create model that has the regular Unet model architecture without dropout layers, without data augmentation and with Sparse Categorical Cross entropy loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='16-6-1'></a>\n",
    "#### 16.6.1 - Create the model\n",
    "\n",
    "In this section we will create the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sixth_version_model  = genral_unet_model(INPUT_SHAPE, basic_data_augmentation, unet_encoder, unet_decoder, data_augmentation_prob = 0, dropout_prob = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_7 (InputLayer)        [(None, 128, 128, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " data_augmentation_layer_6   (None, 128, 128, 3)          0         ['input_7[0][0]']             \n",
      " (DataAugmentationLayer)                                                                          \n",
      "                                                                                                  \n",
      " conv2d_114 (Conv2D)         (None, 128, 128, 64)         1792      ['data_augmentation_layer_6[0]\n",
      "                                                                    [0]']                         \n",
      "                                                                                                  \n",
      " batch_normalization_108 (B  (None, 128, 128, 64)         256       ['conv2d_114[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_115 (Conv2D)         (None, 128, 128, 64)         36928     ['batch_normalization_108[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_109 (B  (None, 128, 128, 64)         256       ['conv2d_115[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_24 (MaxPooli  (None, 64, 64, 64)           0         ['batch_normalization_109[0][0\n",
      " ng2D)                                                              ]']                           \n",
      "                                                                                                  \n",
      " conv2d_116 (Conv2D)         (None, 64, 64, 128)          73856     ['max_pooling2d_24[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_110 (B  (None, 64, 64, 128)          512       ['conv2d_116[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_117 (Conv2D)         (None, 64, 64, 128)          147584    ['batch_normalization_110[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_111 (B  (None, 64, 64, 128)          512       ['conv2d_117[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_25 (MaxPooli  (None, 32, 32, 128)          0         ['batch_normalization_111[0][0\n",
      " ng2D)                                                              ]']                           \n",
      "                                                                                                  \n",
      " conv2d_118 (Conv2D)         (None, 32, 32, 256)          295168    ['max_pooling2d_25[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_112 (B  (None, 32, 32, 256)          1024      ['conv2d_118[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_119 (Conv2D)         (None, 32, 32, 256)          590080    ['batch_normalization_112[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_113 (B  (None, 32, 32, 256)          1024      ['conv2d_119[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_26 (MaxPooli  (None, 16, 16, 256)          0         ['batch_normalization_113[0][0\n",
      " ng2D)                                                              ]']                           \n",
      "                                                                                                  \n",
      " conv2d_120 (Conv2D)         (None, 16, 16, 512)          1180160   ['max_pooling2d_26[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_114 (B  (None, 16, 16, 512)          2048      ['conv2d_120[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_121 (Conv2D)         (None, 16, 16, 512)          2359808   ['batch_normalization_114[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_115 (B  (None, 16, 16, 512)          2048      ['conv2d_121[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_27 (MaxPooli  (None, 8, 8, 512)            0         ['batch_normalization_115[0][0\n",
      " ng2D)                                                              ]']                           \n",
      "                                                                                                  \n",
      " conv2d_122 (Conv2D)         (None, 8, 8, 1024)           4719616   ['max_pooling2d_27[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_116 (B  (None, 8, 8, 1024)           4096      ['conv2d_122[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_123 (Conv2D)         (None, 8, 8, 1024)           9438208   ['batch_normalization_116[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_117 (B  (None, 8, 8, 1024)           4096      ['conv2d_123[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_transpose_24 (Conv2  (None, 16, 16, 512)          4719104   ['batch_normalization_117[0][0\n",
      " DTranspose)                                                        ]']                           \n",
      "                                                                                                  \n",
      " dropout_44 (Dropout)        (None, 16, 16, 512)          0         ['conv2d_transpose_24[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_24 (Concatenat  (None, 16, 16, 1024)         0         ['dropout_44[0][0]',          \n",
      " e)                                                                  'batch_normalization_115[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_124 (Conv2D)         (None, 16, 16, 512)          4719104   ['concatenate_24[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_118 (B  (None, 16, 16, 512)          2048      ['conv2d_124[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_125 (Conv2D)         (None, 16, 16, 512)          2359808   ['batch_normalization_118[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_119 (B  (None, 16, 16, 512)          2048      ['conv2d_125[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_transpose_25 (Conv2  (None, 32, 32, 256)          1179904   ['batch_normalization_119[0][0\n",
      " DTranspose)                                                        ]']                           \n",
      "                                                                                                  \n",
      " dropout_45 (Dropout)        (None, 32, 32, 256)          0         ['conv2d_transpose_25[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_25 (Concatenat  (None, 32, 32, 512)          0         ['dropout_45[0][0]',          \n",
      " e)                                                                  'batch_normalization_113[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_126 (Conv2D)         (None, 32, 32, 256)          1179904   ['concatenate_25[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_120 (B  (None, 32, 32, 256)          1024      ['conv2d_126[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_127 (Conv2D)         (None, 32, 32, 256)          590080    ['batch_normalization_120[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_121 (B  (None, 32, 32, 256)          1024      ['conv2d_127[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_transpose_26 (Conv2  (None, 64, 64, 128)          295040    ['batch_normalization_121[0][0\n",
      " DTranspose)                                                        ]']                           \n",
      "                                                                                                  \n",
      " dropout_46 (Dropout)        (None, 64, 64, 128)          0         ['conv2d_transpose_26[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_26 (Concatenat  (None, 64, 64, 256)          0         ['dropout_46[0][0]',          \n",
      " e)                                                                  'batch_normalization_111[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_128 (Conv2D)         (None, 64, 64, 128)          295040    ['concatenate_26[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_122 (B  (None, 64, 64, 128)          512       ['conv2d_128[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_129 (Conv2D)         (None, 64, 64, 128)          147584    ['batch_normalization_122[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_123 (B  (None, 64, 64, 128)          512       ['conv2d_129[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_transpose_27 (Conv2  (None, 128, 128, 64)         73792     ['batch_normalization_123[0][0\n",
      " DTranspose)                                                        ]']                           \n",
      "                                                                                                  \n",
      " dropout_47 (Dropout)        (None, 128, 128, 64)         0         ['conv2d_transpose_27[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_27 (Concatenat  (None, 128, 128, 128)        0         ['dropout_47[0][0]',          \n",
      " e)                                                                  'batch_normalization_109[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_130 (Conv2D)         (None, 128, 128, 64)         73792     ['concatenate_27[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_124 (B  (None, 128, 128, 64)         256       ['conv2d_130[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_131 (Conv2D)         (None, 128, 128, 64)         36928     ['batch_normalization_124[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_125 (B  (None, 128, 128, 64)         256       ['conv2d_131[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_132 (Conv2D)         (None, 128, 128, 34)         2210      ['batch_normalization_125[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 34539042 (131.76 MB)\n",
      "Trainable params: 34527266 (131.71 MB)\n",
      "Non-trainable params: 11776 (46.00 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sixth_version_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compile the model with the Sparse Categorical Cross entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sixth_version_model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              metrics=['accuracy', dice_coefficient])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create visualization callback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, ground_truth in train_dataset.take(1):\n",
    "    sixth_version_viz_callback = VizCallback(fifth_version_model, img[FIRST_EXAMPLE_IN_BATCH], ground_truth[FIRST_EXAMPLE_IN_BATCH])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='16-6-2'></a>\n",
    "#### 16.6.2 - Train the model and evaluate him on train and dev datasets\n",
    "\n",
    "In this section we will train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we will train our model for 40 epochs, and we will see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIXTH_VERSION_EPOCHS = 40\n",
    "sixth_version_history = sixth_version_model.fit(train_dataset, epochs=SIXTH_VERSION_EPOCHS, validation_data=dev_dataset, callbacks=[sixth_version_viz_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the history of this training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(sixth_version_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The history of this training process is:\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\sixth_version_first_run_history_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 40</u></b>: History result of the sixth version model in the first training run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions:**\n",
    "\n",
    "* We can see in the history result, that the loss, Accuracy and Dice Cooefficient on the train dataset converge in a pretty good way, but on the dev dataset they are less converging and moving in a zigzag pattern(especially in 20 epoch and 25-30 epochs). \n",
    "\n",
    "* We can see that the model accuracy is preety high, but the Dice Cooefficient score not high, and the reason for this is that our model less good in predict classes that have small amount of data(we talked about this in section 7). We want get a higher results because this still not good results compare to previous models.\n",
    "\n",
    "* We can see that our model little overfit the loss and Accuracy, but in the Dice Coefficient the model do more overfit. Because this model has not dropout layers and has not data augmentation, we will see that as long as we train more our model ,our model will overfit more the train dataset. \n",
    "\n",
    "In conclusion, we want to get a higer performance on the train and dev datasets, while not overfitt the train dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try train the model more in order to see if we can get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIXTH_VERSION_SECOND_RUN_EPOCHS = 40\n",
    "sixth_version_second_run_history = sixth_version_model.fit(train_dataset, epochs=SIXTH_VERSION_SECOND_RUN_EPOCHS, validation_data=dev_dataset, callbacks=[sixth_version_viz_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the history of this training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(sixth_version_second_run_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The history of this training process is:\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\sixth_version_second_run_history_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 41</u></b>: History result of the sixth version model in the second training run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions:**\n",
    "\n",
    "* We can see in the history result, that the loss, Accuracy and Dice Coefficient on the train dataset converge in a good way(less good than first training run), but on the dev dataset they are not converging but more moving in a zigzag pattern, i.e, the model is really unstable on the dev dataset, and this not good.\n",
    "\n",
    "* We can see that our model overfit the loss, Accuracy and Dice Coefficient in the train dataset.  The model performance in loss, Accuracy and Dice Coefficient on the dev dataset become worser. This is because we not use dropout layers and data augmentation, and hence our model overfit the train dataset.\n",
    "\n",
    "In conclusion, we can see that dropout layers and data augmentation are necessary for get good performance on the dev dataset, and in order our model can generalize to new examples.\n",
    "\n",
    "For this model version we will not show the model's predictions, because this version not good compared to the previous models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='16-7'></a>\n",
    "### 16.7 - Seventh model version\n",
    "\n",
    "In this section we will create model that has the regular Unet model architecture with dropout layer after each max pooling layer in the encoder and after each deconvultion layer in the decoder, bad_data_augmentation function and Sparse Categorical Cross entropy loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='16-7-1'></a>\n",
    "#### 16.7.1 - Create the model\n",
    "\n",
    "In this section we will create the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "seventh_version_model  = genral_unet_model(INPUT_SHAPE, bad_data_augmentation, unet_encoder, unet_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_8 (InputLayer)        [(None, 128, 128, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " data_augmentation_layer_7   (None, 128, 128, 3)          0         ['input_8[0][0]']             \n",
      " (DataAugmentationLayer)                                                                          \n",
      "                                                                                                  \n",
      " conv2d_133 (Conv2D)         (None, 128, 128, 64)         1792      ['data_augmentation_layer_7[0]\n",
      "                                                                    [0]']                         \n",
      "                                                                                                  \n",
      " batch_normalization_126 (B  (None, 128, 128, 64)         256       ['conv2d_133[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_134 (Conv2D)         (None, 128, 128, 64)         36928     ['batch_normalization_126[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_127 (B  (None, 128, 128, 64)         256       ['conv2d_134[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_28 (MaxPooli  (None, 64, 64, 64)           0         ['batch_normalization_127[0][0\n",
      " ng2D)                                                              ]']                           \n",
      "                                                                                                  \n",
      " dropout_48 (Dropout)        (None, 64, 64, 64)           0         ['max_pooling2d_28[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_135 (Conv2D)         (None, 64, 64, 128)          73856     ['dropout_48[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_128 (B  (None, 64, 64, 128)          512       ['conv2d_135[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_136 (Conv2D)         (None, 64, 64, 128)          147584    ['batch_normalization_128[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_129 (B  (None, 64, 64, 128)          512       ['conv2d_136[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_29 (MaxPooli  (None, 32, 32, 128)          0         ['batch_normalization_129[0][0\n",
      " ng2D)                                                              ]']                           \n",
      "                                                                                                  \n",
      " dropout_49 (Dropout)        (None, 32, 32, 128)          0         ['max_pooling2d_29[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_137 (Conv2D)         (None, 32, 32, 256)          295168    ['dropout_49[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_130 (B  (None, 32, 32, 256)          1024      ['conv2d_137[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_138 (Conv2D)         (None, 32, 32, 256)          590080    ['batch_normalization_130[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_131 (B  (None, 32, 32, 256)          1024      ['conv2d_138[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_30 (MaxPooli  (None, 16, 16, 256)          0         ['batch_normalization_131[0][0\n",
      " ng2D)                                                              ]']                           \n",
      "                                                                                                  \n",
      " dropout_50 (Dropout)        (None, 16, 16, 256)          0         ['max_pooling2d_30[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_139 (Conv2D)         (None, 16, 16, 512)          1180160   ['dropout_50[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_132 (B  (None, 16, 16, 512)          2048      ['conv2d_139[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_140 (Conv2D)         (None, 16, 16, 512)          2359808   ['batch_normalization_132[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_133 (B  (None, 16, 16, 512)          2048      ['conv2d_140[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_31 (MaxPooli  (None, 8, 8, 512)            0         ['batch_normalization_133[0][0\n",
      " ng2D)                                                              ]']                           \n",
      "                                                                                                  \n",
      " dropout_51 (Dropout)        (None, 8, 8, 512)            0         ['max_pooling2d_31[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_141 (Conv2D)         (None, 8, 8, 1024)           4719616   ['dropout_51[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_134 (B  (None, 8, 8, 1024)           4096      ['conv2d_141[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_142 (Conv2D)         (None, 8, 8, 1024)           9438208   ['batch_normalization_134[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_135 (B  (None, 8, 8, 1024)           4096      ['conv2d_142[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_transpose_28 (Conv2  (None, 16, 16, 512)          4719104   ['batch_normalization_135[0][0\n",
      " DTranspose)                                                        ]']                           \n",
      "                                                                                                  \n",
      " dropout_52 (Dropout)        (None, 16, 16, 512)          0         ['conv2d_transpose_28[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_28 (Concatenat  (None, 16, 16, 1024)         0         ['dropout_52[0][0]',          \n",
      " e)                                                                  'batch_normalization_133[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_143 (Conv2D)         (None, 16, 16, 512)          4719104   ['concatenate_28[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_136 (B  (None, 16, 16, 512)          2048      ['conv2d_143[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_144 (Conv2D)         (None, 16, 16, 512)          2359808   ['batch_normalization_136[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_137 (B  (None, 16, 16, 512)          2048      ['conv2d_144[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_transpose_29 (Conv2  (None, 32, 32, 256)          1179904   ['batch_normalization_137[0][0\n",
      " DTranspose)                                                        ]']                           \n",
      "                                                                                                  \n",
      " dropout_53 (Dropout)        (None, 32, 32, 256)          0         ['conv2d_transpose_29[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_29 (Concatenat  (None, 32, 32, 512)          0         ['dropout_53[0][0]',          \n",
      " e)                                                                  'batch_normalization_131[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_145 (Conv2D)         (None, 32, 32, 256)          1179904   ['concatenate_29[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_138 (B  (None, 32, 32, 256)          1024      ['conv2d_145[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_146 (Conv2D)         (None, 32, 32, 256)          590080    ['batch_normalization_138[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_139 (B  (None, 32, 32, 256)          1024      ['conv2d_146[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_transpose_30 (Conv2  (None, 64, 64, 128)          295040    ['batch_normalization_139[0][0\n",
      " DTranspose)                                                        ]']                           \n",
      "                                                                                                  \n",
      " dropout_54 (Dropout)        (None, 64, 64, 128)          0         ['conv2d_transpose_30[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_30 (Concatenat  (None, 64, 64, 256)          0         ['dropout_54[0][0]',          \n",
      " e)                                                                  'batch_normalization_129[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_147 (Conv2D)         (None, 64, 64, 128)          295040    ['concatenate_30[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_140 (B  (None, 64, 64, 128)          512       ['conv2d_147[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_148 (Conv2D)         (None, 64, 64, 128)          147584    ['batch_normalization_140[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_141 (B  (None, 64, 64, 128)          512       ['conv2d_148[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_transpose_31 (Conv2  (None, 128, 128, 64)         73792     ['batch_normalization_141[0][0\n",
      " DTranspose)                                                        ]']                           \n",
      "                                                                                                  \n",
      " dropout_55 (Dropout)        (None, 128, 128, 64)         0         ['conv2d_transpose_31[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_31 (Concatenat  (None, 128, 128, 128)        0         ['dropout_55[0][0]',          \n",
      " e)                                                                  'batch_normalization_127[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_149 (Conv2D)         (None, 128, 128, 64)         73792     ['concatenate_31[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_142 (B  (None, 128, 128, 64)         256       ['conv2d_149[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_150 (Conv2D)         (None, 128, 128, 64)         36928     ['batch_normalization_142[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_143 (B  (None, 128, 128, 64)         256       ['conv2d_150[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_151 (Conv2D)         (None, 128, 128, 34)         2210      ['batch_normalization_143[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 34539042 (131.76 MB)\n",
      "Trainable params: 34527266 (131.71 MB)\n",
      "Non-trainable params: 11776 (46.00 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seventh_version_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compile the model with the Sparse Categorical Cross entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seventh_version_model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              metrics=['accuracy', dice_coefficient])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create visualization callback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, ground_truth in train_dataset.take(1):\n",
    "    seventh_version_viz_callback = VizCallback(seventh_version_model, img[FIRST_EXAMPLE_IN_BATCH], ground_truth[FIRST_EXAMPLE_IN_BATCH])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='16-7-2'></a>\n",
    "#### 16.7.2 - Train the model and evaluate him on train and dev datasets\n",
    "\n",
    "In this section we will train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we will train our model for 40 epochs, and we will see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEVENTH_VERSION_EPOCHS = 40\n",
    "seventh_version_history = seventh_version_model.fit(train_dataset, epochs=SEVENTH_VERSION_EPOCHS, validation_data=dev_dataset, callbacks=[seventh_version_viz_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the history of this training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(seventh_version_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The history of this training process is:\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\seventh_version_first_run_history_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 42</u></b>: History result of the seventh version model in the first training run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions:**\n",
    "\n",
    "* We can see in the history result, that the loss, Accuracy and Dice Coefficient on the train dataset converge in a pretty good way, but on the dev dataset they are not converging but more moving in a zigzag pattern, i.e, the model is really unstable on the dev dataset, and this not good.\n",
    "\n",
    "* We can see that our model overfit the Accuracy and Dice Coefficient in the train dataset. This is because we use bad data augmentation.\n",
    "\n",
    "* We get worse results in the loss, Accuracy and Dice Coefficient on the train and dev datasets, compare to previous models. This is because we use bad data augmentation.\n",
    "\n",
    "In conclusion, we can see that good data augmentation is necessary for get good performance on the train and dev datasets.\n",
    "\n",
    "For this model version we will not show the model's predictions, because this version not good compared to the previous models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='16-8'></a>\n",
    "### 16.8 - Eighth model version\n",
    "\n",
    "In this section we will create model that has the regular Unet model architecture with dropout layer after each max pooling layer in the encoder and after each deconvultion layer in the decoder, basic_data_augmentation function, Weighted Sparse Categorical Cross entropy loss function and Learning Rate scheduler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='16-8-1'></a>\n",
    "#### 16.8.1 - Create the model\n",
    "\n",
    "In this section we will create the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "eighth_version_model  = genral_unet_model(INPUT_SHAPE, basic_data_augmentation, unet_encoder, unet_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_9 (InputLayer)        [(None, 128, 128, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " data_augmentation_layer_8   (None, 128, 128, 3)          0         ['input_9[0][0]']             \n",
      " (DataAugmentationLayer)                                                                          \n",
      "                                                                                                  \n",
      " conv2d_152 (Conv2D)         (None, 128, 128, 64)         1792      ['data_augmentation_layer_8[0]\n",
      "                                                                    [0]']                         \n",
      "                                                                                                  \n",
      " batch_normalization_144 (B  (None, 128, 128, 64)         256       ['conv2d_152[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_153 (Conv2D)         (None, 128, 128, 64)         36928     ['batch_normalization_144[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_145 (B  (None, 128, 128, 64)         256       ['conv2d_153[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_32 (MaxPooli  (None, 64, 64, 64)           0         ['batch_normalization_145[0][0\n",
      " ng2D)                                                              ]']                           \n",
      "                                                                                                  \n",
      " dropout_56 (Dropout)        (None, 64, 64, 64)           0         ['max_pooling2d_32[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_154 (Conv2D)         (None, 64, 64, 128)          73856     ['dropout_56[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_146 (B  (None, 64, 64, 128)          512       ['conv2d_154[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_155 (Conv2D)         (None, 64, 64, 128)          147584    ['batch_normalization_146[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_147 (B  (None, 64, 64, 128)          512       ['conv2d_155[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_33 (MaxPooli  (None, 32, 32, 128)          0         ['batch_normalization_147[0][0\n",
      " ng2D)                                                              ]']                           \n",
      "                                                                                                  \n",
      " dropout_57 (Dropout)        (None, 32, 32, 128)          0         ['max_pooling2d_33[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_156 (Conv2D)         (None, 32, 32, 256)          295168    ['dropout_57[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_148 (B  (None, 32, 32, 256)          1024      ['conv2d_156[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_157 (Conv2D)         (None, 32, 32, 256)          590080    ['batch_normalization_148[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_149 (B  (None, 32, 32, 256)          1024      ['conv2d_157[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_34 (MaxPooli  (None, 16, 16, 256)          0         ['batch_normalization_149[0][0\n",
      " ng2D)                                                              ]']                           \n",
      "                                                                                                  \n",
      " dropout_58 (Dropout)        (None, 16, 16, 256)          0         ['max_pooling2d_34[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_158 (Conv2D)         (None, 16, 16, 512)          1180160   ['dropout_58[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_150 (B  (None, 16, 16, 512)          2048      ['conv2d_158[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_159 (Conv2D)         (None, 16, 16, 512)          2359808   ['batch_normalization_150[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_151 (B  (None, 16, 16, 512)          2048      ['conv2d_159[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_35 (MaxPooli  (None, 8, 8, 512)            0         ['batch_normalization_151[0][0\n",
      " ng2D)                                                              ]']                           \n",
      "                                                                                                  \n",
      " dropout_59 (Dropout)        (None, 8, 8, 512)            0         ['max_pooling2d_35[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_160 (Conv2D)         (None, 8, 8, 1024)           4719616   ['dropout_59[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_152 (B  (None, 8, 8, 1024)           4096      ['conv2d_160[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_161 (Conv2D)         (None, 8, 8, 1024)           9438208   ['batch_normalization_152[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_153 (B  (None, 8, 8, 1024)           4096      ['conv2d_161[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_transpose_32 (Conv2  (None, 16, 16, 512)          4719104   ['batch_normalization_153[0][0\n",
      " DTranspose)                                                        ]']                           \n",
      "                                                                                                  \n",
      " dropout_60 (Dropout)        (None, 16, 16, 512)          0         ['conv2d_transpose_32[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_32 (Concatenat  (None, 16, 16, 1024)         0         ['dropout_60[0][0]',          \n",
      " e)                                                                  'batch_normalization_151[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_162 (Conv2D)         (None, 16, 16, 512)          4719104   ['concatenate_32[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_154 (B  (None, 16, 16, 512)          2048      ['conv2d_162[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_163 (Conv2D)         (None, 16, 16, 512)          2359808   ['batch_normalization_154[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_155 (B  (None, 16, 16, 512)          2048      ['conv2d_163[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_transpose_33 (Conv2  (None, 32, 32, 256)          1179904   ['batch_normalization_155[0][0\n",
      " DTranspose)                                                        ]']                           \n",
      "                                                                                                  \n",
      " dropout_61 (Dropout)        (None, 32, 32, 256)          0         ['conv2d_transpose_33[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_33 (Concatenat  (None, 32, 32, 512)          0         ['dropout_61[0][0]',          \n",
      " e)                                                                  'batch_normalization_149[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_164 (Conv2D)         (None, 32, 32, 256)          1179904   ['concatenate_33[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_156 (B  (None, 32, 32, 256)          1024      ['conv2d_164[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_165 (Conv2D)         (None, 32, 32, 256)          590080    ['batch_normalization_156[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_157 (B  (None, 32, 32, 256)          1024      ['conv2d_165[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_transpose_34 (Conv2  (None, 64, 64, 128)          295040    ['batch_normalization_157[0][0\n",
      " DTranspose)                                                        ]']                           \n",
      "                                                                                                  \n",
      " dropout_62 (Dropout)        (None, 64, 64, 128)          0         ['conv2d_transpose_34[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_34 (Concatenat  (None, 64, 64, 256)          0         ['dropout_62[0][0]',          \n",
      " e)                                                                  'batch_normalization_147[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_166 (Conv2D)         (None, 64, 64, 128)          295040    ['concatenate_34[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_158 (B  (None, 64, 64, 128)          512       ['conv2d_166[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_167 (Conv2D)         (None, 64, 64, 128)          147584    ['batch_normalization_158[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_159 (B  (None, 64, 64, 128)          512       ['conv2d_167[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_transpose_35 (Conv2  (None, 128, 128, 64)         73792     ['batch_normalization_159[0][0\n",
      " DTranspose)                                                        ]']                           \n",
      "                                                                                                  \n",
      " dropout_63 (Dropout)        (None, 128, 128, 64)         0         ['conv2d_transpose_35[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_35 (Concatenat  (None, 128, 128, 128)        0         ['dropout_63[0][0]',          \n",
      " e)                                                                  'batch_normalization_145[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_168 (Conv2D)         (None, 128, 128, 64)         73792     ['concatenate_35[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_160 (B  (None, 128, 128, 64)         256       ['conv2d_168[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_169 (Conv2D)         (None, 128, 128, 64)         36928     ['batch_normalization_160[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_161 (B  (None, 128, 128, 64)         256       ['conv2d_169[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_170 (Conv2D)         (None, 128, 128, 34)         2210      ['batch_normalization_161[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 34539042 (131.76 MB)\n",
      "Trainable params: 34527266 (131.71 MB)\n",
      "Non-trainable params: 11776 (46.00 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "eighth_version_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compile the model with the Weighted Sparse Categorical Cross entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eighth_version_model.compile(optimizer='adam',\n",
    "              loss=weighted_sparse_categorical_crossentropy,\n",
    "              metrics=['accuracy', dice_coefficient])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create visualization callback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, ground_truth in train_dataset.take(1):\n",
    "    eighth_version_viz_callback = VizCallback(eighth_version_model, img[FIRST_EXAMPLE_IN_BATCH], ground_truth[FIRST_EXAMPLE_IN_BATCH])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create Learning Rate scheduler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_LR_EIGHTH_VERSION = 0.001\n",
    "NEW_LR_EIGHTH_VERSION = 0.0001\n",
    "NUM_OF_EPOCHS_FOR_INITIAL_LR_EIGHTH_VERSION = 20\n",
    "    \n",
    "eighth_version_lr_callback = LearningRateScheduler(lambda epoch: lr_scheduler(epoch, INITIAL_LR_EIGHTH_VERSION, NEW_LR_EIGHTH_VERSION, NUM_OF_EPOCHS_FOR_INITIAL_LR_EIGHTH_VERSION))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='16-8-2'></a>\n",
    "#### 16.8.2 - Train the model and evaluate him on train and dev datasets\n",
    "\n",
    "In this section we will train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we will train our model for 40 epochs, and we will see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EIGHTH_VERSION_EPOCHS = 40\n",
    "eighth_version_history = eighth_version_model.fit(train_dataset, epochs=EIGHTH_VERSION_EPOCHS, validation_data=dev_dataset, callbacks=[eighth_version_viz_callback, eighth_version_lr_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the history of this training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(eighth_version_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The history of this training process is:\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\eighth_version_first_run_history_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 43</u></b>: History result of the eighth version model in the first training run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see the results of the model in the second epoch and in the last epoch:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\eighth_version_first_run_second_epoch_result.png\" style=\"width:200;height:200;\">\n",
    "    <img src=\"Images\\eighth_version_first_run_last_epoch_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 44</u></b>: Results of the eighth version model in the first training run in the second epoch and in the last epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions:**\n",
    "\n",
    "* We can see in the history result, that the loss, Accuracy and Dice Cooefficient on the train dataset and on the dev dataset converge in a pretty good way. This is because we use Learning Rate scheduler and from epoch 20 we take little steps in the training process.\n",
    "\n",
    "* We can see that the model accuracy is preety high, but the Dice Cooefficient score not high, and the reason for this is that our model less good in predict classes that have small amount of data(we talked about this in section 7).\n",
    "\n",
    "* We can see that our model little overfit the loss, Accuracy and Dice Coefficient on train dataset. He less overfit the train dataset compare to previous models.\n",
    "\n",
    "In conclusion, we need to try get better results in the dev dataset while not overfit the train dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create Learning Rate scheduler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OF_EPOCHS_FOR_INITIAL_LR_EIGHTH_VERSION_SECOND_RUN = 0\n",
    "    \n",
    "eighth_version_second_run_lr_callback = LearningRateScheduler(lambda epoch: lr_scheduler(epoch, INITIAL_LR_EIGHTH_VERSION, NEW_LR_EIGHTH_VERSION, NUM_OF_EPOCHS_FOR_INITIAL_LR_EIGHTH_VERSION_SECOND_RUN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try train the model more in order to see if we can get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EIGHTH_VERSION_SECOND_RUN_EPOCHS = 40\n",
    "eighth_version_second_run_history = eighth_version_model.fit(train_dataset, epochs=EIGHTH_VERSION_SECOND_RUN_EPOCHS, validation_data=dev_dataset, callbacks=[eighth_version_viz_callback, eighth_version_second_run_lr_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the history of this training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(eighth_version_second_run_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The history of this training process is:\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\eighth_version_second_run_history_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 45</u></b>: History result of the eighth version model in the second training run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can compare the results of the model in the last epoch of the first training run and in the last epoch of the second training:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\eighth_version_first_run_last_epoch_result.png\" style=\"width:200;height:200;\">\n",
    "    <img src=\"Images\\eighth_version_second_run_last_epoch_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 46</u></b>: Results of the eighth version model in the first training run in the last epoch and in the second training run in the last epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions:**\n",
    "\n",
    "* We can see in the history result, that the loss, Accuracy and Dice Cooefficient on the train dataset converge in a pretty good way, but on the dev dataset our model become less stable, and moving in a zigzag pattern.\n",
    "\n",
    "* We get better results in the last epoch in the second running compare to the last epoch in the first running. Now our model can predict correctly more little detials in the image.\n",
    "We can see that our model much improved in Accuracy and Dice Cooefficient.\n",
    "\n",
    "* We can see that our model more overfit the train dataset.\n",
    "\n",
    "In overall, the model performance improved on the train dataset, and the performance on the the dev dataset didn't get any worse, but our model overfit the train dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try train the model little more in order to see if we can get better results in the dev dataset and maybee less overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create Learning Rate scheduler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OF_EPOCHS_FOR_INITIAL_LR_EIGHTH_VERSION_THIRD_RUN = 0\n",
    "    \n",
    "eighth_version_third_run_lr_callback = LearningRateScheduler(lambda epoch: lr_scheduler(epoch, INITIAL_LR_EIGHTH_VERSION, NEW_LR_EIGHTH_VERSION, NUM_OF_EPOCHS_FOR_INITIAL_LR_EIGHTH_VERSION_THIRD_RUN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EIGHTH_VERSION_THIRD_RUN_EPOCHS = 20\n",
    "eighth_version_third_run_history = eighth_version_model.fit(train_dataset, epochs=EIGHTH_VERSION_THIRD_RUN_EPOCHS, validation_data=dev_dataset, callbacks=[eighth_version_viz_callback, eighth_version_third_run_lr_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the history of this training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(eighth_version_third_run_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The history of this training process is:\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\eighth_version_third_run_history_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 47</u></b>: History result of the eighth version model in the third training run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can compare the results of the model in the last epoch of the second training run and in the last epoch of the third training:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\eighth_version_second_run_last_epoch_result.png\" style=\"width:200;height:200;\">\n",
    "    <img src=\"Images\\eighth_version_third_run_last_epoch_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 48</u></b>: Results of the third version model in the second training run in the last epoch and in the third training run in the last epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions:**\n",
    "\n",
    "* We can see in the history result, that the loss, Accuracy and Dice Cooefficient on the train dataset converge in a pretty good way, but on the dev dataset our model not stable, and moving in a zigzag pattern.\n",
    "\n",
    "* We get better results in the last epoch in the third running compare to the last epoch in the second running in the train dataset, but our model performance getting worse on the dev dataset compare to the second running.\n",
    "\n",
    "* We can see that our model much more overfit the train dataset.\n",
    "\n",
    "In overall, the model performance improved on the train dataset, and the model performance on the the dev dataset gets worse, and our model overfit the train dataset.\n",
    "\n",
    "Hence, we will use the eighth version model in the second running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='16-8-3'></a>\n",
    "#### 16.8.3 - Model's predictions on the train and dev datasets\n",
    "\n",
    "In this section we will show examples of model's predictions on the train and dev datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the model's predictions on train dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_predictions(eighth_version_model, train_dataset, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Six examples of model's predictions on the train dataset are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\eighth_version_predictions_train_part1.png\" style=\"width:200;height:200;\">\n",
    "    <img src=\"Images\\eighth_version_predictions_train_part2.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 49</u></b>: Model's predictions on the train dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the model's predictions on dev dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_predictions(eighth_version_model, dev_dataset, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Six examples of model's predictions on the dev dataset are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\eighth_version_predictions_dev_part1.png\" style=\"width:200;height:200;\">\n",
    "    <img src=\"Images\\eighth_version_predictions_dev_part2.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 50</u></b>: Model's predictions on the dev dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, we get good results and the loss, Accuracy and Dice Cooefficient on the dev dataset were more converge and were not move in a zigzag pattern, but we still need to try to improve the model performance espically in the dev dataset, i.e, we need that our model will be more genral and less overfitt the train dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='16-9'></a>\n",
    "### 16.9 - Nineth model version\n",
    "\n",
    "In this section we will create model that has the regular Unet model architecture with dropout layer after each max pooling layer in the encoder and after each deconvultion layer in the decoder, basic_data_augmentation function, Weighted Sparse Categorical Cross entropy loss function and Dropout scheduler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='16-9-1'></a>\n",
    "#### 16.9.1 - Create the model\n",
    "\n",
    "In this section we will create the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "nineth_version_model  = genral_unet_model(INPUT_SHAPE, basic_data_augmentation, unet_encoder, unet_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_10 (InputLayer)       [(None, 128, 128, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " data_augmentation_layer_9   (None, 128, 128, 3)          0         ['input_10[0][0]']            \n",
      " (DataAugmentationLayer)                                                                          \n",
      "                                                                                                  \n",
      " conv2d_171 (Conv2D)         (None, 128, 128, 64)         1792      ['data_augmentation_layer_9[0]\n",
      "                                                                    [0]']                         \n",
      "                                                                                                  \n",
      " batch_normalization_162 (B  (None, 128, 128, 64)         256       ['conv2d_171[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_172 (Conv2D)         (None, 128, 128, 64)         36928     ['batch_normalization_162[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_163 (B  (None, 128, 128, 64)         256       ['conv2d_172[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_36 (MaxPooli  (None, 64, 64, 64)           0         ['batch_normalization_163[0][0\n",
      " ng2D)                                                              ]']                           \n",
      "                                                                                                  \n",
      " dropout_64 (Dropout)        (None, 64, 64, 64)           0         ['max_pooling2d_36[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_173 (Conv2D)         (None, 64, 64, 128)          73856     ['dropout_64[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_164 (B  (None, 64, 64, 128)          512       ['conv2d_173[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_174 (Conv2D)         (None, 64, 64, 128)          147584    ['batch_normalization_164[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_165 (B  (None, 64, 64, 128)          512       ['conv2d_174[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_37 (MaxPooli  (None, 32, 32, 128)          0         ['batch_normalization_165[0][0\n",
      " ng2D)                                                              ]']                           \n",
      "                                                                                                  \n",
      " dropout_65 (Dropout)        (None, 32, 32, 128)          0         ['max_pooling2d_37[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_175 (Conv2D)         (None, 32, 32, 256)          295168    ['dropout_65[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_166 (B  (None, 32, 32, 256)          1024      ['conv2d_175[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_176 (Conv2D)         (None, 32, 32, 256)          590080    ['batch_normalization_166[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_167 (B  (None, 32, 32, 256)          1024      ['conv2d_176[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_38 (MaxPooli  (None, 16, 16, 256)          0         ['batch_normalization_167[0][0\n",
      " ng2D)                                                              ]']                           \n",
      "                                                                                                  \n",
      " dropout_66 (Dropout)        (None, 16, 16, 256)          0         ['max_pooling2d_38[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_177 (Conv2D)         (None, 16, 16, 512)          1180160   ['dropout_66[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_168 (B  (None, 16, 16, 512)          2048      ['conv2d_177[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_178 (Conv2D)         (None, 16, 16, 512)          2359808   ['batch_normalization_168[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_169 (B  (None, 16, 16, 512)          2048      ['conv2d_178[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_39 (MaxPooli  (None, 8, 8, 512)            0         ['batch_normalization_169[0][0\n",
      " ng2D)                                                              ]']                           \n",
      "                                                                                                  \n",
      " dropout_67 (Dropout)        (None, 8, 8, 512)            0         ['max_pooling2d_39[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_179 (Conv2D)         (None, 8, 8, 1024)           4719616   ['dropout_67[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_170 (B  (None, 8, 8, 1024)           4096      ['conv2d_179[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_180 (Conv2D)         (None, 8, 8, 1024)           9438208   ['batch_normalization_170[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_171 (B  (None, 8, 8, 1024)           4096      ['conv2d_180[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_transpose_36 (Conv2  (None, 16, 16, 512)          4719104   ['batch_normalization_171[0][0\n",
      " DTranspose)                                                        ]']                           \n",
      "                                                                                                  \n",
      " dropout_68 (Dropout)        (None, 16, 16, 512)          0         ['conv2d_transpose_36[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_36 (Concatenat  (None, 16, 16, 1024)         0         ['dropout_68[0][0]',          \n",
      " e)                                                                  'batch_normalization_169[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_181 (Conv2D)         (None, 16, 16, 512)          4719104   ['concatenate_36[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_172 (B  (None, 16, 16, 512)          2048      ['conv2d_181[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_182 (Conv2D)         (None, 16, 16, 512)          2359808   ['batch_normalization_172[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_173 (B  (None, 16, 16, 512)          2048      ['conv2d_182[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_transpose_37 (Conv2  (None, 32, 32, 256)          1179904   ['batch_normalization_173[0][0\n",
      " DTranspose)                                                        ]']                           \n",
      "                                                                                                  \n",
      " dropout_69 (Dropout)        (None, 32, 32, 256)          0         ['conv2d_transpose_37[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_37 (Concatenat  (None, 32, 32, 512)          0         ['dropout_69[0][0]',          \n",
      " e)                                                                  'batch_normalization_167[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_183 (Conv2D)         (None, 32, 32, 256)          1179904   ['concatenate_37[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_174 (B  (None, 32, 32, 256)          1024      ['conv2d_183[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_184 (Conv2D)         (None, 32, 32, 256)          590080    ['batch_normalization_174[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_175 (B  (None, 32, 32, 256)          1024      ['conv2d_184[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_transpose_38 (Conv2  (None, 64, 64, 128)          295040    ['batch_normalization_175[0][0\n",
      " DTranspose)                                                        ]']                           \n",
      "                                                                                                  \n",
      " dropout_70 (Dropout)        (None, 64, 64, 128)          0         ['conv2d_transpose_38[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_38 (Concatenat  (None, 64, 64, 256)          0         ['dropout_70[0][0]',          \n",
      " e)                                                                  'batch_normalization_165[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_185 (Conv2D)         (None, 64, 64, 128)          295040    ['concatenate_38[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_176 (B  (None, 64, 64, 128)          512       ['conv2d_185[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_186 (Conv2D)         (None, 64, 64, 128)          147584    ['batch_normalization_176[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_177 (B  (None, 64, 64, 128)          512       ['conv2d_186[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_transpose_39 (Conv2  (None, 128, 128, 64)         73792     ['batch_normalization_177[0][0\n",
      " DTranspose)                                                        ]']                           \n",
      "                                                                                                  \n",
      " dropout_71 (Dropout)        (None, 128, 128, 64)         0         ['conv2d_transpose_39[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_39 (Concatenat  (None, 128, 128, 128)        0         ['dropout_71[0][0]',          \n",
      " e)                                                                  'batch_normalization_163[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_187 (Conv2D)         (None, 128, 128, 64)         73792     ['concatenate_39[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_178 (B  (None, 128, 128, 64)         256       ['conv2d_187[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_188 (Conv2D)         (None, 128, 128, 64)         36928     ['batch_normalization_178[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_179 (B  (None, 128, 128, 64)         256       ['conv2d_188[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_189 (Conv2D)         (None, 128, 128, 34)         2210      ['batch_normalization_179[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 34539042 (131.76 MB)\n",
      "Trainable params: 34527266 (131.71 MB)\n",
      "Non-trainable params: 11776 (46.00 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nineth_version_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compile the model with the Weighted Sparse Categorical Cross entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nineth_version_model.compile(optimizer='adam',\n",
    "              loss=weighted_sparse_categorical_crossentropy,\n",
    "              metrics=['accuracy', dice_coefficient])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create visualization callback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, ground_truth in train_dataset.take(1):\n",
    "    nineth_version_viz_callback = VizCallback(nineth_version_model, img[FIRST_EXAMPLE_IN_BATCH], ground_truth[FIRST_EXAMPLE_IN_BATCH])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create Dropout scheduler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NINETH_VERSION_DROP_SCHEDULE = {13:0.4, 23:0.5}  \n",
    "nineth_version_dropout_scheduler = DropoutScheduler(drop_schedule=NINETH_VERSION_DROP_SCHEDULE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='16-9-2'></a>\n",
    "#### 16.9.2 - Train the model and evaluate him on train and dev datasets\n",
    "\n",
    "In this section we will train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we will train our model for 40 epochs, and we will see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NINETH_VERSION_EPOCHS = 40\n",
    "nineth_version_history = nineth_version_model.fit(train_dataset, epochs=NINETH_VERSION_EPOCHS, validation_data=dev_dataset, callbacks=[nineth_version_viz_callback, nineth_version_dropout_scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the history of this training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(nineth_version_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The history of this training process is:\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\nineth_version_first_run_history_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 51</u></b>: History result of the nineth version model in the first training run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see the results of the third version model in the first training run in the last epoch(up) and of the nineth version model in the first training run in the last epoch(down):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\third_version_first_run_last_epoch_result.png\" style=\"width:200;height:200;\">\n",
    "    <img src=\"Images\\nineth_version_first_run_last_epoch_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 52</u></b>: Results of the third version model in the first training run in the last epoch(up) and of the nineth version model in the first training run in the last epoch(down) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions:**\n",
    "\n",
    "* We can see in the history result, that the loss, Accuracy and Dice Cooefficient on the train dataset converge in a pretty good way, but on the dev dataset they are less converging but more moving in a zigzag pattern(ecpeically in epochs 0-5 and 25-30).\n",
    "\n",
    "* We can see that our model overfit the loss and Accuracy and Dice Coefficient on train dataset.\n",
    "\n",
    "Although we apply Dropout scheduler, we still have the problem of overfitting and bad converge in loss and Accuracy and Dice Coefficient on dev dataset. Hence, we don't sucess improve the third version model, and therefore we will not continue with this model.\n",
    "\n",
    "Hence, for this model version we will not show the model's predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='16-10'></a>\n",
    "### 16.10 - Tenth model version\n",
    "\n",
    "In this section we will create model that has the regular Unet model architecture with dropout layer after each max pooling layer in the encoder and after each deconvultion layer in the decoder, basic_data_augmentation function, Weighted Sparse Categorical Cross entropy loss function and Learning Rate and Dropout scheduler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='16-10-1'></a>\n",
    "#### 16.10.1 - Create the model\n",
    "\n",
    "In this section we will create the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tenth_version_model  = genral_unet_model(INPUT_SHAPE, basic_data_augmentation, unet_encoder, unet_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_11 (InputLayer)       [(None, 128, 128, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " data_augmentation_layer_10  (None, 128, 128, 3)          0         ['input_11[0][0]']            \n",
      "  (DataAugmentationLayer)                                                                         \n",
      "                                                                                                  \n",
      " conv2d_190 (Conv2D)         (None, 128, 128, 64)         1792      ['data_augmentation_layer_10[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " batch_normalization_180 (B  (None, 128, 128, 64)         256       ['conv2d_190[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_191 (Conv2D)         (None, 128, 128, 64)         36928     ['batch_normalization_180[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_181 (B  (None, 128, 128, 64)         256       ['conv2d_191[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_40 (MaxPooli  (None, 64, 64, 64)           0         ['batch_normalization_181[0][0\n",
      " ng2D)                                                              ]']                           \n",
      "                                                                                                  \n",
      " dropout_72 (Dropout)        (None, 64, 64, 64)           0         ['max_pooling2d_40[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_192 (Conv2D)         (None, 64, 64, 128)          73856     ['dropout_72[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_182 (B  (None, 64, 64, 128)          512       ['conv2d_192[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_193 (Conv2D)         (None, 64, 64, 128)          147584    ['batch_normalization_182[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_183 (B  (None, 64, 64, 128)          512       ['conv2d_193[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_41 (MaxPooli  (None, 32, 32, 128)          0         ['batch_normalization_183[0][0\n",
      " ng2D)                                                              ]']                           \n",
      "                                                                                                  \n",
      " dropout_73 (Dropout)        (None, 32, 32, 128)          0         ['max_pooling2d_41[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_194 (Conv2D)         (None, 32, 32, 256)          295168    ['dropout_73[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_184 (B  (None, 32, 32, 256)          1024      ['conv2d_194[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_195 (Conv2D)         (None, 32, 32, 256)          590080    ['batch_normalization_184[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_185 (B  (None, 32, 32, 256)          1024      ['conv2d_195[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_42 (MaxPooli  (None, 16, 16, 256)          0         ['batch_normalization_185[0][0\n",
      " ng2D)                                                              ]']                           \n",
      "                                                                                                  \n",
      " dropout_74 (Dropout)        (None, 16, 16, 256)          0         ['max_pooling2d_42[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_196 (Conv2D)         (None, 16, 16, 512)          1180160   ['dropout_74[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_186 (B  (None, 16, 16, 512)          2048      ['conv2d_196[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_197 (Conv2D)         (None, 16, 16, 512)          2359808   ['batch_normalization_186[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_187 (B  (None, 16, 16, 512)          2048      ['conv2d_197[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_43 (MaxPooli  (None, 8, 8, 512)            0         ['batch_normalization_187[0][0\n",
      " ng2D)                                                              ]']                           \n",
      "                                                                                                  \n",
      " dropout_75 (Dropout)        (None, 8, 8, 512)            0         ['max_pooling2d_43[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_198 (Conv2D)         (None, 8, 8, 1024)           4719616   ['dropout_75[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_188 (B  (None, 8, 8, 1024)           4096      ['conv2d_198[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_199 (Conv2D)         (None, 8, 8, 1024)           9438208   ['batch_normalization_188[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_189 (B  (None, 8, 8, 1024)           4096      ['conv2d_199[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_transpose_40 (Conv2  (None, 16, 16, 512)          4719104   ['batch_normalization_189[0][0\n",
      " DTranspose)                                                        ]']                           \n",
      "                                                                                                  \n",
      " dropout_76 (Dropout)        (None, 16, 16, 512)          0         ['conv2d_transpose_40[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_40 (Concatenat  (None, 16, 16, 1024)         0         ['dropout_76[0][0]',          \n",
      " e)                                                                  'batch_normalization_187[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_200 (Conv2D)         (None, 16, 16, 512)          4719104   ['concatenate_40[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_190 (B  (None, 16, 16, 512)          2048      ['conv2d_200[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_201 (Conv2D)         (None, 16, 16, 512)          2359808   ['batch_normalization_190[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_191 (B  (None, 16, 16, 512)          2048      ['conv2d_201[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_transpose_41 (Conv2  (None, 32, 32, 256)          1179904   ['batch_normalization_191[0][0\n",
      " DTranspose)                                                        ]']                           \n",
      "                                                                                                  \n",
      " dropout_77 (Dropout)        (None, 32, 32, 256)          0         ['conv2d_transpose_41[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_41 (Concatenat  (None, 32, 32, 512)          0         ['dropout_77[0][0]',          \n",
      " e)                                                                  'batch_normalization_185[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_202 (Conv2D)         (None, 32, 32, 256)          1179904   ['concatenate_41[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_192 (B  (None, 32, 32, 256)          1024      ['conv2d_202[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_203 (Conv2D)         (None, 32, 32, 256)          590080    ['batch_normalization_192[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_193 (B  (None, 32, 32, 256)          1024      ['conv2d_203[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_transpose_42 (Conv2  (None, 64, 64, 128)          295040    ['batch_normalization_193[0][0\n",
      " DTranspose)                                                        ]']                           \n",
      "                                                                                                  \n",
      " dropout_78 (Dropout)        (None, 64, 64, 128)          0         ['conv2d_transpose_42[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_42 (Concatenat  (None, 64, 64, 256)          0         ['dropout_78[0][0]',          \n",
      " e)                                                                  'batch_normalization_183[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_204 (Conv2D)         (None, 64, 64, 128)          295040    ['concatenate_42[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_194 (B  (None, 64, 64, 128)          512       ['conv2d_204[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_205 (Conv2D)         (None, 64, 64, 128)          147584    ['batch_normalization_194[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_195 (B  (None, 64, 64, 128)          512       ['conv2d_205[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_transpose_43 (Conv2  (None, 128, 128, 64)         73792     ['batch_normalization_195[0][0\n",
      " DTranspose)                                                        ]']                           \n",
      "                                                                                                  \n",
      " dropout_79 (Dropout)        (None, 128, 128, 64)         0         ['conv2d_transpose_43[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_43 (Concatenat  (None, 128, 128, 128)        0         ['dropout_79[0][0]',          \n",
      " e)                                                                  'batch_normalization_181[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_206 (Conv2D)         (None, 128, 128, 64)         73792     ['concatenate_43[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_196 (B  (None, 128, 128, 64)         256       ['conv2d_206[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_207 (Conv2D)         (None, 128, 128, 64)         36928     ['batch_normalization_196[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_197 (B  (None, 128, 128, 64)         256       ['conv2d_207[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_208 (Conv2D)         (None, 128, 128, 34)         2210      ['batch_normalization_197[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 34539042 (131.76 MB)\n",
      "Trainable params: 34527266 (131.71 MB)\n",
      "Non-trainable params: 11776 (46.00 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tenth_version_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compile the model with the Weighted Sparse Categorical Cross entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tenth_version_model.compile(optimizer='adam',\n",
    "              loss=weighted_sparse_categorical_crossentropy,\n",
    "              metrics=['accuracy', dice_coefficient])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create visualization callback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, ground_truth in train_dataset.take(1):\n",
    "    tenth_version_viz_callback = VizCallback(tenth_version_model, img[FIRST_EXAMPLE_IN_BATCH], ground_truth[FIRST_EXAMPLE_IN_BATCH])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create Dropout scheduler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TENTH_VERSION_DROP_SCHEDULE = {13: 0.35, 22: 0.4}\n",
    "tenth_version_dropout_scheduler = DropoutScheduler(drop_schedule=TENTH_VERSION_DROP_SCHEDULE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create Learning Rate scheduler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_LR_TENTH_VERSION = 0.001\n",
    "NEW_LR_TENTH_VERSION = 0.0001\n",
    "NUM_OF_EPOCHS_FOR_INITIAL_LR_TENTH_VERSION = 20\n",
    "    \n",
    "tenth_version_lr_callback = LearningRateScheduler(lambda epoch: lr_scheduler(epoch, INITIAL_LR_TENTH_VERSION, NEW_LR_TENTH_VERSION, NUM_OF_EPOCHS_FOR_INITIAL_LR_TENTH_VERSION))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='16-10-2'></a>\n",
    "#### 16.10.2 - Train the model and evaluate him on train and dev datasets\n",
    "\n",
    "In this section we will train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we will train our model for 40 epochs, and we will see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TENTH_VERSION_EPOCHS = 40\n",
    "tenth_version_history = tenth_version_model.fit(train_dataset, epochs=TENTH_VERSION_EPOCHS, validation_data=dev_dataset, callbacks=[tenth_version_viz_callback, tenth_version_lr_callback, tenth_version_dropout_scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the history of this training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(tenth_version_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The history of this training process is:\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\tenth_version_first_run_history_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 53</u></b>: History result of the tenth version model in the first training run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see the results of the model in the second epoch and in the last epoch:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\tenth_version_first_run_second_epoch_result.png\" style=\"width:200;height:200;\">\n",
    "    <img src=\"Images\\tenth_version_first_run_last_epoch_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 54</u></b>: Results of the tenth version model in the first training run in the second epoch and in the last epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions:**\n",
    "\n",
    "* We can see in the history result, that the loss, Accuracy and Dice Cooefficient on the train dataset and on the dev dataset converge in a pretty good way. This is because we use Learning Rate scheduler and from epoch 20 we take little steps in the training process.\n",
    "\n",
    "* We can see that the model accuracy is preety high, but the Dice Cooefficient score not high, and the reason for this is that our model less good in predict classes that have small amount of data(we talked about this in section 7). Compared to previous models we need to try to get higer preformance.\n",
    "\n",
    "* We can see that our model little overfit the loss, Accuracy and Dice Coefficient on train dataset. This is because we use Dropout scheduler.\n",
    "\n",
    "In conclusion, we need to try get better results in the dev dataset while not overfit the train dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try train the model more in order to see if we can get better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create Dropout scheduler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TENTH_VERSION_SECOND_RUN_DROP_SCHEDULE = {1: 0.4} \n",
    "tenth_version_second_run_dropout_scheduler = DropoutScheduler(drop_schedule=TENTH_VERSION_SECOND_RUN_DROP_SCHEDULE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create Learning Rate scheduler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OF_EPOCHS_FOR_INITIAL_LR_TENTH_VERSION_SECOND_RUN = 20\n",
    "    \n",
    "tenth_version_second_run_lr_callback = LearningRateScheduler(lambda epoch: lr_scheduler(epoch, INITIAL_LR_TENTH_VERSION, NEW_LR_TENTH_VERSION, NUM_OF_EPOCHS_FOR_INITIAL_LR_TENTH_VERSION_SECOND_RUN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TENTH_VERSION_SECOND_RUN_EPOCHS = 40\n",
    "tenth_version_second_run_history = tenth_version_model.fit(train_dataset, epochs=TENTH_VERSION_SECOND_RUN_EPOCHS, validation_data=dev_dataset, callbacks=[tenth_version_viz_callback, tenth_version_second_run_lr_callback, tenth_version_second_run_dropout_scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the history of this training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(tenth_version_second_run_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The history of this training process is:\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\tenth_version_second_run_history_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 55</u></b>: History result of the tenth version model in the second training run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can compare the results of the model in the last epoch of the first training run and in the last epoch of the second training:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\tenth_version_first_run_last_epoch_result.png\" style=\"width:200;height:200;\">\n",
    "    <img src=\"Images\\tenth_version_second_run_last_epoch_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 56</u></b>: Results of the tenth version model in the first training run in the last epoch and in the second training run in the last epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions:**\n",
    "\n",
    "* We can see in the history result, that the loss, Accuracy and Dice Cooefficient on the train dataset converge in a pretty good way(but less converge compare to the first training), but on the dev dataset our model become much less stable, and moving in a zigzag pattern.\n",
    "\n",
    "* We get better results in the last epoch in the second running compare to the last epoch in the first running in the train dataset, but our model performance not improved much on the dev dataset compare to the first running.\n",
    "\n",
    "* We can see that our model much more overfit the train dataset.\n",
    "\n",
    "In overall, the model performance improved on the train dataset, and the model performance on the the dev dataset not improved, and our model overfit the train dataset(much more compered to the first training run). In genral the model performance is not good compared pervious models, and thus adding Learning Rate and Dropout scheduler not helped us, and we will not continue to train this model.\n",
    "\n",
    "Hence, we will use the tenth version model in the first running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='16-10-3'></a>\n",
    "#### 16.10.3 - Model's predictions on the train and dev datasets\n",
    "\n",
    "In this section we will show examples of model's predictions on the train and dev datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the model's predictions on train dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_predictions(tenth_version_model, train_dataset, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Six examples of model's predictions on the train dataset are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\tenth_version_predictions_train_part1.png\" style=\"width:200;height:200;\">\n",
    "    <img src=\"Images\\tenth_version_predictions_train_part2.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 57</u></b>: Model's predictions on the train dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the model's predictions on dev dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_predictions(tenth_version_model, dev_dataset, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Six examples of model's predictions on the dev dataset are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\tenth_version_predictions_dev_part1.png\" style=\"width:200;height:200;\">\n",
    "    <img src=\"Images\\tenth_version_predictions_dev_part2.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 58</u></b>: Model's predictions on the dev dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, we got nice results and our model less overfit the train dataset, but the model performance is not good compared pervious models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='16-11'></a>\n",
    "### 16.11 - Eleventh model version\n",
    "\n",
    "In this section we will create model that has the regular Unet model architecture with dropout layer after each max pooling layer in the encoder and after each deconvultion layer in the decoder, advance_data_augmentation function and Weighted Sparse Categorical Cross entropy loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='16-11-1'></a>\n",
    "#### 16.11.1 - Create the model\n",
    "\n",
    "In this section we will create the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "eleventh_version_model  = genral_unet_model(INPUT_SHAPE, advance_data_augmentation, unet_encoder, unet_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_12 (InputLayer)       [(None, 128, 128, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " data_augmentation_layer_11  (None, 128, 128, 3)          0         ['input_12[0][0]']            \n",
      "  (DataAugmentationLayer)                                                                         \n",
      "                                                                                                  \n",
      " conv2d_209 (Conv2D)         (None, 128, 128, 64)         1792      ['data_augmentation_layer_11[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " batch_normalization_198 (B  (None, 128, 128, 64)         256       ['conv2d_209[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_210 (Conv2D)         (None, 128, 128, 64)         36928     ['batch_normalization_198[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_199 (B  (None, 128, 128, 64)         256       ['conv2d_210[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_44 (MaxPooli  (None, 64, 64, 64)           0         ['batch_normalization_199[0][0\n",
      " ng2D)                                                              ]']                           \n",
      "                                                                                                  \n",
      " dropout_80 (Dropout)        (None, 64, 64, 64)           0         ['max_pooling2d_44[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_211 (Conv2D)         (None, 64, 64, 128)          73856     ['dropout_80[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_200 (B  (None, 64, 64, 128)          512       ['conv2d_211[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_212 (Conv2D)         (None, 64, 64, 128)          147584    ['batch_normalization_200[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_201 (B  (None, 64, 64, 128)          512       ['conv2d_212[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_45 (MaxPooli  (None, 32, 32, 128)          0         ['batch_normalization_201[0][0\n",
      " ng2D)                                                              ]']                           \n",
      "                                                                                                  \n",
      " dropout_81 (Dropout)        (None, 32, 32, 128)          0         ['max_pooling2d_45[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_213 (Conv2D)         (None, 32, 32, 256)          295168    ['dropout_81[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_202 (B  (None, 32, 32, 256)          1024      ['conv2d_213[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_214 (Conv2D)         (None, 32, 32, 256)          590080    ['batch_normalization_202[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_203 (B  (None, 32, 32, 256)          1024      ['conv2d_214[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_46 (MaxPooli  (None, 16, 16, 256)          0         ['batch_normalization_203[0][0\n",
      " ng2D)                                                              ]']                           \n",
      "                                                                                                  \n",
      " dropout_82 (Dropout)        (None, 16, 16, 256)          0         ['max_pooling2d_46[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_215 (Conv2D)         (None, 16, 16, 512)          1180160   ['dropout_82[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_204 (B  (None, 16, 16, 512)          2048      ['conv2d_215[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_216 (Conv2D)         (None, 16, 16, 512)          2359808   ['batch_normalization_204[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_205 (B  (None, 16, 16, 512)          2048      ['conv2d_216[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_47 (MaxPooli  (None, 8, 8, 512)            0         ['batch_normalization_205[0][0\n",
      " ng2D)                                                              ]']                           \n",
      "                                                                                                  \n",
      " dropout_83 (Dropout)        (None, 8, 8, 512)            0         ['max_pooling2d_47[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_217 (Conv2D)         (None, 8, 8, 1024)           4719616   ['dropout_83[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_206 (B  (None, 8, 8, 1024)           4096      ['conv2d_217[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_218 (Conv2D)         (None, 8, 8, 1024)           9438208   ['batch_normalization_206[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_207 (B  (None, 8, 8, 1024)           4096      ['conv2d_218[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_transpose_44 (Conv2  (None, 16, 16, 512)          4719104   ['batch_normalization_207[0][0\n",
      " DTranspose)                                                        ]']                           \n",
      "                                                                                                  \n",
      " dropout_84 (Dropout)        (None, 16, 16, 512)          0         ['conv2d_transpose_44[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_44 (Concatenat  (None, 16, 16, 1024)         0         ['dropout_84[0][0]',          \n",
      " e)                                                                  'batch_normalization_205[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_219 (Conv2D)         (None, 16, 16, 512)          4719104   ['concatenate_44[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_208 (B  (None, 16, 16, 512)          2048      ['conv2d_219[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_220 (Conv2D)         (None, 16, 16, 512)          2359808   ['batch_normalization_208[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_209 (B  (None, 16, 16, 512)          2048      ['conv2d_220[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_transpose_45 (Conv2  (None, 32, 32, 256)          1179904   ['batch_normalization_209[0][0\n",
      " DTranspose)                                                        ]']                           \n",
      "                                                                                                  \n",
      " dropout_85 (Dropout)        (None, 32, 32, 256)          0         ['conv2d_transpose_45[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_45 (Concatenat  (None, 32, 32, 512)          0         ['dropout_85[0][0]',          \n",
      " e)                                                                  'batch_normalization_203[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_221 (Conv2D)         (None, 32, 32, 256)          1179904   ['concatenate_45[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_210 (B  (None, 32, 32, 256)          1024      ['conv2d_221[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_222 (Conv2D)         (None, 32, 32, 256)          590080    ['batch_normalization_210[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_211 (B  (None, 32, 32, 256)          1024      ['conv2d_222[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_transpose_46 (Conv2  (None, 64, 64, 128)          295040    ['batch_normalization_211[0][0\n",
      " DTranspose)                                                        ]']                           \n",
      "                                                                                                  \n",
      " dropout_86 (Dropout)        (None, 64, 64, 128)          0         ['conv2d_transpose_46[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_46 (Concatenat  (None, 64, 64, 256)          0         ['dropout_86[0][0]',          \n",
      " e)                                                                  'batch_normalization_201[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_223 (Conv2D)         (None, 64, 64, 128)          295040    ['concatenate_46[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_212 (B  (None, 64, 64, 128)          512       ['conv2d_223[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_224 (Conv2D)         (None, 64, 64, 128)          147584    ['batch_normalization_212[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_213 (B  (None, 64, 64, 128)          512       ['conv2d_224[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_transpose_47 (Conv2  (None, 128, 128, 64)         73792     ['batch_normalization_213[0][0\n",
      " DTranspose)                                                        ]']                           \n",
      "                                                                                                  \n",
      " dropout_87 (Dropout)        (None, 128, 128, 64)         0         ['conv2d_transpose_47[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_47 (Concatenat  (None, 128, 128, 128)        0         ['dropout_87[0][0]',          \n",
      " e)                                                                  'batch_normalization_199[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_225 (Conv2D)         (None, 128, 128, 64)         73792     ['concatenate_47[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_214 (B  (None, 128, 128, 64)         256       ['conv2d_225[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_226 (Conv2D)         (None, 128, 128, 64)         36928     ['batch_normalization_214[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_215 (B  (None, 128, 128, 64)         256       ['conv2d_226[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_227 (Conv2D)         (None, 128, 128, 34)         2210      ['batch_normalization_215[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 34539042 (131.76 MB)\n",
      "Trainable params: 34527266 (131.71 MB)\n",
      "Non-trainable params: 11776 (46.00 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "eleventh_version_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compile the model with the Weighted Sparse Categorical Cross entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eleventh_version_model.compile(optimizer='adam',\n",
    "              loss=weighted_sparse_categorical_crossentropy,\n",
    "              metrics=['accuracy', dice_coefficient])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create visualization callback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, ground_truth in train_dataset.take(1):\n",
    "    eleventh_version_viz_callback = VizCallback(eleventh_version_model, img[FIRST_EXAMPLE_IN_BATCH], ground_truth[FIRST_EXAMPLE_IN_BATCH])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create Learning Rate scheduler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_LR_ELEVENTH_VERSION = 0.001\n",
    "NEW_LR_ELEVENTH_VERSION = 0.0001\n",
    "NUM_OF_EPOCHS_FOR_INITIAL_LR_ELEVENTH_VERSION = 20\n",
    "    \n",
    "eleventh_version_lr_callback = LearningRateScheduler(lambda epoch: lr_scheduler(epoch, INITIAL_LR_ELEVENTH_VERSION, NEW_LR_ELEVENTH_VERSION, NUM_OF_EPOCHS_FOR_INITIAL_LR_ELEVENTH_VERSION))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='16-11-2'></a>\n",
    "#### 16.11.2 - Train the model and evaluate him on train and dev datasets\n",
    "\n",
    "In this section we will train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we will train our model for 40 epochs, and we will see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ELEVENTH_VERSION_EPOCHS = 40\n",
    "eleventh_version_history = eleventh_version_model.fit(train_dataset, epochs=ELEVENTH_VERSION_EPOCHS, validation_data=dev_dataset, callbacks=[eleventh_version_viz_callback, eleventh_version_lr_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the history of this training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(eleventh_version_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The history of this training process is:\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\eleventh_version_first_run_history_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 59</u></b>: History result of the eleventh version model in the first training run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see the results of the model in the second epoch and in the last epoch:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\eleventh_version_first_run_second_epoch_result.png\" style=\"width:200;height:200;\">\n",
    "    <img src=\"Images\\eleventh_version_first_run_last_epoch_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 60</u></b>: Results of the eleventh version model in the first training run in the second epoch and in the last epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions:**\n",
    "\n",
    "* We can see in the history result, that the loss, Accuracy and Dice Cooefficient on the train dataset and on the dev dataset converge in a pretty good way. This is because we use Learning Rate scheduler and from epoch 20 we take little steps in the training process.\n",
    "\n",
    "* We can see that the model accuracy is preety high, but the Dice Cooefficient score not high, and the reason for this is that our model less good in predict classes that have small amount of data(we talked about this in section 7). We need to try to get higer preformance in train and dev datasets.\n",
    "\n",
    "* We can see that our model less overfit the loss, Accuracy and Dice Coefficient on train dataset compare to previous models. This is because we use new function of data augmentation that she better than the initial data augmentation function.\n",
    "\n",
    "In conclusion, we need to try get better results in the train and dev datasets while not overfit the train dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try train the model more in order to see if we can get better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create Learning Rate scheduler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OF_EPOCHS_FOR_INITIAL_LR_ELEVENTH_VERSION_SECOND_RUN = 0\n",
    "\n",
    "eleventh_version_second_run_lr_callback = LearningRateScheduler(lambda epoch: lr_scheduler(epoch, INITIAL_LR_ELEVENTH_VERSION, NEW_LR_ELEVENTH_VERSION, NUM_OF_EPOCHS_FOR_INITIAL_LR_ELEVENTH_VERSION_SECOND_RUN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ELEVENTH_VERSION_SECOND_RUN_EPOCHS = 40\n",
    "eleventh_version_second_run_history = eleventh_version_model.fit(train_dataset, epochs=ELEVENTH_VERSION_SECOND_RUN_EPOCHS, validation_data=dev_dataset, callbacks=[eleventh_version_viz_callback, eleventh_version_second_run_lr_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the history of this training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(eleventh_version_second_run_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The history of this training process is:\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\eleventh_version_second_run_history_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 61</u></b>: History result of the eleventh version model in the second training run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can compare the results of the model in the last epoch of the first training run and in the last epoch of the second training:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\eleventh_version_first_run_last_epoch_result.png\" style=\"width:200;height:200;\">\n",
    "    <img src=\"Images\\eleventh_version_second_run_last_epoch_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 62</u></b>: Results of the eleventh version model in the first training run in the last epoch and in the second training run in the last epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions:**\n",
    "\n",
    "* We can see in the history result, that the loss, Accuracy and Dice Cooefficient on the train dataset converge in a pretty good way(but less converge compare to the first training), but on the dev dataset our model become much less stable, and moving in a zigzag pattern.\n",
    "\n",
    "* We get better results in the last epoch in the second running compare to the last epoch in the first running in the train dataset, but our model performance not improved much on the dev dataset compare to the first running, and even get worse.\n",
    "\n",
    "* We can see that our model much more overfit the train dataset.\n",
    "\n",
    "In overall, the model performance improved on the train dataset, and the model performance on the the dev dataset not improved(and even get worse), and our model overfit the train dataset(much more compered to the first training run). \n",
    "\n",
    "Hence, we will use the eleventh version model in the first running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='16-11-3'></a>\n",
    "#### 16.11.3 - Model's predictions on the train and dev datasets\n",
    "\n",
    "In this section we will show examples of model's predictions on the train and dev datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the model's predictions on train dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_predictions(eleventh_version_model, train_dataset, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Six examples of model's predictions on the train dataset are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\eleventh_version_predictions_train_part1.png\" style=\"width:200;height:200;\">\n",
    "    <img src=\"Images\\eleventh_version_predictions_train_part2.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 63</u></b>: Model's predictions on the train dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the model's predictions on dev dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_predictions(eleventh_version_model, dev_dataset, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Six examples of model's predictions on the dev dataset are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\eleventh_version_predictions_dev_part1.png\" style=\"width:200;height:200;\">\n",
    "    <img src=\"Images\\eleventh_version_predictions_dev_part2.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 64</u></b>: Model's predictions on the dev dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, we got nice results, our model less overfit the loss, Accuracy and Dice Coefficient on train dataset compare to previous models, and the loss, Accuracy and Dice Cooefficient on the dev dataset were more converge and were not move in a zigzag pattern, but the model performance is not good enough compared pervious models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='16-12'></a>\n",
    "### 16.12 - Twelfth model version\n",
    "\n",
    "In this section we will create model that has the regular Unet model architecture with dropout layer after each max pooling layer in the encoder and after each deconvultion layer in the decoder, basic_data_augmentation function, second version of normalization, Weighted Sparse Categorical Cross entroy loss function and Learning Rate scheduler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='16-12-1'></a>\n",
    "#### 16.12.1 - Create the model\n",
    "\n",
    "In this section we will create the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "twelfth_version_model  = genral_unet_model(INPUT_SHAPE, basic_data_augmentation, unet_encoder, unet_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_13 (InputLayer)       [(None, 128, 128, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " data_augmentation_layer_12  (None, 128, 128, 3)          0         ['input_13[0][0]']            \n",
      "  (DataAugmentationLayer)                                                                         \n",
      "                                                                                                  \n",
      " conv2d_228 (Conv2D)         (None, 128, 128, 64)         1792      ['data_augmentation_layer_12[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " batch_normalization_216 (B  (None, 128, 128, 64)         256       ['conv2d_228[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_229 (Conv2D)         (None, 128, 128, 64)         36928     ['batch_normalization_216[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_217 (B  (None, 128, 128, 64)         256       ['conv2d_229[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_48 (MaxPooli  (None, 64, 64, 64)           0         ['batch_normalization_217[0][0\n",
      " ng2D)                                                              ]']                           \n",
      "                                                                                                  \n",
      " dropout_88 (Dropout)        (None, 64, 64, 64)           0         ['max_pooling2d_48[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_230 (Conv2D)         (None, 64, 64, 128)          73856     ['dropout_88[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_218 (B  (None, 64, 64, 128)          512       ['conv2d_230[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_231 (Conv2D)         (None, 64, 64, 128)          147584    ['batch_normalization_218[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_219 (B  (None, 64, 64, 128)          512       ['conv2d_231[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_49 (MaxPooli  (None, 32, 32, 128)          0         ['batch_normalization_219[0][0\n",
      " ng2D)                                                              ]']                           \n",
      "                                                                                                  \n",
      " dropout_89 (Dropout)        (None, 32, 32, 128)          0         ['max_pooling2d_49[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_232 (Conv2D)         (None, 32, 32, 256)          295168    ['dropout_89[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_220 (B  (None, 32, 32, 256)          1024      ['conv2d_232[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_233 (Conv2D)         (None, 32, 32, 256)          590080    ['batch_normalization_220[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_221 (B  (None, 32, 32, 256)          1024      ['conv2d_233[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_50 (MaxPooli  (None, 16, 16, 256)          0         ['batch_normalization_221[0][0\n",
      " ng2D)                                                              ]']                           \n",
      "                                                                                                  \n",
      " dropout_90 (Dropout)        (None, 16, 16, 256)          0         ['max_pooling2d_50[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_234 (Conv2D)         (None, 16, 16, 512)          1180160   ['dropout_90[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_222 (B  (None, 16, 16, 512)          2048      ['conv2d_234[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_235 (Conv2D)         (None, 16, 16, 512)          2359808   ['batch_normalization_222[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_223 (B  (None, 16, 16, 512)          2048      ['conv2d_235[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_51 (MaxPooli  (None, 8, 8, 512)            0         ['batch_normalization_223[0][0\n",
      " ng2D)                                                              ]']                           \n",
      "                                                                                                  \n",
      " dropout_91 (Dropout)        (None, 8, 8, 512)            0         ['max_pooling2d_51[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_236 (Conv2D)         (None, 8, 8, 1024)           4719616   ['dropout_91[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_224 (B  (None, 8, 8, 1024)           4096      ['conv2d_236[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_237 (Conv2D)         (None, 8, 8, 1024)           9438208   ['batch_normalization_224[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_225 (B  (None, 8, 8, 1024)           4096      ['conv2d_237[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_transpose_48 (Conv2  (None, 16, 16, 512)          4719104   ['batch_normalization_225[0][0\n",
      " DTranspose)                                                        ]']                           \n",
      "                                                                                                  \n",
      " dropout_92 (Dropout)        (None, 16, 16, 512)          0         ['conv2d_transpose_48[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_48 (Concatenat  (None, 16, 16, 1024)         0         ['dropout_92[0][0]',          \n",
      " e)                                                                  'batch_normalization_223[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_238 (Conv2D)         (None, 16, 16, 512)          4719104   ['concatenate_48[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_226 (B  (None, 16, 16, 512)          2048      ['conv2d_238[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_239 (Conv2D)         (None, 16, 16, 512)          2359808   ['batch_normalization_226[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_227 (B  (None, 16, 16, 512)          2048      ['conv2d_239[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_transpose_49 (Conv2  (None, 32, 32, 256)          1179904   ['batch_normalization_227[0][0\n",
      " DTranspose)                                                        ]']                           \n",
      "                                                                                                  \n",
      " dropout_93 (Dropout)        (None, 32, 32, 256)          0         ['conv2d_transpose_49[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_49 (Concatenat  (None, 32, 32, 512)          0         ['dropout_93[0][0]',          \n",
      " e)                                                                  'batch_normalization_221[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_240 (Conv2D)         (None, 32, 32, 256)          1179904   ['concatenate_49[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_228 (B  (None, 32, 32, 256)          1024      ['conv2d_240[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_241 (Conv2D)         (None, 32, 32, 256)          590080    ['batch_normalization_228[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_229 (B  (None, 32, 32, 256)          1024      ['conv2d_241[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_transpose_50 (Conv2  (None, 64, 64, 128)          295040    ['batch_normalization_229[0][0\n",
      " DTranspose)                                                        ]']                           \n",
      "                                                                                                  \n",
      " dropout_94 (Dropout)        (None, 64, 64, 128)          0         ['conv2d_transpose_50[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_50 (Concatenat  (None, 64, 64, 256)          0         ['dropout_94[0][0]',          \n",
      " e)                                                                  'batch_normalization_219[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_242 (Conv2D)         (None, 64, 64, 128)          295040    ['concatenate_50[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_230 (B  (None, 64, 64, 128)          512       ['conv2d_242[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_243 (Conv2D)         (None, 64, 64, 128)          147584    ['batch_normalization_230[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_231 (B  (None, 64, 64, 128)          512       ['conv2d_243[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_transpose_51 (Conv2  (None, 128, 128, 64)         73792     ['batch_normalization_231[0][0\n",
      " DTranspose)                                                        ]']                           \n",
      "                                                                                                  \n",
      " dropout_95 (Dropout)        (None, 128, 128, 64)         0         ['conv2d_transpose_51[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_51 (Concatenat  (None, 128, 128, 128)        0         ['dropout_95[0][0]',          \n",
      " e)                                                                  'batch_normalization_217[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_244 (Conv2D)         (None, 128, 128, 64)         73792     ['concatenate_51[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_232 (B  (None, 128, 128, 64)         256       ['conv2d_244[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_245 (Conv2D)         (None, 128, 128, 64)         36928     ['batch_normalization_232[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_233 (B  (None, 128, 128, 64)         256       ['conv2d_245[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_246 (Conv2D)         (None, 128, 128, 34)         2210      ['batch_normalization_233[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 34539042 (131.76 MB)\n",
      "Trainable params: 34527266 (131.71 MB)\n",
      "Non-trainable params: 11776 (46.00 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "twelfth_version_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compile the model with the Weighted Sparse Categorical Cross entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twelfth_version_model.compile(optimizer='adam',\n",
    "              loss=weighted_sparse_categorical_crossentropy,\n",
    "              metrics=['accuracy', dice_coefficient])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create visualization callback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, ground_truth in train_dataset.take(1):\n",
    "    twelfth_version_viz_callback = VizCallback(twelfth_version_model, img[FIRST_EXAMPLE_IN_BATCH], ground_truth[FIRST_EXAMPLE_IN_BATCH])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create Learning Rate scheduler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_LR_TWELFTH_VERSION = 0.001\n",
    "NEW_LR_TWELFTH_VERSION = 0.0001\n",
    "NUM_OF_EPOCHS_FOR_INITIAL_LR_TWELFTH_VERSION = 20\n",
    "    \n",
    "twelfth_version_lr_callback = LearningRateScheduler(lambda epoch: lr_scheduler(epoch, INITIAL_LR_TWELFTH_VERSION, NEW_LR_TWELFTH_VERSION, NUM_OF_EPOCHS_FOR_INITIAL_LR_TWELFTH_VERSION))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='16-12-2'></a>\n",
    "#### 16.12.2 - Train the model and evaluate him on train and dev datasets\n",
    "\n",
    "In this section we will train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we will train our model for 40 epochs, and we will see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TWELFTH_VERSION_EPOCHS = 40\n",
    "twelfth_version_history = twelfth_version_model.fit(train_dataset, epochs=TWELFTH_VERSION_EPOCHS, validation_data=dev_dataset, callbacks=[twelfth_version_viz_callback, twelfth_version_lr_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the history of this training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(twelfth_version_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The history of this training process is:\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\twelfth_version_first_run_history_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 65</u></b>: History result of the twelfth version model in the first training run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see the results of the model in the second epoch and in the last epoch:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\twelfth_version_first_run_second_epoch_result.png\" style=\"width:200;height:200;\">\n",
    "    <img src=\"Images\\twelfth_version_first_run_last_epoch_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 66</u></b>: Results of the twelfth version model in the first training run in the second epoch and in the last epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions:**\n",
    "\n",
    "* We can see in the history result, that the loss, Accuracy and Dice Cooefficient on the train dataset and on the dev dataset converge in a pretty good way. This is because we use Learning Rate scheduler and from epoch 20 we take little steps in the training process.\n",
    "\n",
    "* We can see that the model accuracy is preety high, but the Dice Cooefficient score not high, and the reason for this is that our model less good in predict classes that have small amount of data(we talked about this in section 7). We need to try to get higer preformance in train and dev datasets.\n",
    "\n",
    "* We can see that our model less overfit the loss, Accuracy and Dice Coefficient on train dataset compare to previous models. Maybee this is because we use new normalization method, or because we use Learning Rate scheduler and from epoch 20 we take little steps in the training process.\n",
    "\n",
    "In conclusion, we need to try get better results in the train and dev datasets while not overfit the train dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try train the model more in order to see if we can get better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create Learning Rate scheduler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OF_EPOCHS_FOR_INITIAL_LR_TWELFTH_VERSION_SECOND_RUN = 0\n",
    "\n",
    "twelfth_version_second_run_lr_callback = LearningRateScheduler(lambda epoch: lr_scheduler(epoch, INITIAL_LR_TWELFTH_VERSION, NEW_LR_TWELFTH_VERSION, NUM_OF_EPOCHS_FOR_INITIAL_LR_TWELFTH_VERSION_SECOND_RUN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TWELFTH_VERSION_SECOND_RUN_EPOCHS = 40\n",
    "twelfth_version_second_run_history = twelfth_version_model.fit(train_dataset_second_version, epochs=TWELFTH_VERSION_SECOND_RUN_EPOCHS, validation_data=dev_dataset_second_version, callbacks=[twelfth_version_viz_callback, twelfth_version_second_run_lr_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the history of this training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(twelfth_version_second_run_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The history of this training process is:\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\twelfth_version_second_run_history_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 67</u></b>: History result of the twelfth version model in the second training run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can compare the results of the model in the last epoch of the first training run and in the last epoch of the second training:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\twelfth_version_first_run_last_epoch_result.png\" style=\"width:200;height:200;\">\n",
    "    <img src=\"Images\\twelfth_version_second_run_last_epoch_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 68</u></b>: Results of the twelfth version model in the first training run in the last epoch and in the second training run in the last epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions:**\n",
    "\n",
    "* We can see in the history result, that the loss, Accuracy and Dice Cooefficient on the train dataset converge in a pretty good way(but less converge compare to the first training), but on the dev dataset our model become much less stable, and moving in a zigzag pattern.\n",
    "\n",
    "* We get better results in the last epoch in the second running compare to the last epoch in the first running in the train dataset, but our model performance not improved much on the dev dataset compare to the first running, and even get worse in the loss.\n",
    "\n",
    "* We can see that our model much more overfit the train dataset.\n",
    "\n",
    "In overall, the model performance improved on the train dataset, and the model performance on the the dev dataset not improved much(and even get worse in the loss), and our model more overfit the train dataset. \n",
    "We can see that this model has very similar performance to the eighth version model performance, and thus the fact that we changed the normaliztation method did not help. Hence, we will not continue to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='16-12-3'></a>\n",
    "#### 16.12.3 - Model's predictions on the train and dev datasets\n",
    "\n",
    "In this section we will show examples of model's predictions on the train and dev datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the model's predictions on train dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_predictions(twelfth_version_model, train_dataset_second_version, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Six examples of model's predictions on the train dataset are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\twelfth_version_predictions_train_part1.png\" style=\"width:200;height:200;\">\n",
    "    <img src=\"Images\\twelfth_version_predictions_train_part2.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 69</u></b>: Model's predictions on the train dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the model's predictions on dev dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_predictions(twelfth_version_model, dev_dataset_second_version, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Six examples of model's predictions on the dev dataset are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\twelfth_version_predictions_dev_part1.png\" style=\"width:200;height:200;\">\n",
    "    <img src=\"Images\\twelfth_version_predictions_dev_part2.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 70</u></b>: Model's predictions on the dev dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, we got similar results to the eighth version model performance, and thus the fact that we changed the normaliztation method did not help us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='16-13'></a>\n",
    "### 16.13 - Thirteen model version\n",
    "\n",
    "In this section we will create model that has the regular Unet model architecture with dropout layer after each max pooling layer in the encoder and after each deconvultion layer in the decoder, advance_data_augmentation function, second version of normalization, Weighted Sparse Categorical Cross entroy loss function and Learning Rate scheduler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='16-13-1'></a>\n",
    "#### 16.13.1 - Create the model\n",
    "\n",
    "In this section we will create the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "thirteen_version_model  = genral_unet_model(INPUT_SHAPE, advance_data_augmentation, unet_encoder, unet_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_13\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_14 (InputLayer)       [(None, 128, 128, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " data_augmentation_layer_13  (None, 128, 128, 3)          0         ['input_14[0][0]']            \n",
      "  (DataAugmentationLayer)                                                                         \n",
      "                                                                                                  \n",
      " conv2d_247 (Conv2D)         (None, 128, 128, 64)         1792      ['data_augmentation_layer_13[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " batch_normalization_234 (B  (None, 128, 128, 64)         256       ['conv2d_247[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_248 (Conv2D)         (None, 128, 128, 64)         36928     ['batch_normalization_234[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_235 (B  (None, 128, 128, 64)         256       ['conv2d_248[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_52 (MaxPooli  (None, 64, 64, 64)           0         ['batch_normalization_235[0][0\n",
      " ng2D)                                                              ]']                           \n",
      "                                                                                                  \n",
      " dropout_96 (Dropout)        (None, 64, 64, 64)           0         ['max_pooling2d_52[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_249 (Conv2D)         (None, 64, 64, 128)          73856     ['dropout_96[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_236 (B  (None, 64, 64, 128)          512       ['conv2d_249[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_250 (Conv2D)         (None, 64, 64, 128)          147584    ['batch_normalization_236[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_237 (B  (None, 64, 64, 128)          512       ['conv2d_250[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_53 (MaxPooli  (None, 32, 32, 128)          0         ['batch_normalization_237[0][0\n",
      " ng2D)                                                              ]']                           \n",
      "                                                                                                  \n",
      " dropout_97 (Dropout)        (None, 32, 32, 128)          0         ['max_pooling2d_53[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_251 (Conv2D)         (None, 32, 32, 256)          295168    ['dropout_97[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_238 (B  (None, 32, 32, 256)          1024      ['conv2d_251[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_252 (Conv2D)         (None, 32, 32, 256)          590080    ['batch_normalization_238[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_239 (B  (None, 32, 32, 256)          1024      ['conv2d_252[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_54 (MaxPooli  (None, 16, 16, 256)          0         ['batch_normalization_239[0][0\n",
      " ng2D)                                                              ]']                           \n",
      "                                                                                                  \n",
      " dropout_98 (Dropout)        (None, 16, 16, 256)          0         ['max_pooling2d_54[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_253 (Conv2D)         (None, 16, 16, 512)          1180160   ['dropout_98[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_240 (B  (None, 16, 16, 512)          2048      ['conv2d_253[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_254 (Conv2D)         (None, 16, 16, 512)          2359808   ['batch_normalization_240[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_241 (B  (None, 16, 16, 512)          2048      ['conv2d_254[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_55 (MaxPooli  (None, 8, 8, 512)            0         ['batch_normalization_241[0][0\n",
      " ng2D)                                                              ]']                           \n",
      "                                                                                                  \n",
      " dropout_99 (Dropout)        (None, 8, 8, 512)            0         ['max_pooling2d_55[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_255 (Conv2D)         (None, 8, 8, 1024)           4719616   ['dropout_99[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_242 (B  (None, 8, 8, 1024)           4096      ['conv2d_255[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_256 (Conv2D)         (None, 8, 8, 1024)           9438208   ['batch_normalization_242[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_243 (B  (None, 8, 8, 1024)           4096      ['conv2d_256[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_transpose_52 (Conv2  (None, 16, 16, 512)          4719104   ['batch_normalization_243[0][0\n",
      " DTranspose)                                                        ]']                           \n",
      "                                                                                                  \n",
      " dropout_100 (Dropout)       (None, 16, 16, 512)          0         ['conv2d_transpose_52[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_52 (Concatenat  (None, 16, 16, 1024)         0         ['dropout_100[0][0]',         \n",
      " e)                                                                  'batch_normalization_241[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_257 (Conv2D)         (None, 16, 16, 512)          4719104   ['concatenate_52[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_244 (B  (None, 16, 16, 512)          2048      ['conv2d_257[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_258 (Conv2D)         (None, 16, 16, 512)          2359808   ['batch_normalization_244[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_245 (B  (None, 16, 16, 512)          2048      ['conv2d_258[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_transpose_53 (Conv2  (None, 32, 32, 256)          1179904   ['batch_normalization_245[0][0\n",
      " DTranspose)                                                        ]']                           \n",
      "                                                                                                  \n",
      " dropout_101 (Dropout)       (None, 32, 32, 256)          0         ['conv2d_transpose_53[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_53 (Concatenat  (None, 32, 32, 512)          0         ['dropout_101[0][0]',         \n",
      " e)                                                                  'batch_normalization_239[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_259 (Conv2D)         (None, 32, 32, 256)          1179904   ['concatenate_53[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_246 (B  (None, 32, 32, 256)          1024      ['conv2d_259[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_260 (Conv2D)         (None, 32, 32, 256)          590080    ['batch_normalization_246[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_247 (B  (None, 32, 32, 256)          1024      ['conv2d_260[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_transpose_54 (Conv2  (None, 64, 64, 128)          295040    ['batch_normalization_247[0][0\n",
      " DTranspose)                                                        ]']                           \n",
      "                                                                                                  \n",
      " dropout_102 (Dropout)       (None, 64, 64, 128)          0         ['conv2d_transpose_54[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_54 (Concatenat  (None, 64, 64, 256)          0         ['dropout_102[0][0]',         \n",
      " e)                                                                  'batch_normalization_237[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_261 (Conv2D)         (None, 64, 64, 128)          295040    ['concatenate_54[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_248 (B  (None, 64, 64, 128)          512       ['conv2d_261[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_262 (Conv2D)         (None, 64, 64, 128)          147584    ['batch_normalization_248[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_249 (B  (None, 64, 64, 128)          512       ['conv2d_262[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_transpose_55 (Conv2  (None, 128, 128, 64)         73792     ['batch_normalization_249[0][0\n",
      " DTranspose)                                                        ]']                           \n",
      "                                                                                                  \n",
      " dropout_103 (Dropout)       (None, 128, 128, 64)         0         ['conv2d_transpose_55[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_55 (Concatenat  (None, 128, 128, 128)        0         ['dropout_103[0][0]',         \n",
      " e)                                                                  'batch_normalization_235[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_263 (Conv2D)         (None, 128, 128, 64)         73792     ['concatenate_55[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_250 (B  (None, 128, 128, 64)         256       ['conv2d_263[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_264 (Conv2D)         (None, 128, 128, 64)         36928     ['batch_normalization_250[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_251 (B  (None, 128, 128, 64)         256       ['conv2d_264[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_265 (Conv2D)         (None, 128, 128, 34)         2210      ['batch_normalization_251[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 34539042 (131.76 MB)\n",
      "Trainable params: 34527266 (131.71 MB)\n",
      "Non-trainable params: 11776 (46.00 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "thirteen_version_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compile the model with the Weighted Sparse Categorical Cross entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thirteen_version_model.compile(optimizer='adam',\n",
    "              loss=weighted_sparse_categorical_crossentropy,\n",
    "              metrics=['accuracy', dice_coefficient])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create visualization callback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, ground_truth in train_dataset.take(1):\n",
    "    thirteen_version_viz_callback = VizCallback(thirteen_version_model, img[FIRST_EXAMPLE_IN_BATCH], ground_truth[FIRST_EXAMPLE_IN_BATCH])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create Learning Rate scheduler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_LR_THIRTEEN_VERSION = 0.001\n",
    "NEW_LR_THIRTEEN_VERSION = 0.0001\n",
    "NUM_OF_EPOCHS_FOR_INITIAL_LR_THIRTEEN_VERSION = 20\n",
    "    \n",
    "thirteen_version_lr_callback = LearningRateScheduler(lambda epoch: lr_scheduler(epoch, INITIAL_LR_THIRTEEN_VERSION, NEW_LR_THIRTEEN_VERSION, NUM_OF_EPOCHS_FOR_INITIAL_LR_THIRTEEN_VERSION))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='16-13-2'></a>\n",
    "#### 16.13.2 - Train the model and evaluate him on train and dev datasets\n",
    "\n",
    "In this section we will train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we will train our model for 40 epochs, and we will see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THIRTEEN_VERSION_EPOCHS = 40\n",
    "thirteen_version_history = thirteen_version_model.fit(train_dataset_second_version, epochs=THIRTEEN_VERSION_EPOCHS, validation_data=dev_dataset_second_version, callbacks=[thirteen_version_viz_callback, thirteen_version_lr_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the history of this training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(thirteen_version_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The history of this training process is:\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\thirteen_version_first_run_history_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 71</u></b>: History result of the thirteen version model in the first training run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see the results of the model in the second epoch and in the last epoch:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\thirteen_version_first_run_second_epoch_result.png\" style=\"width:200;height:200;\">\n",
    "    <img src=\"Images\\thirteen_version_first_run_last_epoch_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 72</u></b>: Results of the thirteen version model in the first training run in the second epoch and in the last epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions:**\n",
    "\n",
    "* We can see in the history result, that the loss, Accuracy and Dice Cooefficient on the train dataset and on the dev dataset converge in a pretty good way. This is because we use Learning Rate scheduler and from epoch 20 we take little steps in the training process.\n",
    "\n",
    "* We can see that the model accuracy is preety high, but the Dice Cooefficient score not high, and the reason for this is that our model less good in predict classes that have small amount of data(we talked about this in section 7). We need to try to get higer preformance in train and dev datasets.\n",
    "\n",
    "* We can see that our model less overfit the loss, Accuracy and Dice Coefficient on train dataset compare to previous models. This is because we use new function of data augmentation that she better than the initial data augmentation function. In additoin maybee this is because we use new normalization method, or because we use Learning Rate scheduler and from epoch 20 we take little steps in the training process.\n",
    "\n",
    "In conclusion, we need to try get better results in the train and dev datasets while not overfit the train dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try train the model more in order to see if we can get better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create Learning Rate scheduler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OF_EPOCHS_FOR_INITIAL_LR_THIRTEEN_VERSION_SECOND_RUN = 0\n",
    "\n",
    "thirteen_version_second_run_lr_callback = LearningRateScheduler(lambda epoch: lr_scheduler(epoch, INITIAL_LR_THIRTEEN_VERSION, NEW_LR_THIRTEEN_VERSION, NUM_OF_EPOCHS_FOR_INITIAL_LR_THIRTEEN_VERSION_SECOND_RUN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THIRTEEN_VERSION_SECOND_RUN_EPOCHS = 40\n",
    "thirteen_version_second_run_history = thirteen_version_model.fit(train_dataset_second_version, epochs=THIRTEEN_VERSION_SECOND_RUN_EPOCHS, validation_data=dev_dataset_second_version, callbacks=[thirteen_version_viz_callback, thirteen_version_second_run_lr_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the history of this training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(thirteen_version_second_run_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The history of this training process is:\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\thirteen_version_second_run_history_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 73</u></b>: History result of the thirteen version model in the second training run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can compare the results of the model in the last epoch of the first training run and in the last epoch of the second training:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\thirteen_version_first_run_last_epoch_result.png\" style=\"width:200;height:200;\">\n",
    "    <img src=\"Images\\thirteen_version_second_run_last_epoch_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 74</u></b>: Results of the thirteen version model in the first training run in the last epoch and in the second training run in the last epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions:**\n",
    "\n",
    "* We can see in the history result, that the loss, Accuracy and Dice Cooefficient on the train dataset converge in a pretty good way(but less converge compare to the first training), but on the dev dataset our model become much less stable, and moving in a zigzag pattern.\n",
    "\n",
    "* We get better results in the last epoch in the second running compare to the last epoch in the first running in the train dataset, but our model performance not improved much on the dev dataset compare to the first running, and even get worse in the loss.\n",
    "\n",
    "* We can see that our model much more overfit the train dataset.\n",
    "\n",
    "In overall, the model performance improved on the train dataset, and the model performance on the the dev dataset not improved much(and even get worse in the loss), and our model more overfit the train dataset. \n",
    "We can see that this model has very similar performance to the eleventh version model performance, and thus the fact that we changed the normaliztation method did not help. Hence, we will not continue to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='16-13-3'></a>\n",
    "#### 16.13.3 - Model's predictions on the train and dev datasets\n",
    "\n",
    "In this section we will show examples of model's predictions on the train and dev datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the model's predictions on train dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_predictions(thirteen_version_model, train_dataset_second_version, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Six examples of model's predictions on the train dataset are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\thirteen_version_predictions_train_part1.png\" style=\"width:200;height:200;\">\n",
    "    <img src=\"Images\\thirteen_version_predictions_train_part2.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 75</u></b>: Model's predictions on the train dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the model's predictions on dev dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_predictions(thirteen_version_model, dev_dataset_second_version, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Six examples of model's predictions on the dev dataset are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\thirteen_version_predictions_dev_part1.png\" style=\"width:200;height:200;\">\n",
    "    <img src=\"Images\\thirteen_version_predictions_dev_part2.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 76</u></b>: Model's predictions on the dev dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, we got similar results to the eleventh version model performance, and thus the fact that we changed the normaliztation method did not help us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='17'></a>\n",
    "## 17 - Final model\n",
    "\n",
    "In this section we will choose the final model that we will use, show his predictions on the test dataset and evaluate him on the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='17-1'></a>\n",
    "### 17.1 - Choose the model that we will use\n",
    "\n",
    "We need to Choose the final model from versions 1, 3, 8, 10, 11, 12 and 13(other versions not good as we explained before).\n",
    "\n",
    "Firstly, let's recall the final results of each version. Let's plot the final history result of each version and result of the last epoch(in the training run that we choosed in each model):\n",
    "\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\first_version_second_run_history_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 77</u></b>: History result of the first version model\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\first_version_second_run_last_epoch_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 78</u></b>: Results of the first version model in the last epoch\n",
    "\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\third_version_second_run_history_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 79</u></b>: History result of the third version model\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\third_version_second_run_last_epoch_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 80</u></b>: Results of the third version model in the last epoch\n",
    "\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\eighth_version_second_run_history_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 81</u></b>: History result of the eighth version model\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\eighth_version_second_run_last_epoch_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 82</u></b>: Results of the eighth version model in the last epoch\n",
    "\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\tenth_version_first_run_history_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 83</u></b>: History result of the tenth version model\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\tenth_version_first_run_last_epoch_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 84</u></b>: Results of the tenth version model in the last epoch\n",
    "\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\eleventh_version_first_run_history_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 85</u></b>: History result of the eleventh version model\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\eleventh_version_first_run_last_epoch_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 86</u></b>: Results of the eleventh version model in the last epoch\n",
    "\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\twelfth_version_second_run_history_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 87</u></b>: History result of the twelfth version model\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\twelfth_version_second_run_last_epoch_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 88</u></b>: Results of the twelfth version model in the last epoch\n",
    "\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\thirteen_version_second_run_history_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 89</u></b>: History result of the thirteen version model\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\thirteen_version_second_run_last_epoch_result.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 90</u></b>: Results of the thirteen version model in the last epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in the first version model we get the best results(acoording the evaluation matric that we defined before) on the dev dataset(also on the train dataset). \n",
    "\n",
    "Thus we will choose the first version model as our final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='17-2'></a>\n",
    "### 17.2 - Final model's predictions on the test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the final model's predictions on the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_predictions(first_version_model, test_dataset, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Six examples of the final model's predictions on the test dataset are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\first_version_predictions_test_part1.png\" style=\"width:200;height:200;\">\n",
    "    <img src=\"Images\\first_version_predictions_test_part2.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 91</u></b>: Final model's predictions on the test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='17-3'></a>\n",
    "### 17.3 - Final model evaluation on the test dataset\n",
    "\n",
    "Let's show the final model's evaluation on the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_result_test = first_version_model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the output of this is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\first_version_test_evaluation.png\" style=\"width:200;height:200;\">\n",
    "</div>\n",
    "<caption><center> <u><b>Figure 92</u></b>: Final model's evaluation on the test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='18'></a>\n",
    "## 18 - Summary \n",
    "\n",
    "So we have reached the end of the project, and I must to say that I really enjoyed from the project. I feel that I improved much while from this prject, and I managed to solve and overcome many challenges. This is my first project of this magnitude, and thus I learned so much from this project, include important thing that is read papers. I am very proud of me, and I am see that I managed to achieve many goals that I set for myself, and of course I still have a lot to improve on.\n",
    "\n",
    "The subject of the project in my opinion is very interesting, and what I learned in this project also useful for other applications like medical applications and more.\n",
    "\n",
    "Also, I managed to sucess to get very good results for first project, and I sure that with more resarch and try more architectures that are not Unet can improve the performence. One thing that worth to note is that we had small dataset and I belive that if we had larger dataset with more data from each class(espically from minority classes) we could get better performance.\n",
    "\n",
    "In conclusion, I really enjoyed from this project, and I sure that I learned so much from this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='19'></a>\n",
    "## 19 - Goals for the future\n",
    "\n",
    "I can imporve this project. My goals for the future for this project are:\n",
    "\n",
    "* Try more encoders and decoders for the Unet architecture. For example we can use VGG-16 or ResNet in the encoder and decoder.\n",
    "\n",
    "* Try more architectures that are not Unet for semantic segmentation\n",
    "\n",
    "* Try to improve the performance and overcome the problem that our dataset is small and we have the problem of imbalanced classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it, we finished this project ( for now:) ).\n",
    "\n",
    "Thank you very much for reading this project. I am very appreciate this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images\\thank_you.gif\" style=\"width:800px;height:500px;\">\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
